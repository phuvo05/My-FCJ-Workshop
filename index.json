[{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Migrating from AWS CodeDeploy to Amazon ECS for blue/green deployments by Mike Rizzo, Islam Mahgoub, and Olly Pomeroy on 16 SEP 2025 in Amazon Elastic Container Service, AWS CodeDeploy, Best Practices, Developer Tools, DevOps, Technical How-to Permalink Share\nWith blue/green deployments, you can release new software by shifting traffic between two identical environments that are running different versions of an application. This mitigates common risks associated with deploying new software releases, by facilitating the safe testing of new deployments and providing a rollback capability with near-zero downtime.\nUntil recently, Amazon Elastic Container Service (Amazon ECS) only supported rolling updates as a native deployment strategy, and you needed to use AWS CodeDeploy if you wanted to implement blue/green deployments. This has changed with the recent launch of ECS blue/green deployments.\nECS blue/green deployments offer similar functionality to CodeDeploy, but there are some differences in the features available and their implementation. This post is targeted at organizations that currently use CodeDeploy for blue/green deployments on Amazon ECS, and that are considering a migration to the new Amazon ECS capability. It provides guidance on (1) factors to consider when planning your migration, (2) mapping CodeDeploy concepts to their equivalent in ECS blue/green deployments, and (3) migration strategies.\nPlanning your migration When migrating from CodeDeploy to ECS blue/green deployments, you should consider the following points as part of your planning process:\nNew possibilities: ECS blue/green deployments enable a number of use cases that are not supported with CodeDeploy. These include the following:\nService discovery options: CodeDeploy only supports services fronted by Elastic Load Balancing (ELB), whereas ECS blue/green deployments support both ELB and ECS ServiceConnect.\nHeadless service support: ECS blue/green deployments can be used in situations where no service exposure is needed, for example queue processing services.\nAmazon EBS support: ECS blue/green deployments support configuration of Amazon Elastic Block Store (Amazon EBS) volumes at service deployment.\nMultiple target groups: the ECS deployment controller allows a service to be associated with multiple target groups, which means it can be simultaneously accessible through multiple load balancers (for example for separation of internal and external service exposure).\nFlexible ALB listener configuration: CodeDeploy needs separate listeners for different services, and for production and test endpoints. ECS blue/green operates at the listener rule level, which means that you can benefit from using a single listener with advanced request routing based on host name, HTTP headers, path, method, query string or source IP. For example, you can use a common listener port for multiple services using path-based routing, and support A/B testing using query string based routing. You can also support blue/green production and test traffic on the same listener port.\nOperational improvements: ECS blue/green deployments offer (1) better alignment with existing Amazon ECS features (such as circuit breaker, deployment history and lifecycle hooks), which helps transition between different Amazon ECS deployment strategies, (2) longer lifecycle hook execution time (CodeDeploy hooks are limited to 1 hour), and (3) improved AWS CloudFormation support (no need for separate AppSpec files for service revisions and lifecycle hooks).\nDeployment configuration limitations: CodeDeploy supports canary, linear, and all-at-once deployment configurations. As of this writing, ECS blue/green only supports all-at-once. If you are using CodeDeploy canary or linear deployments, then you first need to switch to a CodeDeploy all-at-once configuration before migrating to ECS blue/green deployments.\nAPI/CLI differences: There are differences in APIs (and associated CLI commands) between the two approaches. Mapping from one API to the other is usually straightforward but be aware that ECS blue/green deployments rely more extensively on lifecycle hooks to control deployment steps. For example, where CodeDeploy supports a wait time option for testing a new deployment (before re-routing production traffic to it), you need to use a hook to achieve this with ECS blue/green deployments.\nConsole differences: If you are using the CodeDeploy console as part of your operations, then be aware that the Amazon ECS console does not offer options for manual override of the deployment’s progress (for example to force re-routing, or early termination of bake time). Instead, you can create a custom UI (integrated with your wider operational processes) through Amazon ECS lifecycle hooks (which is arguably a safer approach).\nMigration path: There are a number of options available for migrating a service across from CodeDeploy to ECS blue/green deployments, and you need to consider which one works best for your environment. These options, along with their associated pros and cons, are covered in more detail later in this post.\nPipeline support: Support for ECS blue/green deployments may initially be limited in existing pipeline tools. More advanced pipeline integrations may necessitate the use of custom actions for an interim period. As of this writing, the CodePipeline Amazon ECS “standard” action can be used to deploy container image changes through ECS blue/green deployments (but not other service configuration changes).\nFrom CodeDeploy to ECS blue/green deployments When estimating implementation costs for migrating to ECS blue/green deployments, you must understand the API differences and how you can map CodeDeploy features to ECS blue/green deployment equivalents. Assuming you are starting from a CodeDeploy “all-at-once” configuration, this section walks you through the key differences.\nLoad balancer configuration and service creation When creating an Amazon ECS service using CodeDeploy, you first create a load balancer with a production listener and (optionally) a test listener. Each listener is configured with a single (default) rule that routes all traffic to a single target group (the primary target group) as shown in Figure 1(a). Then, you create an Amazon ECS service configured to use the listener and target group, with deploymentController type set to CODE_DEPLOY. Service creation results in the creation of a (blue) TaskSet registered with the specified target group.\nFigure 1: Load balancer initial configuration\nWith the service created, you create a CodeDeploy deployment group (as part of a CodeDeploy application), and configure it with details of the ECS cluster, service name, load balancer listeners, two target groups (the primary target group used in the production listener rule, and a secondary target group to be used for replacement tasks), an AWS Identity and Access Management (IAM) service role to grant CodeDeploy permissions to manipulate Amazon ECS and ELB resources, and various parameters that control the deployment behavior.\nECS blue/green deployments specify the deployment configuration in the Amazon ECS service itself. The load balancer production listener must be pre-configured with a rule that includes two target groups with weights of 1 and 0 respectively. As part of service creation, you specify the Amazon Resource Name (ARN) of this listener rule, the two target groups, an IAM role (to grant Amazon ECS permission to manipulate the listener and target groups), deploymentController type set to ECS, and deploymentConfiguration.strategy set to BLUE_GREEN. This creates a (blue) ServiceRevision with tasks that are registered with the primary target group.\nAlthough both approaches result in the creation of an initial set of tasks, the underlying implementation differs in that CodeDeploy uses a TaskSet, whereas Amazon ECS uses a ServiceRevision. The latter was introduced as part of the Amazon ECS service deployments API, which offers greater visibility into the deployment process and the service deployment history.\nDeploying a service revision Figure 2 shows how a new service revision is deployed. CodeDeploy deploys a new version of a service using CreateDeployment(), specifying the CodeDeploy application name, deployment group name, and revision details in an AppSpec file. This must contain the task definition for the new revision, and the container name and port to use. ECS blue/green deployments create a new service deployment by calling UpdateService(), passing details of the replacement task definition.\nFigure 2: Deploying a service revision\nOptionally, the CodeDeploy AppSpec file can also be used to specify more service configuration changes, such as networking configuration and capacity provider strategy, and to specify lifecycle hooks (see the following section). When you use Amazon ECS, you specify these changes using UpdateService().\nFigure 3: Re-routing traffic\nFigure 3 shows the difference in the way traffic re-routing is achieved. In CodeDeploy the deployment creates a replacement (green) TaskSet and registers its tasks with the secondary target group. When this becomes healthy, it is available for testing (optional) and for production. In both cases, re-routing is achieved by changing the respective listener rule to point at the secondary target group associated with the green TaskSet. Rollback is achieved by changing the production listener rule back to the primary target group.\nIn contrast, with ECS blue/green deployments, the service deployment creates a new ServiceRevision with (green) tasks and registers them with the secondary target group. Then, re-routing and rollback are achieved by switching the weights on the listener rule.\nLifecycle hooks Both CodeDeploy and ECS blue/green deployments support (optional) lifecycle hooks, wherein AWS Lambda functions can be triggered by specific lifecycle events. Hooks are useful for augmenting the deployment workflow with custom logic. For example, you can use a lifecycle hook to automate testing on a test port, before proceeding to re-route live traffic to the production port.\nCodeDeploy and ECS blue/green deployments broadly follow similar lifecycles, but there are differences in the way configuration options and lifecycle hooks are specified:\nCodeDeploy specifies lifecycle hooks as part of the AppSpec file that is supplied to CreateDeployment(). This means that the hooks need to be configured for every deployment. ECS blue/green deployments specifies the hooks (along with an IAM role that grants Amazon ECS permissions to invoke the associated Lambda functions) as part of the service configuration, and any changes would need an UpdateService() call.\nCodeDeploy and Amazon ECS lifecycle events are equivalent, but they have different names, as shown in the table below:\nLifecycle event CodeDeploy ECS blue/green Before new tasks are created BeforeInstall PRE_SCALE_UP New tasks are ready AfterInstall POST_SCALE_UP Before test port is enabled No equivalent TEST_TRAFFIC_SHIFT Test port is ready to receive traffic AfterAllowTestTraffic POST_TEST_TRAFFIC_SHIFT Before re-routing prod traffic to green BeforeAllowTraffic PRODUCTION_TRAFFIC_SHIFT Re-routing prod traffic to green has completed AfterAllowTraffic POST_PRODUCTION_TRAFFIC_SHIFT Both CodeDeploy and ECS blue/green deployments use Lambda for hook implementation, but the expected inputs and outputs differ, particularly in the way the Lambda function returns a hook status response. In CodeDeploy the function must call PutLifecycleEventHookExecutionStatus() to return the hook execution status, which can either be Succeeded or Failed. In Amazon ECS the Lambda response itself is used to indicate the hook execution status.\nCodeDeploy invokes each hook as a one-off call, and expects a final execution status to be returned within one hour. Amazon ECS hooks are more flexible in that they can return an IN_PROGRESS indicator, which signals that the hook should be re-invoked repeatedly until it results in SUCCEEDED or FAILED. The hook is invoked every 30s by default, but the timing of the next invocation can be configured by passing a parameter in the response.\nOther implementation considerations CodeDeploy offers a number of advanced options for deployment groups, which you may need to map to Amazon ECS equivalents. These include the following:\nAmazon Simple Notification Service (Amazon SNS) triggers: use Amazon EventBridge events from Amazon ECS to publish state changes to SNS topics.\nAmazon CloudWatch alarm detection and automatic rollback: use Amazon ECS deployment failure detection features.\nMigration path Having considered the implementation differences between CodeDeploy and ECS blue/green deployments, you also need to identify an appropriate migration approach. A few options are available, and you must assess which one best aligns with your architecture and requirements. The factors involved include the following:\nDowntime: Will there be any downtime, and if so for how long?\nRollback to CodeDeploy: Do you need to retain the ability to roll back the migration if the switch to ECS blue/green deployments goes wrong? You can think of this as a “blue/green strategy for the blue/green solution!”\nService discovery: Can you accommodate a change of service address (new ALB URI) or do you need to retain the same address?\nPerformance and/or speed of deployment\nCost\nIf you plan to continue to front your service using a load balancer, the following migration options represent variations on the extent to which existing resources are re-used, considering both the Amazon ECS service itself and load balancer resources. In all cases you must create an IAM role to pass to the Amazon ECS deployment controller, which enables it to manipulate the necessary load balancer resources.\nOption 1: In-place update In this approach, you update the existing Amazon ECS service to use the Amazon ECS deployment controller with the blue/green deployment strategy instead of the CodeDeploy deployment controller. You reuse the same load balancer listener and target groups that are used for CodeDeploy. As mentioned previously, CodeDeploy configures the listeners of the load balancer attached to the service with a single (default) rule that routes all traffic to a single target group (the primary target group). For ECS blue/green deployments, the load balancer listeners must be pre-configured with a rule that includes the two target groups with weights of 1 and 0. Accordingly, the following steps are needed:\nChange the default rule of the production/test listeners to include the alternate target group and set the weight of the target group and alternate target group to 1 and 0 respectively.\nUpdate the existing Amazon ECS service by calling UpdateService(), setting the parameter deploymentController to ECS, and the parameter deploymentStrategy to BLUE_GREEN. You pass the ARNs of the IAM role, target group, the alternative target group, the production listener rule, and the test listener rule (optional).\nThe Amazon ECS deployment controller creates a new service revision with new tasks under the alternate target group, then immediately re-routes traffic to this target group. Wait for this to complete, then verify that the service is working as expected.\nDelete the CodeDeploy resources for this Amazon ECS service, because you are now using ECS blue/green deployments.\nIn-place update is a safe operation, but you should be careful to (1) automate the process (particularly when changing the listener configuration) to minimize the possibility of manual error, and (2) test this process thoroughly in a developer and/or UAT environment. You also need to be aware that traffic is re-routed immediately as soon as the Amazon ECS controller finishes creating the initial service revision. Furthermore, there is no option to test this revision prior to re-routing (although the tasks should be identical to those that were running in the CodeDeploy task set).\nOption 2: New service and existing load balancer This approach uses a blue/green strategy for the migration (in other words, a blue/green migration of the blue/green solution). You create a new parallel blue/green setup using ECS blue/green deployments, verify it, switch from the CodeDeploy setup to the new ECS blue/green deployments setup, then delete the CodeDeploy resources.\nLeave the listeners, the target groups, and the Amazon ECS service for the CodeDeploy setup intact so that you can rollback to this setup if needed.\nCreate new target groups and new listeners (with different ports from the original listeners) under the existing load balancer. Then, create a new Amazon ECS service that matches the existing Amazon ECS service, except that you use ECS as the deployment controller, BLUE_GREEN as a deployment strategy, and pass the ARNs for the IAM role, new target groups, and the new listener rules.\nVerify the new setup (using the new listener’s ports). If everything goes well, then change the ports of the original listener to different port numbers (to free up the original ports), and switch the ports on the new listener to the original ports, thereby routing traffic to the new setup.\nObserve the new setup, and if everything continues to work as expected, you can delete the CodeDeploy setup.\nFigure 4 depicts this approach.\nFigure 4: Option 2 – New service and existing load balancer\nOption 3: New service and new load balancer Like the preceding approach, this approach uses a blue/green strategy for the migration. The key difference is that the switch from the CodeDeploy setup to the ECS blue/green deployment setup happens at another routing layer above the load balancer (as shown in Figure 5). Possible implementations for this layer include Amazon Route 53, Amazon API Gateway, and Amazon CloudFront.\nThis approach is suitable for users who already have this routing layer, and if all the communication with the Amazon ECS service is happening through it (in other words there is no direct communication at the load balancer level). When compared with Option 2, this option has the benefit of zero downtime but is a bit more expensive.\nFigure 5: Option 3 – New service and new load balancer\nComparison The table below compares these three migration approaches across a number of factors that may have varying levels of importance to you. You can use this table to assess which option is best suited to your own particular circumstances and priorities.\nOption 1: In-place update\rOption 2: New service and existing load balancer\rOption 3: New service and new load balancer\rMigration complexity\rSimple\nUpdate existing Amazon ECS service deployment controller and deployment strategy\nMore complex\nCreate new Amazon ECS service, target groups, and listeners, and swap ports\nMore complex\nCreate new Amazon ECS service, target groups, load balancer, and listeners, and change the routing layer config\nRisk mitigation options\rMedium\nNo parallel blue/green setup available for testing. Focus on process automation and test\nStrong\nParallel blue/green setup, test the new setup before re-routing the traffic\nStrong\nParallel blue/green setup, test the new setup before re-routing the traffic\nDeployment controller rollback\rSimple\nChange the service deployment controller back to CODE_DEPLOY\nSimple\nReverse the ports swap\nSimple\nRollback the routing layer config changes\nDowntime\rNo down time\rMinimal disruption during ports swap\rNo down time\rApplicability\rNo constraints\rNo constraints\rRequires additional routing layer\rCost\rNo additional cost\rAdditional cost\nTwo co-existing Amazon ECS services with associated tasks\nAdditional cost\nTwo co-existing Amazon ECS services with associated tasks, and an added load balancer\nConclusion In this post, we discussed migrating from AWS CodeDeploy to Amazon ECS for blue/green deployments. This discussion included the following:\nfactors to consider before deciding to migrate,\nkey architectural differences, and associated implementation considerations,\nthree different ways to approach migration.\nIf you’re currently using CodeDeploy and are considering a move to ECS blue/green deployments, then you can use this post as a guide to assess feasibility and plan your migration. For more information about ECS blue/green deployments, check out the Amazon ECS developer guide.\nAbout the authors Mike Rizzo Mike Rizzo is a Principal Solutions Architect in the AWS UK Financial Services team. He has a keen interest in application modernization, and specifically use of containers, serverless, and artificial intelligence to enable this in the cloud. In his spare time, you’ll find him running and cycling around the Suffolk countryside, cooking Maltese food, and playing Fortnite!\nIslam Mahgoub Islam Mahgoub is a Senior Solutions Architect at AWS with over 15 years of experience in application, integration, and technology architecture. At AWS, he helps customers build new cloud-centered solutions and modernize their legacy applications using AWS services. Outside of work, Islam enjoys walking, watching movies, and listening to music.\nOlly Pomeroy Olly Pomeroy is a Senior Container Specialist Solution Architect at AWS.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Break down data silos and seamlessly query Iceberg tables in Amazon SageMaker from Snowflake by Nidhi Gupta and Andries Engelbrecht on 15 SEP 2025 in Advanced (300), Amazon SageMaker Lakehouse, Amazon Simple Storage Service (S3), AWS Glue, AWS Lake Formation, Partner solutions, S3 Select, Technical How-to Permalink Comments Share\nOrganizations often struggle to unify their data ecosystems across multiple platforms and services. The connectivity between Amazon SageMaker and Snowflake’s AI Data Cloud offers a powerful solution to this challenge, so businesses can take advantage of the strengths of both environments while maintaining a cohesive data strategy.\nIn this post, we demonstrate how you can break down data silos and enhance your analytical capabilities by querying Apache Iceberg tables in the lakehouse architecture of SageMaker directly from Snowflake. With this capability, you can access and analyze data stored in Amazon Simple Storage Service (Amazon S3) through AWS Glue Data Catalog using an AWS Glue Iceberg REST endpoint, all secured by AWS Lake Formation, without the need for complex extract, transform, and load (ETL) processes or data duplication. You can also automate table discovery and refresh using Snowflake catalog-linked databases for Iceberg. In the following sections, we show how to set up this integration so Snowflake users can seamlessly query and analyze data stored in AWS, thereby improving data accessibility, reducing redundancy, and enabling more comprehensive analytics across your entire data ecosystem.\nBusiness use cases and key benefits The capability to query Iceberg tables in SageMaker from Snowflake delivers significant value across multiple industries:\nFinancial services – Enhance fraud detection through unified analysis of transaction data and customer behavior patterns Healthcare – Improve patient outcomes through integrated access to clinical, claims, and research data Retail – Increase customer retention rates by connecting sales, inventory, and customer behavior data for personalized experiences Manufacturing – Boost production efficiency through unified sensor and operational data analytics Telecommunications – Reduce customer churn with comprehensive analysis of network performance and customer usage data Key benefits of this capability include:\nAccelerated decision-making – Reduce time to insight through integrated data access across platforms Cost optimization – Accelerate time to insight by querying data directly in storage without the need for ingestion Improved data fidelity – Reduce data inconsistencies by establishing a single source of truth Enhanced collaboration – Increase cross-functional productivity through simplified data sharing between data scientists and analysts By using the lakehouse architecture of SageMaker with Snowflake’s serverless and zero-tuning computational power, you can break down data silos, enabling comprehensive analytics and democratizing data access. This integration supports a modern data architecture that prioritizes flexibility, security, and analytical performance, ultimately driving faster, more informed decision-making across the enterprise.\nSolution overview The following diagram shows the architecture for catalog integration between Snowflake and Iceberg tables in the lakehouse.\nThe workflow consists of the following components:\nData storage and management: Amazon S3 serves as the primary storage layer, hosting the Iceberg table data The Data Catalog maintains the metadata for these tables Lake Formation provides credential vending Authentication flow: Snowflake initiates queries using a catalog integration configuration Lake Formation vends temporary credentials through AWS Security Token Service (AWS STS) These credentials are automatically refreshed based on the configured refresh interval Query flow: Snowflake users submit queries against the mounted Iceberg tables The AWS Glue Iceberg REST endpoint processes these requests Query execution uses Snowflake’s compute resources while reading directly from Amazon S3 Results are returned to Snowflake users while maintaining all security controls There are four patterns to query Iceberg tables in SageMaker from Snowflake:\nIceberg tables in an S3 bucket using an AWS Glue Iceberg REST endpoint and Snowflake Iceberg REST catalog integration, with credential vending from Lake Formation Iceberg tables in an S3 bucket using an AWS Glue Iceberg REST endpoint and Snowflake Iceberg REST catalog integration, using Snowflake external volumes to Amazon S3 data storage Iceberg tables in an S3 bucket using AWS Glue API catalog integration, also using Snowflake external volumes to Amazon S3 Amazon S3 Tables using Iceberg REST catalog integration with credential vending from Lake Formation In this post, we implement the first of these four access patterns using catalog integration for the AWS Glue Iceberg REST endpoint with Signature Version 4 (SigV4) authentication in Snowflake.\nPrerequisites You must have the following prerequisites:\nA Snowflake account. An AWS Identity and Access Management (IAM) role that is a Lake Formation data lake administrator in your AWS account. A data lake administrator is an IAM principal that can register Amazon S3 locations, access the Data Catalog, grant Lake Formation permissions to other users, and view AWS CloudTrail. See Create a data lake administrator for more information. An existing AWS Glue database named iceberg_db and Iceberg table named customer with data stored in an S3 general purpose bucket with a unique name. To create the table, refer to the table schema and dataset. A user-defined IAM role that Lake Formation assumes when accessing the data in the aforementioned S3 location to vend scoped credentials (see Requirements for roles used to register locations). For this post, we use the IAM role LakeFormationLocationRegistrationRole. The solution takes approximately 30–45 minutes to set up. Cost varies based on data volume and query frequency. Use the AWS Pricing Calculator for specific estimates.\nCreate an IAM role for Snowflake To create an IAM role for Snowflake, you first create a policy for the role:\nOn the IAM console, choose Policies in the navigation pane. Choose Create policy. Choose the JSON editor and enter the following policy (provide your AWS Region and account ID), then choose Next. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGlueCatalogTableAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetCatalog\u0026#34;, \u0026#34;glue:GetCatalogs\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:GetPartition\u0026#34;, \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;region\\\u0026gt;:\u0026lt;account-id\\\u0026gt;:catalog\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;region\\\u0026gt;:\u0026lt;account-id\\\u0026gt;:database/iceberg_db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;region\\\u0026gt;:\u0026lt;account-id\\\u0026gt;:table/iceberg_db/\\*\u0026#34;, ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lakeformation:GetDataAccess\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } Enter iceberg-table-access as the policy name. Choose Create policy. Now you can create the role and attach the policy you created.\nChoose Roles in the navigation pane. Choose Create role. Choose AWS account. Under Options, select Require External Id and enter an external ID of your choice. Choose Next. Choose the policy you created (iceberg-table-access policy). Enter snowflake_access_role as the role name. Choose Create role. Configure Lake Formation access controls To configure your Lake Formation access controls, first set up the application integration:\nSign in to the Lake Formation console as a data lake administrator. Choose Administration in the navigation pane. Select Application integration settings. Enable Allow external engines to access data in Amazon S3 locations with full table access. Choose Save. Now you can grant permissions to the IAM role.\nChoose Data permissions in the navigation pane. Choose Grant. Configure the following settings: For Principals, select IAM users and roles and choose snowflake_access_role. For Resources, select Named Data Catalog resources. For Catalog, choose your AWS account ID. For Database, choose iceberg_db. For Table, choose customer. For Permissions, select SUPER. Choose Grant. SUPER access is required for mounting the Iceberg table in Amazon S3 as a Snowflake table.\nRegister the S3 data lake location Complete the following steps to register the S3 data lake location:\nAs data lake administrator on the Lake Formation console, choose Data lake locations in the navigation pane. Choose Register location. Configure the following: For S3 path, enter the S3 path to the bucket where you will store your data. For IAM role, choose LakeFormationLocationRegistrationRole. For Permission mode, choose Lake Formation. Choose Register location. Set up the Iceberg REST integration in Snowflake Complete the following steps to set up the Iceberg REST integration in Snowflake:\nLog in to Snowflake as an admin user. Execute the following SQL command (provide your Region, account ID, and external ID that you provided during IAM role creation): CREATE OR REPLACE CATALOG INTEGRATION glue_irc_catalog_int CATALOG_SOURCE = ICEBERG_REST TABLE_FORMAT = ICEBERG CATALOG_NAMESPACE = \u0026#39;iceberg_db\u0026#39; REST_CONFIG = ( CATALOG_URI = \u0026#39;https://glue.\u0026lt;region\u0026gt;.amazonaws.com/iceberg\u0026#39; CATALOG_API_TYPE = AWS_GLUE CATALOG_NAME = \u0026#39;\u0026lt;account-id\u0026gt;\u0026#39; ACCESS_DELEGATION_MODE = VENDED_CREDENTIALS ) REST_AUTHENTICATION = ( TYPE = SIGV4 SIGV4_IAM_ROLE = \u0026#39;arn:aws:iam::\u0026lt;account-id\u0026gt;:role/snowflake_access_role\u0026#39; SIGV4_SIGNING_REGION = \u0026#39;\u0026lt;region\u0026gt;\u0026#39; SIGV4_EXTERNAL_ID = \u0026#39;\u0026lt;external-id\u0026gt;\u0026#39; ) REFRESH_INTERVAL_SECONDS = 120 ENABLED = TRUE; Execute the following SQL command and retrieve the value for API_AWS_IAM_USER_ARN: DESCRIBE CATALOG INTEGRATION glue_irc_catalog_int; On the IAM console, update the trust relationship for snowflake_access_role with the value for API_AWS_IAM_USER_ARN: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;\u0026lt;API_AWS_IAM_USER_ARN\u0026gt;\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;sts:ExternalId\u0026#34;: [ \u0026#34;\u0026lt;external-id\u0026gt;\u0026#34; ] } } } \\] } Verify the catalog integration: SELECT SYSTEM$VERIFY_CATALOG_INTEGRATION(\u0026#39;glue_irc_catalog_int\u0026#39;); Mount the S3 table as a Snowflake table: CREATE OR REPLACE ICEBERG TABLE s3iceberg_customer CATALOG = \u0026#39;glue_irc_catalog_int\u0026#39; CATALOG_NAMESPACE = \u0026#39;iceberg_db\u0026#39; CATALOG_TABLE_NAME = \u0026#39;customer\u0026#39; AUTO_REFRESH = TRUE; Query the Iceberg table from Snowflake To test the configuration, log in to Snowflake as an admin user and run the following sample query:\nSELECT * FROM s3iceberg_customer LIMIT 10; Clean up To clean up your resources, complete the following steps:\nDelete the database and table in AWS Glue. Drop the Iceberg table, catalog integration, and database in Snowflake: DROP ICEBERG TABLE iceberg_customer; DROP CATALOG INTEGRATION glue_irc_catalog_int; Make sure all resources are properly cleaned up to avoid unexpected charges.\nConclusion In this post, we demonstrated how to establish a secure and efficient connection between your Snowflake environment and SageMaker to query Iceberg tables in Amazon S3. This capability can help your organization maintain a single source of truth while also letting teams use their preferred analytics tools, ultimately breaking down data silos and enhancing collaborative analysis capabilities.\nTo further explore and implement this solution in your environment, consider the following resources:\nTechnical documentation: Review the Amazon SageMaker Lakehouse User Guide Explore Security in AWS Lake Formation for best practices to optimize your security controls Learn more about Iceberg table format and its benefits for data lakes Refer to Configuring secure access from Snowflake to Amazon S3 Related blog posts: Build real-time data lakes with Snowflake and Amazon S3 Tables Simplify data access for your enterprise using Amazon SageMaker Lakehouse These resources can help you to implement and optimize this integration pattern for your specific use case. As you begin this journey, remember to start small, validate your architecture with test data, and gradually scale your implementation based on your organization’s needs.\nAbout the authors Nidhi Gupta Nidhi is a Senior Partner Solutions Architect at AWS, specializing in data and analytics. She helps customers and partners build and optimize Snowflake workloads on AWS. Nidhi has extensive experience leading production releases and deployments, with focus on Data, AI, ML, generative AI, and Advanced Analytics.\nAndries Engelbrecht Andries is a Principal Partner Solutions Engineer at Snowflake working with AWS. He supports product and service integrations, as well the development of joint solutions with AWS. Andries has over 25 years of experience in the field of data and analytics.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Navigating Amazon GuardDuty protection plans and Extended Threat Detection by Nisha Amthul, Shachar Hirshberg, and Sujay Doshi on 15 SEP 2025 in Amazon GuardDuty, Intermediate (200), Security, Identity, \u0026amp; Compliance Permalink Comments Share\nOrganizations are innovating and growing their cloud presence to deliver better customer experiences and drive business value. To support and protect this growth, organizations can use Amazon GuardDuty, a threat detection service that continuously monitors for malicious activity and unauthorized behavior across your AWS environment. GuardDuty uses artificial intelligence (AI), machine learning (ML), and anomaly detection using both AWS and industry-leading threat intelligence to help protect your AWS accounts, workloads, and data. Building on these foundational capabilities, GuardDuty offers a comprehensive suite of protection plans and the Extended Threat Detection feature.\nIn this post, we explore how to use these features to provide robust security coverage for your AWS workloads, helping you detect sophisticated threats across your AWS environment.\nUnderstanding GuardDuty protection plans GuardDuty starts with foundational security monitoring, which analyzes AWS CloudTrail management events, Amazon Virtual Private Cloud (Amazon VPC) Flow Logs, and DNS logs. Building on this foundation, GuardDuty offers several protection plans that extend its threat detection capabilities to additional AWS services and data sources. These protection plans are optional features that analyze data from specific AWS services in your environment to provide enhanced security coverage. GuardDuty offers the flexibility to customize how new accounts inherit protection plans, so you can add coverage for your accounts or select specific accounts based on your security needs. You can enable or disable these protection plans at any time to align with your evolving workload requirements.\nHere are the available GuardDuty protection plans and their capabilities:\nGuardDuty protection plan Description S3 Protection Identifies potential security risks such as data exfiltration and destruction attempts in your Amazon Simple Storage Service (Amazon S3) buckets. EKS Protection EKS audit log monitoring analyzes Kubernetes audit logs from your Amazon Elastic Kubernetes Service (Amazon EKS) clusters for potentially suspicious and malicious activities. Runtime Monitoring Monitors and analyzes operating system-level events on your Amazon EKS, Amazon Elastic Compute Cloud (Amazon EC2), and Amazon Elastic Container Service (Amazon ECS) (including AWS Fargate), to detect potential runtime threats. Malware Protection for EC2 Detects the potential presence of malware by scanning the Amazon Elastic Block Store (Amazon EBS) volumes associated with your EC2 instances. There is an option to use this feature on-demand. Malware Protection for S3 Detects the potential presence of malware in the newly uploaded objects within your S3 buckets. RDS Protection Analyzes and profiles your RDS login activity for potential access threats to the supported Amazon Aurora and Amazon Relational Database Service (Amazon RDS) databases. Lambda Protection Monitors AWS Lambda network activity logs, starting with VPC Flow Logs, to detect threats to your Lambda functions. Examples of these potential threats include crypto mining and communicating with malicious servers. Let’s explore how these protection plans help secure different aspects of your AWS environment.\nS3 Protection S3 Protection extends threat detection capabilities of GuardDuty to your S3 buckets by monitoring object-level API operations. Beyond basic monitoring, it analyzes patterns of behavior to detect sophisticated threats. When a threat actor attempts to exfiltrate data, GuardDuty can detect unusual sequences of API calls, such as ListBucket operations followed by suspicious GetObject requests from unusual locations. It also identifies potential security risks like attempts to disable S3 server access logging or unauthorized changes to bucket policies that could indicate an attempt to make buckets public. For instance, GuardDuty would generate an UnauthorizedAccess finding if it detects these suspicious API calls originating from known malicious IP addresses.\nEKS Protection For containerized workloads, EKS Protection monitors your Amazon EKS clusters’ control plane audit logs for security threats. It’s specifically designed to detect container-based exploits by analyzing Kubernetes audit logs from your EKS clusters. GuardDuty detects scenarios such as containers deployed with suspicious characteristics (like known malicious images), attempted privilege escalation through role binding modifications, and suspicious service account activities that could indicate compromise of your Kubernetes environment. When detecting such activities, GuardDuty would generate a PrivilegeEscalation finding, alerting you to potential unauthorized access attempts within your clusters. For a comprehensive understanding of the tactics, techniques, and procedures (TTPs), see the AWS Threat Technique Catalog.\nRuntime Monitoring Runtime Monitoring provides deeper visibility into potential threats by analyzing runtime behavior in EC2 instances, EKS clusters, and container workloads. This capability detects threats that manifest at the operating system level by monitoring process executions, file system changes, and network connections. GuardDuty can identify defense evasion tactics, execution of suspicious processes, and file access patterns indicating potential malware activity. For example, if a compromised instance attempts to disable security monitoring or creates unusual processes, GuardDuty would generate a Runtime finding indicating potential malicious activity at the OS level.\nMalware Protection Malware Protection offers two distinct capabilities: scanning EBS volumes attached to EC2 instances and scanning objects uploaded to S3 buckets. For EC2 instances, GuardDuty can perform both agentless scan-on-demand and continuous scanning of EBS volumes, detecting both known malware and potentially malicious files using advanced heuristics. For S3, it automatically scans newly uploaded objects, helping protect against malware distribution through your S3 buckets. When malware is detected, GuardDuty generates a Malware finding, specifying whether the threat was found in an EC2 instance or S3 bucket, helping you quickly identify and respond to the threat.\nRDS Protection RDS Protection focuses on database security by analyzing login activity for supported Amazon Aurora databases. It creates behavioral baselines of normal database access patterns and can detect anomalous sign-in attempts that might indicate unauthorized access attempts. This includes detecting unusual sign-in patterns, access from unexpected locations, and potential database compromise attempts. When suspicious database access is detected, GuardDuty generates an RDS finding, alerting you to potential unauthorized access or credential compromise.\nLambda Protection Lambda Protection monitors your serverless applications by analyzing Lambda function activity through VPC Flow Logs. It can detect threats specific to serverless environments, such as when Lambda functions exhibit signs of compromise through unexpected network connections or potential cryptocurrency mining activity. If a Lambda function attempts to communicate with known malicious IP addresses or shows signs of cryptojacking, GuardDuty will generate a Lambda finding, so you can quickly identify and remediate compromised functions.\nEach protection plan adds specialized detection capabilities designed for specific workload types, working together to provide comprehensive threat detection across your AWS environment. By enabling the protection plans relevant to your workloads, you can help make sure that GuardDuty provides targeted security monitoring for your specific use cases\nTailoring GuardDuty protection plans to your workload types To maximize threat detection coverage, consider enabling all applicable GuardDuty protection plans across your AWS environment. This approach helps provide comprehensive coverage while maintaining cost efficiency, because you’re only charged for active protections on resources that exist in your account. For example, if you don’t use Amazon EKS, you won’t incur charges for EKS Protection even if it’s enabled. This strategy also helps facilitate automatic security coverage if teams deploy new services, without requiring immediate security team intervention. You retain the flexibility to adjust your protection plans at any time as your workload requirements evolve.\nBased on AWS security best practices, we offer recommendations for different protection plan combinations aligned with common workload profiles. These recommendations help you understand how different protection plans work together to secure your specific architectures. For Amazon EC2 and Amazon S3 workloads, GuardDuty recommends Foundational, Amazon S3 Protection, and Amazon GuardDuty Malware Protection for Amazon EC2 to detect threats to compute instances, data storage, and AWS Identity and Access Management (IAM) misuse.\nContainer-heavy environments using Amazon EKS and Amazon ECS benefit from Foundational, Amazon EKS Protection, Amazon GuardDuty Runtime Monitoring, and Amazon GuardDuty Malware Protection for Amazon EC2. These plans work together to monitor container control-plane and runtime for threats and malware.\nFor serverless-first architectures built on Lambda, GuardDuty suggests Foundational, AWS Lambda Protection, and Amazon S3 Protection (if using Amazon S3 triggers) to identify anomalous function behavior and suspicious traffic patterns.\nData systems using Amazon Aurora or Amazon RDS should consider Foundational, Amazon RDS Protection, Amazon S3 Protection, and Amazon GuardDuty Malware Protection for Amazon S3. This combination helps detect anomalous database sign-ins and potential S3 bucket misuse.\nFor regulated environments or those implementing zero-trust architectures, enabling all GuardDuty protection plans helps provide comprehensive threat detection coverage that can support your broader security monitoring and compliance program requirements.\nFor quick reference, here’s what protection plans you should use to actively monitor your different workload types:\nWorkload profile Expected security outcomes Recommended GuardDuty plans Amazon EC2 and Amazon S3 Detect threats to compute instances, data storage, and IAM misuse Foundational, Amazon S3 Protection, and Amazon GuardDuty Malware Protection for Amazon EC2 Container-heavy (Amazon EKS, Amazon ECS) Monitor container control-plane and runtime for threats and malware Foundational, Amazon EKS Protection, Amazon GuardDuty Runtime Monitoring, and Amazon GuardDuty Malware Protection for Amazon EC2 Serverless-first (AWS Lambda) Identify anomalous function behavior and suspicious traffic patterns Foundational, GuardDuty Lambda Protection, GuardDuty S3 Protection (if using Amazon S3 triggers), and GuardDuty Runtime Monitoring for ECS on Fargate Data system (Amazon Aurora or Amazon RDS) Detect anomalous database logins and potential S3 bucket misuse Foundational, Amazon RDS Protection, GuardDuty S3 Protection, and Amazon GuardDuty Malware Protection for Amazon S3 Regulated and Zero-Trust Comprehensive threat detection to support compliance requirements All Amazon GuardDuty protection plans The power of GuardDuty Extended Threat Detection Building upon these protection plans, GuardDuty offers Extended Threat Detection by default at no additional cost, using AI/ML capabilities to provide improved threat detection for your applications, workloads, and data. This capability correlates security signals to identify active threat sequences, offering a more comprehensive approach to cloud security.\nExtended Threat Detection includes a Critical severity level for the most urgent and high-confidence threats based on correlating multiple steps taken by adversaries, such as privilege discovery, API manipulation, persistence activities, and data exfiltration. Integration with the MITRE ATT\u0026amp;CK® framework allows GuardDuty to map observed activities to tactics and techniques, providing context for security teams. To help teams respond quickly, GuardDuty provides specific remediation recommendations based on AWS best practices for each identified threat.\nReal-world protection: Extended Threat Detection in action To understand how GuardDuty protection plans and Extended Threat Detection work together in practice, let’s examine two sophisticated threat scenarios that security teams commonly face: data compromise and container cluster compromise.\nData compromise detection GuardDuty Extended Threat Detection continuously analyzes and correlates events across multiple protection plans, providing comprehensive visibility when data compromise attempts occur in Amazon S3. For example, in a recent incident, GuardDuty identified a critical severity attack sequence spanning 24 hours. The sequence began with discovery actions through unusual S3 API calls, progressed to defense evasion through CloudTrail modifications, and culminated in potential data exfiltration attempts.\nDuring the discovery phase, S3 Protection detected an IAM role making unusual ListBuckets and GetObject API calls across multiple buckets—a significant deviation from their normal pattern of accessing only specific assigned buckets. Extended Threat Detection then correlated this suspicious activity with subsequent actions from the same IAM role: attempts to disable CloudTrail logging and modify bucket policies (classic signs of defense evasion), followed by the creation of new access keys. This connected sequence of events, all from the same identity, indicated a progressing exploit moving from initial discovery to establishing persistence through credential creation.\nContainer environment compromise Protecting containerized environments requires visibility across multiple layers of your Amazon EKS infrastructure. GuardDuty combines signals from EKS control plane (through EKS Protection), container runtime behavior (through Runtime Monitoring), and foundational infrastructure logs to provide comprehensive threat detection for your Kubernetes clusters. For example, EKS Protection detects suspicious activities at the Kubernetes control plane level, such as unusual kubernetes API server authentication attempts or the creation of service accounts with elevated permissions. Runtime Monitoring provides visibility into container behavior, identifying unexpected privileged commands or suspicious file system access. Together with foundational logs, these components provide multi-layer threat detection for your container workloads.\nHere’s how these components worked together in detecting an attack sequence: The exploit began when EKS Protection detected unusual Kubernetes API server authentication attempts from a container within the cluster. Runtime Monitoring simultaneously observed commands that deviated from the container’s baseline behavior, such as privilege escalation attempts and unauthorized system calls. As the exploit progressed, GuardDuty detected the creation of a Kubernetes service account with elevated permissions, followed by attempts to mount sensitive host paths to containers.\nThe scenario then escalated when the compromised Kubernetes Pod established connections to other Pods across namespaces, suggesting lateral movement. GuardDuty Extended Threat Detection correlated these events with the Pod accessing sensitive Kubernetes secrets and AWS credentials stored in Kubernetes ConfigMaps. The final stage revealed the compromised Pod making AWS API calls using stolen credentials, targeting resources outside the cluster’s normal operational scope.\nThe detection of this multi-stage attack, spanning container exploitation, privilege escalation, and credential theft, demonstrates the power of the correlation capabilities of Extended Threat Detection. Security teams received a single critical finding that mapped the entire exploit sequence to MITRE ATT\u0026amp;CK® tactics, providing clear visibility into the exploit progression and specific remediation steps.\nThese real-world scenarios illustrate how GuardDuty protection plans work in concert with Extended Threat Detection to provide deep security insights. The combination of targeted protection plans and AI-powered correlation helps security teams identify and respond to sophisticated threats that might otherwise go unnoticed or be difficult to piece together manually.\nConclusion GuardDuty protection plans, coupled with its built-in Extended Threat Detection feature, offer a powerful suite of managed detections to secure your AWS environment. By tailoring your security strategy to your specific workload types and using AI-powered insights, you can significantly enhance your ability to detect and respond to sophisticated threats. To get started with GuardDuty protection plans and Extended Threat Detection, visit the GuardDuty console. Each protection plan includes a 30-day trial at no additional cost per AWS account and AWS Region, allowing you to evaluate the security coverage for your specific needs. Remember, you can adjust your enabled plans at any time to align with your evolving security requirements and workload changes. By using these capabilities, you can strengthen your organization’s threat detection and response in the face of evolving security risks.\nNisha Amthul Nisha is a Senior Product Marketing Manager at AWS Security, specializing in detection and response solutions. She has a strong foundation in product management and product marketing within the domains of information security and data protection. When not at work, you’ll find her cake decorating, strength training, and chasing after her two energetic kiddos.\nSujay Doshi Sujay is a Senior Product Manager at AWS, focusing on security services. With over 10 years of experience in product management and software development, he leads the product strategy for Amazon GuardDuty. Prior to AWS, Sujay held leadership roles at various technology companies. He’s passionate about cloud security and describes himself as “a data nerd with a penchant for finding needles in the cyber haystack.\nShachar Hirshberg Shachar was a Senior Product Manager for Amazon GuardDuty with over a decade of experience in building, designing, launching, and scaling enterprise software. He is passionate about further improving how customers harness AWS services to enable innovation and enhance the security of their cloud environments. Outside of work, Shachar is an avid traveler and a skiing enthusiast.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.4-api-gateway-integration/5.4.1-create-http-api/","title":"Create HTTP API","tags":[],"description":"","content":"Create HTTP API In this step, you will create an HTTP API that will serve as the endpoint for clients to send questions to your Lambda function.\n🔹 Step 1 — Go to API Gateway and click Create API Open the Amazon API Gateway Console and select Create API.\n🔹 Step 2 — Select Build under HTTP API Choose:\nBuild → HTTP API\n🔹 Step 3 — Rename the API Set:\nAPI name: bedrock-chatbot-api Click Next, then Create to finish creating the API.\n🎯 Result You have successfully created an HTTP API, and it is now ready to be integrated with your Lambda function in the next step.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.1-create-lambda/","title":"Create Lambda Function","tags":[],"description":"","content":"Create Lambda Function In this step, you will create a new AWS Lambda function that will receive requests from the client and send prompts to Amazon Bedrock using the Converse API.\n🔹 Step 1 — Open the Lambda creation page Go to the AWS Lambda Console Select Functions Click Create function 🔹 Step 2 — Set the name and configuration for the Lambda function Configure the function as follows:\nFunction name: bedrock-chatbot-lambda Runtime: Python 3.12 (recommended and stable for this workshop) Architecture: x86_64 or arm64 (both are supported) Permissions → Use an existing role Select the role you created in the Prerequisites section, for example:\nlambda-bedrock-role Then click Create function.\n🎯 Expected result You now have an empty Lambda function that:\nIs ready for adding the Converse API invocation code Has an execution role with Bedrock permissions + CloudWatch logging Can be tested directly in the Lambda Console In the next section, you will configure the IAM role further and prepare the source code to call the model.\nContinue to 5.3.2 – Add code to call the Converse API.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.1-week1/1.1.1-day01-2025-09-08/","title":"Day 01 - Introduction to Cloud Computing","tags":[],"description":"","content":"Date: 2025-09-08 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes What Is Cloud Computing? The on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Benefits of Cloud Computing Pay only for what you use, optimizing cost efficiency. Accelerate development through automation and managed services. Scale resources up or down as needed. Deploy applications globally in minutes. Why AWS? AWS has been the global cloud leader for 13 consecutive years (as of 2023). Unique culture, vision, and long-term customer obsession. AWS pricing philosophy: customers should pay less over time for the same resources. Every AWS Leadership Principle is focused on delivering real customer value. How to Get Started with AWS There are many learning paths—self-study is completely possible. Register an AWS Free Tier account to explore. Recommended course platforms: Udemy A Cloud Guru Explore AWS learning paths: AWS Learning Paths "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.2-week2/1.2.1-day06-2025-09-15/","title":"Day 06 - Amazon VPC Fundamentals","tags":[],"description":"","content":"Date: 2025-09-15 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Networking Services on AWS Amazon Virtual Private Cloud (VPC) Amazon Virtual Private Cloud (Amazon VPC) allows you to launch AWS resources into a virtual network you define. A VPC exists within a single Region. When creating a VPC, you must define an IPv4 CIDR block (required) and optionally an IPv6 one. The default limit is 5 VPCs per Region per Account. Commonly used to separate environments such as Production, Development, and Staging. To achieve full resource isolation, use separate AWS Accounts rather than multiple VPCs. Subnets A subnet resides within one Availability Zone. The subnet CIDR must be a subset of the parent VPC\u0026rsquo;s CIDR block. AWS reserves 5 IP addresses in each subnet: network, broadcast, router, DNS, and future use. Reserved IP Addresses Example (10.0.0.0/24):\n10.0.0.0 - Network address 10.0.0.1 - VPC router 10.0.0.2 - DNS server 10.0.0.3 - Reserved for future use 10.0.0.255 - Broadcast address Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking Basics Create VPC → 03-03.1 Create Subnet → 03-03.2 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.3-week3/1.3.1-day11-2025-09-22/","title":"Day 11 - Amazon EC2 Fundamentals","tags":[],"description":"","content":"Date: 2025-09-22 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Compute on AWS Amazon Elastic Compute Cloud (EC2) Amazon EC2 provides resizable compute capacity in the cloud, similar to a virtual or physical server. It supports workloads such as web hosting, applications, databases, authentication services, and other general-purpose server tasks. Instance Types\nEC2 configurations are defined by instance types, not custom hardware. Each type specifies: CPU (Intel, AMD, ARM – Graviton 1/2/3) / GPU Memory Network Storage Instance Type Categories:\nGeneral Purpose: T3, T4g, M5, M6i (balanced compute, memory, networking) Compute Optimized: C5, C6i, C7g (high-performance processors) Memory Optimized: R5, R6i, X2 (fast performance for memory-intensive workloads) Storage Optimized: I3, D2, H1 (high sequential read/write to local storage) Accelerated Computing: P4, G5, Inf1 (GPU/FPGA for ML, graphics) Amazon Machine Images (AMI) AMI (Amazon Machine Image) is a template that defines the software configuration of an instance, including OS, apps, and settings. Types of AMIs: Provided by AWS (Amazon Linux, Windows, Ubuntu, etc.) AWS Marketplace AMIs Custom AMIs created by users Benefits of Custom AMIs\nFaster instance launch and setup Simplified backup and restore Consistent environment across multiple instances AMI Components:\nRoot volume template (OS and applications) Launch permissions Block device mapping Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Create an AWS Account → 01-01 Setup Virtual MFA Device → 01-02 Create Admin Group and Admin User → 01-03 Account Authentication Support → 01-04 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.4-week4/1.4.1-day16-2025-09-29/","title":"Day 16 - Amazon S3 Fundamentals","tags":[],"description":"","content":"Date: 2025-09-29 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Storage Services on AWS Amazon Simple Storage Service (S3) Amazon S3 is an object storage service designed to store and retrieve any amount of data from anywhere on the web. It offers virtually unlimited scalability, high availability, strong security, and excellent performance.\nCore S3 Features Buckets and Objects: Data is stored as objects inside buckets. Each object can be up to 5 TB. Availability and Durability: S3 is designed for 99.99% availability and 99.999999999% (11 nines) durability. Security: Multiple layers of security including IAM, bucket policies, ACLs, and encryption. Scalability: Automatically scales storage and request throughput without performance degradation. S3 Object Structure:\nKey: Object name/path Value: Object data Version ID: For versioning Metadata: System and user metadata Access Control: Permissions S3 Access Points Access Points simplify managing data access for shared datasets in S3.\nPer-application access control: Each access point has its own policy. Operational simplicity: Eases permission management for shared datasets used by many applications. Network controls: Can be configured to accept requests only from specific VPCs. S3 Storage Classes Choose among storage classes optimized for different access patterns and cost profiles:\nS3 Standard: For frequently accessed data; highest availability and performance. S3 Intelligent-Tiering: Automatically moves objects between tiers to optimize cost. S3 Standard-IA (Infrequent Access): Lower cost for infrequently accessed data with millisecond retrieval. S3 One Zone-IA: Like Standard-IA but stored in a single AZ. S3 Glacier Flexible Retrieval: Low-cost archival with minutes-to-hours retrieval. S3 Glacier Deep Archive: Lowest-cost archival with ~12-hour retrieval. Storage Class Comparison:\nClass Durability Availability Min Storage Retrieval Standard 11 9\u0026rsquo;s 99.99% None Instant Intelligent-Tiering 11 9\u0026rsquo;s 99.9% None Instant Standard-IA 11 9\u0026rsquo;s 99.9% 30 days Instant One Zone-IA 11 9\u0026rsquo;s 99.5% 30 days Instant Glacier Flexible 11 9\u0026rsquo;s 99.99% 90 days Minutes-hours Glacier Deep Archive 11 9\u0026rsquo;s 99.99% 180 days 12 hours Hands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Part 1) Create S3 Bucket → 57-2.1 Load Data → 57-2.2 Enable Static Website → 57-3 Configure Public Access Block → 57-4 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.5-week5/1.5.1-day21-2025-10-06/","title":"Day 21 - Shared Responsibility &amp; IAM Basics","tags":[],"description":"","content":"Date: 2025-10-06 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Security Shared Responsibility Model In cloud computing, security is a shared responsibility between the cloud provider and the customer. Customers must securely configure services, apply best practices, and use security controls from the hypervisor exposure upward to application/data layers. The split of responsibilities varies by service model: Infrastructure-level services Partially managed services Fully managed services AWS Responsibilities (Security OF the Cloud):\nPhysical security of data centers Hardware and network infrastructure Virtualization infrastructure Managed service operations Customer Responsibilities (Security IN the Cloud):\nData encryption Network configuration Access management Application security Operating system patches (for EC2) AWS Identity and Access Management (IAM) Root Account Has unrestricted access to all AWS services/resources and can remove any attached permissions. Best practices: Create and use an IAM Administrator user for daily operations. Lock away root credentials (dual control). Keep the root user\u0026rsquo;s email and domain valid and renewed. Enable MFA on root account IAM Overview IAM controls access to AWS services and resources in your account. Principals include: root user, IAM users, federated users, IAM roles, assumed-role sessions, AWS services, and anonymous users. Notes: IAM users are not separate AWS accounts. New IAM users start with no permissions. Grant permissions by attaching policies to users, groups, or roles. Use IAM groups to manage many users (groups cannot be nested). Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Part 1) Create EC2 Instance → 48-1.1 Create S3 Bucket → 48-1.2 Generate IAM User and Access Key → 48-2.1 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.6-week6/1.6.1-day26-2025-10-13/","title":"Day 26 - Database Fundamentals","tags":[],"description":"","content":"Date: 2025-10-13 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Database Concepts Review A database is an organized (or semi-structured) collection of information stored on storage devices to support concurrent access by multiple users or programs with different goals. Sessions A session spans from the moment a client connects to the DBMS until the connection is terminated. Primary Key A primary key uniquely identifies each row in a relational table. Foreign Key A foreign key in one table references the primary key of another table, creating a relationship between them. Index An index accelerates data retrieval at the cost of extra writes and storage to maintain the index structure. Indexes locate data without scanning every row; they can be defined over one or more columns. Index Types:\nB-Tree: General purpose, balanced tree structure Hash: Fast equality lookups Bitmap: Efficient for low-cardinality columns Full-Text: Text search optimization Partitioning Partitioning splits a large table into smaller, independent pieces (partitions), potentially placed on different storage. Benefits: better query performance, easier maintenance, and scalability. Common types: Range (e.g., by date) List Hash Composite (combination) Partitioning Example:\n-- Range partitioning by date CREATE TABLE orders ( order_id INT, order_date DATE, amount DECIMAL ) PARTITION BY RANGE (YEAR(order_date)) ( PARTITION p2023 VALUES LESS THAN (2024), PARTITION p2024 VALUES LESS THAN (2025), PARTITION p2025 VALUES LESS THAN (2026) ); Execution Plan / Query Plan A query plan details how the DBMS will execute an SQL statement (access paths, joins, sorts). Types: Estimated plan (before execution) Actual plan (from executed query) Key operators: table scan, index seek/scan, nested loops, hash/merge join, sort, aggregate, filter. Database Logs Database logs record all changes (INSERT/UPDATE/DELETE) and operations. Typical log types: transaction, redo, undo, binary logs. Uses: recovery, integrity, consistency/durability (ACID), replication, performance analysis. Buffers A buffer pool caches pages read from disk to minimize I/O. Management strategies: Replacement: LRU, FIFO, Clock Write policies: immediate vs. deferred Prefetching to warm the cache Hands-On Labs Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Part 1) Create a VPC → 05-2.1 Create EC2 Security Group → 05-2.2 Create RDS Security Group → 05-2.3 Create DB Subnet Group → 05-2.4 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.7-week7/1.7.1-day31-2025-10-20/","title":"Day 31 - Vertical Slice Kickoff","tags":[],"description":"","content":"Date: 2025-10-20 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Project Context Ebook Demo – Vertical Slice 0 Goal: deliver an end-to-end demo of the book-detail experience before scaling the full system. Approach: adopt a Vertical Slice Architecture so every slice is built as a complete feature instead of layer-by-layer. Benefits: immediate demos, earlier bug discovery, and tighter coordination between frontend and backend. Slice Architecture User → Frontend → API → Database → Response → UI Each slice bundles UI, API contracts, backend logic, and demo data. Components can be swapped independently without breaking the rest of the system. Vertical Slice Architecture Core Principles Build features around the user journey instead of isolated layers. Keep the scope tight so each slice can demo quickly and gather feedback. Define slice ownership clearly to ease future extensions. Benefits Accelerates value delivery—you can show stakeholders right away. Lowers integration risk because every slice self-validates. Enables multiple slices to progress in parallel. Key Insights Vertical slices act as the foundation before expanding to additional features. Each slice needs a clear checklist (finished UI, validated contract, accurate backend responses). Treat every slice like a “mini product” with its own lifecycle to keep quality high. Hands-On Labs Define the scope of slice 0 (book-detail flow, minimum viable data). Draw the data-flow diagram and nail down the frontend/backend boundary. Standardize the demo checklist (contract, mock, UI, backend). "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.8-week8/1.8.1-day36-2025-10-27/","title":"Day 36 - NLP Foundations &amp; Applications","tags":[],"description":"","content":"Date: 2025-10-27 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nWhat is Natural Language Processing? Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses on enabling computers to understand, interpret, generate, and interact with human language.\nNLP integrates computational linguistics, machine learning, and deep learning to process large-scale text and speech data.\nTypical NLP Tasks: Text classification Sentiment analysis Named Entity Recognition (NER) Machine translation Part-of-speech (POS) tagging Speech recognition Core Linguistic Components in NLP Phonetics – The Sounds of Human Speech Phonetics studies the physical properties of speech sounds.\nThree Branches: Articulatory phonetics: how sounds are produced (tongue, lips, vocal folds…) Acoustic phonetics: physical sound properties (frequency, amplitude, duration) Auditory phonetics: how humans perceive sounds NLP Relevance: Used in speech recognition, speech synthesis (TTS), acoustic modeling.\nPhonology – Sound Systems of Languages Phonology studies how sounds function within a particular language. It deals with phonemes, stress patterns, allowable sound combinations.\nNLP Relevance: Grapheme-to-phoneme conversion, pronunciation modeling.\nMorphology – Structure of Words Morphology studies how words are formed from smaller units called morphemes.\nExamples: Prefixes: un-, re-, pre- Suffixes: -ing, -ed, -ness Roots/stems: run, happy, form NLP Relevance:\nStemming Lemmatization Tokenization Vocabulary building for BoW models NLP Applications Search Engines Your daily searches in the search engines are facilitated by NLP for query understanding and result ranking.\nSearch Intent Recognition Example When someone searches for \u0026ldquo;glass coffee tables\u0026rdquo;, the intent engine determines that the word \u0026ldquo;glass\u0026rdquo; likely refers to the value of attribute \u0026lsquo;Top Material\u0026rsquo; in coffee tables. It then directs the search engine accordingly to show the coffee tables category with the \u0026lsquo;Top Material\u0026rsquo; attribute set to \u0026lsquo;glass\u0026rsquo;.\nOnline Advertising NLP enables targeted ads by analyzing online behavior through multiple components:\n1. Named Entity Recognizer (NER) Identifies selected information elements called Named Entities (NE). Due to unavailability of labeled data, semi-supervised approaches are adapted to detect project use-case specific entities.\n2. Relationship Extraction One of the classical NLP tasks which aims at extracting semantic relationships from unstructured or semi-structured text documents.\n3. Moment Recognizer (MoRec) Enables analysts to understand forum discussions in the knowledge discovery phase by processing unstructured discussion text and extracting knowledge in terms of events. Events can be defined and configured depending on the use-case under investigation.\nVoice Assistants Siri, Alexa, and Google Assistant use NLP to understand and respond to your voice commands.\nMachine Translation Services like Google Translate rely on NLP to convert text from one language to another.\nChatbots Customer service chatbots use NLP to interact with users and provide assistance.\nText Summarization NLP algorithms can condense long articles into brief summaries.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.9-week9/1.9.1-day41-2025-11-03/","title":"Day 41 - RNN Problems &amp; Why Transformers Are Needed","tags":[],"description":"","content":"Date: 2025-11-03 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nThe Problem with RNNs: Sequential Processing Bottleneck RNNs have dominated NLP for years, but they have fundamental limitations that transformers solve. Let\u0026rsquo;s explore these issues.\nProblem 1: Sequential Computation How RNNs Process Information RNNs must process inputs one step at a time, sequentially:\nTranslation Example (English → French):\nInput: \u0026#34;I am happy\u0026#34; Time Step 1: Process \u0026#34;I\u0026#34; Time Step 2: Process \u0026#34;am\u0026#34; Time Step 3: Process \u0026#34;happy\u0026#34; Impact:\nIf your sentence has 5 words → 5 sequential steps required If your sentence has 1000 words → 1000 sequential steps required Cannot parallelize! Must wait for step t-1 before computing step t Why This Matters Modern GPUs and TPUs are designed for parallel computation Sequential RNNs cannot take advantage of this parallelization Training becomes much slower than necessary Longer sequences = exponentially longer training time Problem 2: Vanishing Gradient Problem The Root Cause When RNNs backpropagate through many time steps, gradients get multiplied repeatedly:\nGradient Flow Through T Steps:\n∂Loss/∂h₀ = ∂Loss/∂hₜ × (∂hₜ/∂hₜ₋₁) × (∂hₜ₋₁/∂hₜ₋₂) × ... × (∂h₁/∂h₀) If each ∂hᵢ/∂hᵢ₋₁ \u0026lt; 1 (which it often is):\nAfter T multiplications: gradient ≈ 0.5^100 ≈ 0 (for T=100) Gradient vanishes to zero Model can\u0026rsquo;t learn long-range dependencies Concrete Example Sentence: \u0026ldquo;The students who studied hard\u0026hellip; passed the exam\u0026rdquo;\nEarly word \u0026ldquo;students\u0026rdquo; needs to influence prediction of \u0026ldquo;passed\u0026rdquo; But gradient has vanished by the time it reaches \u0026ldquo;students\u0026rdquo; Model fails to learn this relationship! Current Workarounds LSTMs and GRUs help a little with gates, but:\nStill have issues with very long sequences (\u0026gt;100-200 words) Cannot fully solve the problem Still require sequential processing Problem 3: Information Bottleneck The Compression Issue Sequence-to-Sequence Architecture:\nEncoder: Word₁ → h₁ → h₂ → h₃ → hₜ (final hidden state) Decoder: hₜ → Word₁\u0026#39; → Word₂\u0026#39; → Word₃\u0026#39; → ... The Bottleneck: All information from the entire input sequence is compressed into a single vector hₜ (the final hidden state).\nWhy This Fails Example Sentence: \u0026ldquo;The government of the United States of America announced\u0026hellip;\u0026rdquo;\nWhen encoding this 8-word sentence:\nFirst word \u0026ldquo;The\u0026rdquo; is processed Information flows through states: h₁ → h₂ → h₃ → \u0026hellip; → h₈ By h₈, information about \u0026ldquo;The\u0026rdquo; has been diluted/lost Only h₈ is passed to decoder Decoder has limited context about early words The Consequence Long sequences lose information Important early context gets diluted Model struggles with long documents Translation quality degrades for long sentences Summary: Why RNNs Have Fundamental Issues Issue Impact Current Solution Sequential Processing Cannot parallelize, slow training N/A - Fundamental to RNN design Vanishing Gradients Can\u0026rsquo;t learn long-range dependencies LSTM/GRU gates (partial fix) Information Bottleneck Early information gets lost Attention mechanism (partial fix) The Transformer Solution: \u0026ldquo;Attention is All You Need\u0026rdquo; Introduced in 2017 by Google researchers (Vaswani et al.), transformers completely replace RNNs with attention mechanisms.\nKey Differences Aspect RNNs Transformers Processing Sequential (one word at a time) Parallel (all words at once) Dependency Each word depends on previous state All words attend to all words Training Speed Slow (sequential) Fast (parallel) Long Sequences Vanishing gradients No sequential bottleneck Long-range Deps. Difficult Easy (direct attention) How Transformers Work (Brief Overview) No RNN: Remove sequential hidden states completely Pure Attention: Let each word \u0026ldquo;attend to\u0026rdquo; every other word Positional Encoding: Add position information since we don\u0026rsquo;t have sequential order Parallel Processing: Process entire sequence at once Example:\nSentence: \u0026#34;I am happy\u0026#34; Instead of: Step 1: Process \u0026#34;I\u0026#34; → h₁ Step 2: Process \u0026#34;am\u0026#34; with h₁ → h₂ Step 3: Process \u0026#34;happy\u0026#34; with h₂ → h₃ Transformer Does: Parallel: \u0026#34;I\u0026#34; attends to {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;} Parallel: \u0026#34;am\u0026#34; attends to {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;} Parallel: \u0026#34;happy\u0026#34; attends to {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;} All at once! Why Everyone Talks About Transformers Speed: Can train much faster on GPUs/TPUs (parallel) Scalability: Can handle very long sequences (no bottleneck) Long-range: Direct attention solves gradient problems Versatility: Works for translation, classification, QA, summarization, chatbots\u0026hellip; Performance: Achieves state-of-the-art on nearly every NLP task\nApplications of Transformers Transformers are used for:\nTranslation (Neural Machine Translation) - High quality, fast Text Summarization (Abstractive \u0026amp; Extractive) Named Entity Recognition (NER) - Identify entities better Question Answering - Understand context better Chatbots \u0026amp; Voice Assistants Sentiment Analysis - Understand emotion/opinion Auto-completion - Smart suggestions Classification - Classify text into categories Market Intelligence - Analyze market sentiment State-of-the-Art Transformer Models GPT-2 (Generative Pre-trained Transformer) Created by: OpenAI Type: Decoder-only transformer Speciality: Text generation Famous for: Generating human-like text (even fooled journalists in 2019!) BERT (Bidirectional Encoder Representations from Transformers) Created by: Google AI Type: Encoder-only transformer Speciality: Text understanding \u0026amp; representations Use: Classification, NER, QA T5 (Text-to-Text Transfer Transformer) Created by: Google Type: Full encoder-decoder (like original transformer) Speciality: Multi-task learning Super versatile: Single model handles translation, classification, QA, summarization, regression "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.10-week10/1.10.1-day46-2025-11-10/","title":"Day 46 - Transfer Learning Fundamentals","tags":[],"description":"","content":"Date: 2025-11-10 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nTransfer Learning: Why It Matters Classical training starts from scratch for every task. Transfer learning reuses a pre-trained model so you converge faster, get better accuracy, and need less labeled data.\nPipelines Compared Classical: data -\u0026gt; random init model -\u0026gt; train -\u0026gt; predict Transfer: pre-train on large corpus -\u0026gt; reuse weights -\u0026gt; fine-tune on target task -\u0026gt; predict [Large unlabeled/labeled data] --pre-train--\u0026gt; [Base model weights] \\ fine-tune on task data --\u0026gt; deploy Two Approaches Feature-based: treat pre-trained embeddings as fixed features; train a new head. Fine-tuning: update (part of) the base model weights on downstream data. Benefits Checklist Faster convergence because weights are warm-started. Better predictions from richer representations. Smaller labeled datasets needed; leverage unlabeled pre-training. Key Considerations Domain shift: choose pre-training data close to target domain when possible. Catastrophic forgetting: use smaller learning rate or freeze early layers. Evaluation: monitor if freezing vs. full fine-tuning impacts overfitting. Practice Targets for Today Sketch a transfer pipeline for your QA task (data, base model, head, metrics). Decide which layers to freeze vs. fine-tune. Prepare a small experiment plan comparing feature-based vs. fine-tuned runs. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.11-week11/1.11.1-day51-2025-11-17/","title":"Day 51 - Lambda Managed Instances Overview","tags":[],"description":"","content":"Date: 2025-11-17 (Monday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nWhy Lambda Managed Instances LMI keeps the Lambda dev model while letting you pick EC2 instance families and pricing (SP/RI), remove cold starts, and allow multi-concurrency per instance.\nWhen to Use Steady, high-traffic workloads needing predictable cost Specialized compute/memory/network requirements Desire to apply EC2 pricing instruments to Lambda functions When to Stay on Default Lambda Spiky/unpredictable traffic Short, infrequent invocations where scale-to-zero is key Benefits Snapshot Same Lambda packaging/runtimes No cold starts; predictable capacity Cost control via EC2 constructs (savings plans, reserved instances) Notes from re:Invent CNS382 Instances are AWS-managed: you can see but not SSH/edit Lifecycle, patching, routing handled by Lambda Multi-concurrency option changes price/perf profile "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.12-week12/1.12.1-day56-2025-11-24/","title":"Day 56 - Nova Models &amp; Agent Launches","tags":[],"description":"","content":"Date: 2025-11-24 (Monday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nNova 2 Family Highlights Sonic: speech-to-speech, multilingual, keeps conversation context, controllable speech Lite: fast, cost-effective reasoning with long context window Omni (preview): multimodal (text, image, video, speech input) with text/image output Forge: program to train custom frontier models on Nova infra Agents for UI \u0026amp; Workflows Nova Act GA: UI agents for browser tasks (forms, search, booking) targeting \u0026gt;90% reliability Action Items Pick one Nova modality to pilot (speech, multimodal, or Lite reasoning) Define evaluation set (latency, quality, cost) and safety/policy checks Check availability/quotas and region support "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/4-eventparticipated/4.1-event1/","title":"Event 1 - Vietnam Cloud Day 2025","tags":[],"description":"","content":"Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Date \u0026amp; Time: Thursday, 18 September 2025 | 9:00 – 17:00 VNT\nLocation: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRegistration Status: Closed\nEvent Overview Vietnam Cloud Day 2025 served as an extensive AWS gathering tailored for developers and business leaders, presenting keynote speeches from government officials, AWS executives, and industry pioneers. The event highlighted AWS\u0026rsquo;s newest services and forward-thinking strategies for AI advancement and cloud transformation through dual tracks: a live broadcast session and on-site breakout presentations.\nAgenda Live Telecast Track Time (VNT) Session Speaker 7:35 - 9:00 Registration - 9:00 - 9:20 Opening Hon. Government Speaker 9:20 - 9:40 Keynote Address Eric Yeo, Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS 9:40 - 10:00 Customer Keynote 1 Dr. Jens Lottner, CEO, Techcombank 10:00 - 10:20 Customer Keynote 2 Ms. Trang Phung, CEO \u0026amp; Co-Founder, U2U Network 10:20 - 10:50 AWS Keynote Jaime Valles, Vice President, General Manager Asia Pacific and Japan, AWS 11:00 – 11:40 Panel Discussion: Navigating the GenAI Revolution Moderator: Jeff Johnson, Managing Director, ASEAN, AWS Panel Discussion Details: Navigating the GenAI Revolution: Strategies for Executive Leadership This conversation examined how senior leaders can successfully guide their organizations through the swift developments in generative AI. The executive panelists provided their perspectives and personal experiences regarding:\nCultivating an innovation-driven culture Connecting AI projects with business goals Handling organizational transitions that come with AI integration Panelists:\nVu Van, Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh, Chairman, Nexttech Group Dieter Botha, CEO, TymeX Breakout Tracks (In-Person Sessions) Track 1: AI \u0026amp; Analytics Focus Time (VNT) Session Speaker 13:15 - 13:30 Opening \u0026amp; Track Introduction Jun Kai Loke, AI/ML Specialist SA, AWS 13:30 - 14:00 Building a Unified Data Foundation on AWS for AI and Analytics Workloads Kien Nguyen, Solutions Architect, AWS 14:00 - 14:30 Building the Future: Gen AI Adoption and Roadmap on AWS Jun Kai Loke, AI/ML Specialist SA, AWS; Tamelly Lim, Storage Specialist SA, AWS 14:30 - 15:00 AI-Driven Development Lifecycle (AI-DLC) - Shaping the Future of Software Implementation Binh Tran, Senior Solutions Architect, AWS 15:00 - 15:30 Tea Break - 15:30 - 16:00 Securing Generative AI Applications with AWS: Fundamentals and Best Practices Taiki Dang, Solutions Architect, AWS 16:00 - 16:30 Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers Michael Armentano, Principal WW GTM Specialist, AWS Session Details Building a Unified Data Foundation on AWS for AI and Analytics Workloads\nThis presentation explored methodologies and recommended practices for establishing a unified, expandable data foundation on AWS. Attendees discovered how to utilize AWS services to develop a solid data infrastructure capable of meeting the requirements of contemporary data-centric applications. Main areas addressed:\nData collection, storage, processing, and oversight Productive data management and application for sophisticated analytics and AI projects Building the Future: Gen AI Adoption and Roadmap on AWS\nAWS shared its holistic vision, developing trends, and tactical roadmap for embracing Generative AI (GenAI) technologies. The presentation addressed essential AWS services and programs designed to enable organizations to harness GenAI for driving innovation and operational efficiency.\nAI-Driven Development Lifecycle (AI-DLC) - Shaping the Future of Software Implementation\nThe AI-Driven Development Lifecycle (AI-DLC) represents a revolutionary, AI-focused methodology redefining software implementation by completely integrating AI as a core collaborator throughout the entire software development process. In contrast to conventional approaches that add AI as a helper to existing human-led workflows, AI-DLC combines AI-driven execution with human supervision and flexible team collaboration to:\nSubstantially accelerate software development speed Improve code quality Encourage innovation Securing Generative AI Applications with AWS: Fundamentals and Best Practices\nThis presentation investigated the distinct security challenges at each tier of the generative AI stack—infrastructure, models, and applications. Attendees discovered how AWS incorporates built-in security features including:\nEncryption Zero-trust framework Ongoing monitoring Detailed access controls These features protect generative AI workloads, maintaining data privacy and integrity throughout the AI lifecycle.\nBeyond Automation: AI Agents as Your Ultimate Productivity Multipliers\nThis presentation introduced a paradigm shift where AI agents function not merely as tools, but as intelligent partners actively propelling business growth. Essential concepts covered:\nAI agents that learn, adapt, and perform complex tasks independently Evolution of operations from manual workflows to remarkable efficiency Exponential productivity growth through AI capabilities Track 2: Cloud Migration \u0026amp; Modernization Focus Time (VNT) Session Speaker 13:15 - 13:30 Opening \u0026amp; Track Introduction Hung Nguyen Gia, Head of Solutions Architect, AWS 13:30 - 14:00 Completing a Large-Scale Migration and Modernisation with AWS Son Do, Technical Account Manager, AWS; Nguyen Van Hai, Director of Software Engineering, Techcombank 14:00 - 14:30 Modernizing Applications with Generative AI-Powered Tools Phuc Nguyen, Solutions Architect, AWS; Alex Tran, AI Director, OCB 14:30 - 15:00 Panel Discussion: Application Modernization - Accelerating Business Transformation Moderator: Hung Nguyen Gia, Head of Solutions Architect, AWS 15:00 - 15:30 Break - 15:30 - 16:00 Transforming VMware with AI-driven Cloud Modernisation Hung Hoang, Customer Solutions Manager, AWS 16:00 - 16:30 AWS Security at Scale: From Development to Production Taiki Dang, Solutions Architect, AWS Session Details Completing a Large-Scale Migration and Modernisation with AWS\nThis presentation centered on important lessons from numerous enterprises that have migrated and modernized their on-premises workloads using AWS. Topics addressed:\nValidated conceptual frameworks and technical recommended practices Modernization approaches that assist organizations in upgrading while migrating AWS migration accelerators and current migration and modernization tools Case study demonstrating how organizations built a solid foundation and tactical roadmap utilizing AWS cloud capabilities to accomplish digital transformation objectives Modernizing Applications with Generative AI-Powered Tools\nThis presentation examined how Amazon Q Developer revolutionizes the software development lifecycle (SDLC) through its autonomous capabilities across:\nAWS Console IDE CLI DevSecOps platforms Key features demonstrated:\nQ\u0026rsquo;s agents speed up code generation and enhance code quality Smooth integration with current workflows Automatic creation of detailed documentation and unit tests Enhanced code maintainability and reliability Comprehension of complex codebases and optimization recommendations Automation of repetitive tasks throughout the development lifecycle Panel Discussion: Application Modernization - Accelerating Business Transformation\nPanelists:\nNguyen Minh Ngan, AI Specialist, OCB Nguyen Manh Tuyen, Head of Data Application, LPBank Securities Vinh Nguyen, Co-Founder \u0026amp; CTO, Ninety Eight Transforming VMware with AI-driven Cloud Modernisation\nThis presentation demonstrated how Vietnamese organizations are speeding up cloud adoption with VMware environments. Key topics:\nHow AWS Transform facilitates fast, secure, and cost-efficient migration Step-by-step guide and downtime-conscious patterns Pathway to modernize onto EKS, RDS, and serverless after initial deployment Ideal for IT leaders, architects, and operations teams preparing large-scale VMware-to-AWS transitions AWS Security at Scale: From Development to Production\nThis presentation examined how to strengthen cloud security stance throughout the complete development and production lifecycle. Topics addressed:\nAWS comprehensive security methodology: identification, prevention, detection, response, and remediation Security-by-design principles integrated throughout the development process Advanced detection and response capabilities How generative AI improves security analysis and automates operations Developing resilient architectures that evolve with emerging threats Building more secure, scalable cloud environments Key Takeaways Thorough understanding of AWS\u0026rsquo;s AI and cloud modernization approach Practical insights into enterprise-level AI adoption and deployment Recommended practices for data foundation, security, and application modernization Real-world case studies and lessons from industry leaders Hands-on knowledge of AWS services for GenAI, migration, and modernization "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Vo Thien Phu\nPhone Number: 0335072711\nEmail: phuvo05kid@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction to Amazon Bedrock Amazon Bedrock is a fully managed platform that provides access to a wide range of powerful large language models (LLMs). It enables applications to interact with AI models through simple API calls without the need to deploy or manage machine learning infrastructure. Bedrock supports a variety of foundation models such as Claude, Llama, and Titan, making it suitable for tasks like text generation, question-answering, content summarization, and other AI-driven use cases. In a serverless architecture, Bedrock can be integrated directly with AWS Lambda to build lightweight, scalable, and easily maintained AI services. Workshop Overview In this workshop, you will build a simple question-and-answer service using Amazon Bedrock. When a user submits a query, a Lambda function will construct a prompt, send it to a Bedrock model, and return the generated response.\nTo support this workflow, the workshop makes use of three core components:\nLambda function – acts as the processing layer, receiving input and invoking Bedrock’s API. Amazon Bedrock Runtime – performs the inference based on the selected model. API Gateway (optional) – exposes an HTTP endpoint for external clients or users to send their questions. The high-level workflow of the system is as follows:\nA user sends a question through API Gateway. API Gateway forwards the request to the Lambda function. Lambda invokes Amazon Bedrock with the constructed prompt. Bedrock returns the generated model output. Lambda formats and returns the response to the client. The diagram below illustrates the overall architecture used in this workshop:\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.1-week1/","title":"Week 1 - Cloud Computing Fundamentals","tags":[],"description":"","content":"Week: 2025-09-08 to 2025-09-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 1 Overview This week covered cloud computing fundamentals, AWS global infrastructure, and the core management tooling offered by AWS.\nKey Topics Introduction to Cloud Computing and its benefits AWS Global Infrastructure (Regions, AZs, Edge Locations) AWS Management Tools (Console, CLI, SDK) Cost Optimization approaches and Support Plans AWS Well-Architected Framework pillars Hands-on Labs Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Worklog Overview This worklog documents the AWS learning journey starting from September 8, 2025.\nStructure The worklog is organized by weeks, with each week consisting of five working days (Monday to Friday).\nMain Topics Cloud Computing Fundamentals\nAWS basics, global infrastructure, and management tools Cost optimization strategies and support plans The AWS Well-Architected Framework Networking\nVPC, subnets, security groups, and NACLs Load balancing (ALB, NLB, GWLB) VPC Peering and Transit Gateway VPN and Direct Connect Compute\nEC2, AMI, EBS, Instance Store Auto Scaling and pricing models Lightsail, EFS, FSx Storage\nS3, storage classes, Glacier Snow Family and Storage Gateway Disaster Recovery and AWS Backup Security \u0026amp; Identity\nIAM, Cognito, and AWS Organizations KMS and Security Hub Identity Center (SSO) Database\nRDS, Aurora, Redshift ElastiCache and DMS Database best practices Advanced Topics\nServerless (Lambda) Containers (ECS, EKS, ECR) Monitoring (CloudWatch, X-Ray, CloudTrail) "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.2-call-bedrock-converse/","title":"Add Code to Call the Converse API","tags":[],"description":"","content":"Add Code to Call the Converse API In this step, you will add Python code to your Lambda Function to send user questions to Amazon Bedrock using the Converse API, and return the model’s response to the client.\nThe Lambda function will perform the following tasks:\nReceive a question from the client or a test event Construct a request following the Converse API format Send the prompt to Amazon Bedrock Runtime Receive the model-generated answer Return the response in JSON format 🔹 Step 1 — Open the Lambda Function source file Open the Lambda function you created Scroll to the Code source section Open the file lambda_function.py 🔹 Step 2 — Replace the entire file content with the code below import json import boto3 # Create a client to call Bedrock Runtime bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;) # Choose a model that supports the Converse API MODEL_ID = \u0026#34;anthropic.claude-3-sonnet-20240229\u0026#34; def lambda_handler(event, context): # Receive data from API Gateway or test event body = json.loads(event.get(\u0026#34;body\u0026#34;, \u0026#34;{}\u0026#34;)) if isinstance(event.get(\u0026#34;body\u0026#34;), str) else event question = body.get(\u0026#34;question\u0026#34;, \u0026#34;Hello! What would you like to ask?\u0026#34;) # Send request to Bedrock using the Converse API response = bedrock.converse( modelId=MODEL_ID, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;text\u0026#34;: question} ] } ] ) # Extract the model\u0026#39;s response answer = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;][0][\u0026#34;text\u0026#34;] # Return the result to the client return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;answer\u0026#34;: answer}) } 🔹 Important note about MODEL_ID The MODEL_ID value inside lambda_function.py can be changed to any model you want to use.\nHowever, the model must support the Converse API.\nTo find the correct Model ID:\nOpen Amazon Bedrock Console → Model catalog Select the model you want to use In the model information panel, look for Model ID Example:\nCopy this Model ID and update the variable:\nMODEL_ID = \u0026#34;your-selected-model-id\u0026#34; If the model does not support the Converse API, Lambda will throw an error when calling bedrock.converse().\n🔹 Step 3 — Deploy the Lambda Function After updating the source code:\nClick Deploy Your Lambda Function is now ready to call Amazon Bedrock 🎯 Expected Outcome At the end of this section, your Lambda Function will be able to:\nReceive questions from a client Send prompts to Bedrock using the Converse API Receive model-generated responses Return the result as JSON You have now completed the core logic of your AI Q\u0026amp;A service.\nContinue to 5.3.3 – Test the Lambda Function.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.1-week1/1.1.2-day02-2025-09-09/","title":"Day 02 - AWS Global Infrastructure","tags":[],"description":"","content":"Date: 2025-09-09 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Infrastructure Data Centers Each data center can host tens of thousands of servers. AWS builds and manages its own custom hardware for efficiency and reliability. Availability Zone (AZ) One or more physically separate data centers within a Region. Each AZ is designed for fault isolation. Connected via low-latency, high-throughput private links. AWS recommends deploying workloads across at least two AZs. Region A Region contains at least three Availability Zones. There are currently 25+ Regions worldwide. Regions are interconnected by the AWS backbone network. Most services are Region-scoped by default. Edge Locations Global network of edge sites designed to serve content with minimal latency. Used by services such as: Amazon CloudFront (CDN) AWS WAF (Web Application Firewall) Amazon Route 53 (DNS Service) Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Create an AWS Account → 01-01 Configure Virtual MFA Device → 01-02 Create Admin Group and Admin User → 01-03 Account Authentication Support → 01-04 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.2-week2/1.2.2-day07-2025-09-16/","title":"Day 07 - VPC Routing &amp; Network Interfaces","tags":[],"description":"","content":"Date: 2025-09-16 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Routing \u0026amp; ENI Route Tables A route table defines how traffic is directed. Each VPC has a default route table containing only a local route allowing internal communication between subnets. Custom route tables can be created, but the local route cannot be deleted. Elastic Network Interface (ENI) An ENI is a virtual network card that can be moved between EC2 instances. It retains its private IP, Elastic IP address, and MAC address when reassigned. Elastic IP (EIP) is a static public IPv4 address that can be associated with an ENI. Unused EIPs incur charges. ENI Use Cases:\nManagement network separate from data network Network and security appliances Dual-homed instances with workloads on distinct subnets Low-budget, high-availability solution VPC Endpoints A VPC Endpoint enables private connectivity to supported AWS services via AWS PrivateLink without using the public Internet. Two types: Interface Endpoint: Uses an ENI with a private IP. Gateway Endpoint: Uses route tables (available for S3 and DynamoDB only). Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (continued) Create Internet Gateway (IGW) → 03-03.3 Create Route Table (Outbound via IGW) → 03-03.4 Create Security Groups → 03-03.5 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.3-week3/1.3.2-day12-2025-09-23/","title":"Day 12 - EC2 Storage &amp; Backup","tags":[],"description":"","content":"Date: 2025-09-23 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Storage \u0026amp; Security Backup in EC2 AWS Backup provides centralized backup for AWS services including EC2. EBS Snapshots back up EBS volumes: Point-in-time backups Incremental (stores only changed blocks) Stored in S3 (not directly accessible) AMI Backup captures the full EC2 configuration as an image. Snapshot Best Practices:\nSchedule regular snapshots Copy snapshots to other regions for DR Tag snapshots for lifecycle management Use Amazon Data Lifecycle Manager (DLM) Key Pair Key Pairs are used for secure authentication when connecting to EC2: Public Key – stored on the instance Private Key – kept by the user for SSH (Linux) or RDP (Windows) Replaces passwords for better security. Important: If you lose your private key, AWS cannot recover it. Key Pair Management:\nCreate key pairs in AWS or import your own Store private keys securely Use different key pairs for different environments Rotate keys regularly Elastic Block Store (EBS) Amazon EBS provides persistent block storage for EC2 instances. Volume types: General Purpose SSD (gp2/gp3) – balance between performance \u0026amp; cost Provisioned IOPS SSD (io1/io2) – for high IOPS workloads Throughput Optimized HDD (st1) – for large, sequential data Cold HDD (sc1) – low-cost, infrequently accessed data Key Features\nAttach/detach volumes from instances Data persists when instances stop Create snapshots for backup or cross-region copy Automatically replicated within an AZ EBS Volume Comparison:\nType Use Case Max IOPS Max Throughput gp3 General purpose 16,000 1,000 MB/s io2 High performance 64,000 1,000 MB/s st1 Big data 500 500 MB/s sc1 Cold storage 250 250 MB/s "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.4-week4/1.4.2-day17-2025-09-30/","title":"Day 17 - S3 Advanced Features","tags":[],"description":"","content":"Date: 2025-09-30 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon S3 Static Website Hosting Host static websites (HTML, CSS, JS, images) directly from S3.\nKey Capabilities Simple setup: A few steps to enable static website hosting on a bucket. Low cost: Pay standard S3 storage and data transfer; no separate web server charges. Elastic scaling: Automatically handles traffic spikes. CDN integration: Easily front with Amazon CloudFront for global performance. Static Website Configuration:\n{ \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; }, \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; } } Cross-Origin Resource Sharing (CORS) CORS allows web resources (fonts, JavaScript, etc.) on one domain to request resources from another domain.\nConfiguring CORS on S3 Define policies: Specify which origins are permitted to access a bucket\u0026rsquo;s content. Control methods: Allow specific HTTP methods (GET, PUT, POST, etc.). Security posture: Prevent unauthorized cross-origin access. CORS Configuration Example:\n[ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;https://example.com\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;], \u0026#34;MaxAgeSeconds\u0026#34;: 3000 } ] Performance and Object Key Design Object key naming can significantly affect S3 performance:\nRandomized prefixes: Distribute keys across partitions for higher parallelism. Avoid sequential prefixes: Don\u0026rsquo;t use monotonically increasing prefixes (e.g., timestamps) for high-throughput workloads. Parallel access: Structure keys to enable concurrent reads/writes. Key Design Best Practices:\n❌ Bad: 2025-09-30-file1.jpg, 2025-09-30-file2.jpg ✅ Good: a1b2/2025-09-30-file1.jpg, c3d4/2025-09-30-file2.jpg S3 Glacier – Long-Term Archival S3 Glacier classes are optimized for ultra–low-cost long-term storage.\nRetrieval Options Expedited / Fast: Minutes; highest cost. Standard: 3–5 hours; balanced cost. Bulk: 5–12 hours; lowest cost for large restores. Glacier Deep Archive The lowest-cost class for multi-year retention, with ~12-hour retrieval times.\nHands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Part 2) Configure Public Objects → 57-5 Test Website → 57-6 Block All Public Access → 57-7.1 Configure CloudFront → 57-7.2 Test CloudFront → 57-7.3 Bucket Versioning → 57-8 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.5-week5/1.5.2-day22-2025-10-07/","title":"Day 22 - IAM Policies &amp; Roles","tags":[],"description":"","content":"Date: 2025-10-07 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes IAM Policies JSON documents defining permissions. Types: Identity-based policies (attached to principals) Resource-based policies (attached to resources) Evaluation rule: explicit Deny overrides Allow across all policies. Pattern to constrain S3 administration:\nAllow all s3:* actions on a specific bucket. Explicitly Deny all non-S3 actions. Policy Structure:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34; } } }] } Policy Evaluation Logic:\nBy default, all requests are denied Explicit allow overrides default deny Explicit deny overrides any allows Permissions boundaries limit maximum permissions IAM Roles Roles provide temporary permissions assumed by users, services, or external identities. Common use cases: Let an AWS service act on your behalf (e.g., EC2 → S3 writes) Cross-account access Federation from external IdPs Credentials for apps on EC2 without storing access keys Benefits\nNo long-term credentials, short-lived sessions, least privilege, and easier large-scale access management. Role Types:\nService Role: For AWS services (EC2, Lambda, etc.) Cross-Account Role: Access resources in another account Identity Provider Role: For federated users Instance Profile: Container for EC2 instance role Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Part 2) Use Access Key → 48-2.2 Create IAM Role → 48-3.1 Use IAM Role → 48-3.2 Clean Up Resources → 48-4 Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Part 1) Create IAM User → 28-2.1 Create IAM Policy → 28-3 Create IAM Role → 28-4 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.6-week6/1.6.2-day27-2025-10-14/","title":"Day 27 - Amazon RDS &amp; Aurora","tags":[],"description":"","content":"Date: 2025-10-14 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes RDBMS vs NoSQL RDBMS An RDBMS stores data in related tables (rows/columns), enforces integrity constraints, uses SQL, and provides ACID guarantees. Popular engines: Oracle, MySQL, SQL Server, PostgreSQL, IBM Db2. NoSQL Overview NoSQL systems target un/semistructured data with high scalability and performance. Types: Document (MongoDB, CouchDB) Key–Value (Redis, DynamoDB) Column-Family (Cassandra, HBase) Graph (Neo4j, Amazon Neptune) Traits: schema flexibility, horizontal scaling, big-data friendliness, CAP-oriented designs. RDBMS vs. NoSQL (high-level) OLTP vs. OLAP OLTP: many small, concurrent transactions; normalized data; short queries; index-heavy. OLAP: complex analytics over large historical datasets; star/snowflake schemas; read-heavy. Amazon RDS \u0026amp; Aurora Amazon Relational Database Service (RDS) Managed relational databases that simplify provisioning, patching, backups, and HA.\nSupported engines: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Amazon Aurora. Key features: automated backups/patching, easy scaling, Multi-AZ high availability, encryption \u0026amp; VPC/IAM/SSL security. Deployment options: Single-AZ Multi-AZ (synchronous standby in another AZ) Read Replicas for scaling reads RDS Features:\nAutomated Backups: Point-in-time recovery up to 35 days Manual Snapshots: User-initiated backups Multi-AZ: Automatic failover for high availability Read Replicas: Scale read workloads (up to 15 replicas) Parameter Groups: Database configuration management Option Groups: Additional features (e.g., Oracle Advanced Security) Amazon Aurora Cloud-native, MySQL/PostgreSQL-compatible relational database re-architected for AWS.\nHighlights: Up to ~5× MySQL / ~3× PostgreSQL performance (typical benchmarks) Storage auto-scales to 128 TB Six-way replication across three AZs; self-healing storage Aurora Serverless (on-demand capacity) Global Database for low-latency multi-region Aurora Features:\nAurora Replicas: Up to 15 read replicas with sub-10ms lag Aurora Serverless: Auto-scaling compute capacity Aurora Global Database: Cross-region replication \u0026lt; 1 second Aurora Backtrack: Rewind database to specific point in time Aurora Parallel Query: Faster analytics on current data Aurora Machine Learning: Native ML integration Aurora vs RDS:\nFeature Aurora RDS Performance 5x MySQL, 3x PostgreSQL Standard Storage Auto-scaling to 128 TB Manual scaling Replicas Up to 15 Up to 5 (MySQL) Failover \u0026lt; 30 seconds 1-2 minutes Backtrack Yes No Hands-On Labs Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Part 2) Create EC2 Instance → 05-3 Create RDS Database Instance → 05-4 Application Deployment → 05-5 Backup and Restore → 05-6 Clean Up Resources → 05-7 Lab 43 – AWS Database Migration Service (DMS) (Part 1) EC2 Connect RDP Client → 43-01 EC2 Connect Fleet Manager → 43-02 SQL Server Source Config → 43-03 Oracle Connect Source DB → 43-04 Oracle Config Source DB → 43-05 Drop Constraint → 43-06 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.7-week7/1.7.2-day32-2025-10-21/","title":"Day 32 - Contract-First &amp; Mocking","tags":[],"description":"","content":"Date: 2025-10-21 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Contract-First Development 5-Step Workflow Write the OpenAPI spec to define the contract. Share the spec as the single source of truth for both frontend and backend. Let the frontend build UI against mock data derived from the contract. Implement the backend API to match the schema (status codes, payloads). Run contract testing to confirm the backend adheres to the spec. paths: /books/{id}: get: summary: Get book detail responses: \u0026#34;200\u0026#34;: $ref: \u0026#34;#/components/responses/BookDetail\u0026#34; Benefits Minimizes API mismatches because everyone references the same spec. Documentation, mock servers, and test scripts can be generated automatically. Makes it easy to review and version the API before real deployment. Insight Defining the contract before coding cuts ~80% of integration issues when teams work in parallel.\nMock APIs with Prism Prism reads OpenAPI specs to return mock responses so the frontend can test UI early. Multiple scenarios (200, 404, 500) are supported by attaching examples to the spec. Keeps delivery on track even when the backend is unfinished or in refactor mode. When to Use It The first sprint of a vertical slice. Need to showcase a flow without production data yet. Want automated UI tests based on the contract. Operational Notes Run Prism on localhost:4010 and point Next.js to the mock via NEXT_PUBLIC_API_URL. Mirror production CORS headers in the mock responses. Always commit the spec before mocking so everyone uses the same version. Hands-On Labs Define the OpenAPI spec for the /books/{id} endpoint. Launch the Prism mock server and verify the UI flow. Write a contract review checklist (status codes, schema, sample data). "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.8-week8/1.8.2-day37-2025-10-28/","title":"Day 37 - Voice Search &amp; Chatbot Architecture","tags":[],"description":"","content":"Date: 2025-10-28 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nVoice Search (How Siri Works) Voice search systems follow a pipeline from speech input to actionable response:\nPipeline Components: 1. Analog to Digital Conversion Speech (utterance) → Sound wave pattern → Spectrogram (frequency pattern) → Sequence of acoustic frames using Fast Fourier Transform (FFT)\n2. Automatic Speech Recognition (ASR) Feature analysis: Extract acoustic features Hidden Markov Model (HMM): Pattern recognition for speech-to-text Viterbi algorithm: Find most likely sequence of hidden states Phonetic dictionary: Map sounds to words Language model: Ensure grammatical correctness 3. NLP Annotation Tokenization POS tagging Named Entity Recognition (NER) 4. Pattern-Action Mappings Map recognized intents to appropriate actions\n5. Service Manager Internal \u0026amp; external APIs (email, SMS, maps, weather, stocks, etc.) Execute the requested action 6. Text-to-Speech (TTS) Convert response back to speech\n7. User Feedback System learns from corrections to improve accuracy\nVoicebot Architecture The voicebot processing pipeline consists of multiple linguistic levels:\nProcessing Layers: Speech Analysis (Phonology) Recognize and transcribe speech using Automatic Speech Recognition (ASR)\nMorphological and Lexical Analysis (Morphology) Analyze word structure and meaning using morphological rules and lexicon\nParsing (Syntax) Understand sentence structure using lexicon and grammar rules\nContextual Reasoning (Semantics) Understand meaning in context using discourse context\nApplication Reasoning and Execution (Reasoning) Use domain knowledge to decide actions\nUtterance Planning Plan what to say in response\nSyntactic Realization Generate grammatically correct sentences\nMorphological Realization Apply correct word forms\nPronunciation Model Generate proper pronunciation\nSpeech Synthesis Convert text back to speech\nChatbot Workflow Step-by-Step Process: 1. User → Chat Client User types: \u0026ldquo;I want to check my account balance.\u0026rdquo; Chat Client = interface where user types (web, app, messenger)\n2. Chat Client → Chatbot Message sent to chatbot system\n3. Chatbot → NLP Engine Chatbot sends message to NLP Engine for analysis\nNLP Engine performs two main tasks: (a) Intent Detection Determine what the user wants to do\nExample: check_balance (b) Entity Extraction Extract important data from the sentence\nExample: account = checking/savings? 4. NLP Engine → Business Logic / Data Services Based on intent, chatbot calls the appropriate service:\nQuery database Call API Execute business rules Process backend logic Example: Call API to get balance from banking system\n5. Data Services → Chatbot Backend returns result:\n\u0026ldquo;Your account balance is $12,500.00\u0026rdquo;\n6. Chatbot → Chat Client Chatbot packages information into natural language response\n7. Display to User User sees the response\nChatbot = Listening + Chatting Listening (NLP - Understanding) Intent recognition Entity extraction Context understanding Chatting (NLG - Generation) Natural language generation Response formulation Personalization Behind the Scenes: Knowledge-based data: Facts, rules, FAQs Machine learning: Learning from interactions Business logic: Application-specific rules Important Distinction: Keyword vs Entity Keywords = words that indicate topics or subjects Entities = specific data points with types and values\nExample: \u0026ldquo;Book a flight to Paris on Friday\u0026rdquo;\nKeywords: book, flight Entities: destination = \u0026ldquo;Paris\u0026rdquo; (LOCATION) date = \u0026ldquo;Friday\u0026rdquo; (DATE) Not all keywords are entities, but all entities are extracted from keywords!\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.9-week9/1.9.2-day42-2025-11-04/","title":"Day 42 - Transformer Architecture Overview","tags":[],"description":"","content":"Date: 2025-11-04 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nTransformer Architecture: The Big Picture The transformer model introduced in the paper \u0026ldquo;Attention is All You Need\u0026rdquo; revolutionized NLP. Let\u0026rsquo;s understand its complete structure.\nHigh-Level Architecture INPUT SEQUENCE ↓ [Tokenization \u0026amp; Embedding] ↓ [Add Positional Encoding] ↓ ┌─────────────────────────────────┐ │ ENCODER (N layers) │ │ ├─ Multi-Head Attention │ │ ├─ Layer Normalization │ │ ├─ Feed-Forward Network │ │ └─ Residual Connections │ └─────────────────────────────────┘ ↓ [Context Vectors from Encoder] ↓ ┌─────────────────────────────────┐ │ DECODER (N layers) │ │ ├─ Masked Multi-Head Attention │ │ ├─ Encoder-Decoder Attention │ │ ├─ Feed-Forward Network │ │ └─ Layer Normalization │ └─────────────────────────────────┘ ↓ [Linear Layer + Softmax] ↓ OUTPUT PROBABILITIES Component 1: Word Embeddings Each word is converted to a dense vector (typically 512-1024 dimensions).\nExample:\nWord: \u0026#34;happy\u0026#34; Embedding: [0.2, -0.5, 0.8, ..., 0.1] // 512 values Component 2: Positional Encoding Problem: Transformers don\u0026rsquo;t have sequential order built in (unlike RNNs). So we must add positional information explicitly.\nSolution: Add positional encoding vectors to embeddings.\nFormula:\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model)) PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model)) Where: - pos = position in sequence (0, 1, 2, ...) - i = dimension index - d_model = embedding dimension (512, 1024, etc.) Intuition:\nPosition 0: \u0026ldquo;I\u0026rdquo; gets PE₀ Position 1: \u0026ldquo;am\u0026rdquo; gets PE₁ Position 2: \u0026ldquo;happy\u0026rdquo; gets PE₂ Example:\nEmbedding(\u0026#34;I\u0026#34;) = [0.2, -0.5, 0.8, ..., 0.1] PE(pos=0) = [0.0, 1.0, 0.0, 1.0, ..., 0.5] Final = [0.2, 0.5, 0.8, 1.0, ..., 0.6] Component 3: Multi-Head Attention Instead of one attention mechanism, we have h different \u0026ldquo;heads\u0026rdquo; running in parallel.\nConcept:\nInput: Query (Q), Key (K), Value (V) matrices Head 1: ScaledDotProductAttention(Q₁, K₁, V₁) Head 2: ScaledDotProductAttention(Q₂, K₂, V₂) ... Head h: ScaledDotProductAttention(Qₕ, Kₕ, Vₕ) Output = Concatenate(Head₁, Head₂, ..., Headₕ) Why Multiple Heads?\nHead 1 might learn \u0026ldquo;subject-verb\u0026rdquo; relationships Head 2 might learn \u0026ldquo;adjective-noun\u0026rdquo; relationships Head 3 might learn \u0026ldquo;pronoun-reference\u0026rdquo; relationships Together: Rich contextual understanding Typical Configuration:\nNumber of heads: 8-16 Dimension per head: 64 (if total = 512, then 512/8 = 64) Component 4: Residual Connections \u0026amp; Layer Normalization Residual Connections Output = Input + Attention(Input) This helps with gradient flow during training and allows networks to go deeper.\nLayer Normalization Normalized = (x - mean) / sqrt(variance + epsilon) Stabilizes training and speeds up convergence.\nComponent 5: Feed-Forward Network After attention, there\u0026rsquo;s a simple 2-layer feed-forward network:\nOutput = ReLU(Linear₁(x)) → Linear₂ Typical dimensions:\nInput: [batch_size, seq_length, 512] ↓ Linear₁ (512 → 2048) [batch_size, seq_length, 2048] ↓ ReLU (non-linear) [batch_size, seq_length, 2048] ↓ Linear₂ (2048 → 512) [batch_size, seq_length, 512] This expands then contracts, allowing for non-linear transformations.\nThe Encoder: Detailed View Single Encoder Layer:\nInput (x) ↓ [Multi-Head Self-Attention] ↓ [+ Residual Connection with input] ↓ [Layer Normalization] ↓ [Feed-Forward Network] ↓ [+ Residual Connection] ↓ [Layer Normalization] ↓ Output Key Point: In encoder, each word attends to ALL words (including itself) in the same sentence.\nEncoder gives: Contextual representation of each word, considering all other words.\nThe Decoder: Detailed View Decoder is similar but with masking:\nInput (shifted right by 1) ↓ [Masked Multi-Head Self-Attention] ← Can only attend to previous positions ↓ [+ Residual + LayerNorm] ↓ [Encoder-Decoder Attention] ← Attends to encoder output ↓ [+ Residual + LayerNorm] ↓ [Feed-Forward Network] ↓ [+ Residual + LayerNorm] ↓ Output Three Attention Mechanisms in Decoder:\nMasked Self-Attention:\nQueries, Keys, Values from decoder Each position can only attend to previous positions Prevents information leakage (decoder doesn\u0026rsquo;t see future words) Encoder-Decoder Attention:\nQueries from decoder Keys, Values from encoder Decoder can attend to any encoder position Feed-Forward:\nSame 2-layer network as encoder Putting It Together: Full Transformer Training Phase Input: \u0026#34;Je suis heureux\u0026#34; (French) Target: \u0026#34;I am happy\u0026#34; (English) Encoder Input: - Tokenize: [Je, suis, heureux] - Embed each token - Add positional encoding - Process through N encoder layers → Output: C (context vectors) Decoder Input: - Target shifted right: [\u0026lt;START\u0026gt;, I, am] - Embed each token - Add positional encoding - Process through N decoder layers - Using masked self-attention - Using encoder-decoder attention on C → Output logits for each position Loss: Compare predicted \u0026#34;am happy\u0026#34; with actual \u0026#34;am happy\u0026#34; Backprop: Update all weights Inference Phase Encoder Input: [Je, suis, heureux] → Output: C (context vectors) Decoder: Step 1: Start with [\u0026lt;START\u0026gt;] Predict next word: \u0026#34;I\u0026#34; Step 2: [\u0026lt;START\u0026gt;, I] Predict next word: \u0026#34;am\u0026#34; Step 3: [\u0026lt;START\u0026gt;, I, am] Predict next word: \u0026#34;happy\u0026#34; Step 4: [\u0026lt;START\u0026gt;, I, am, happy] Predict next word: \u0026lt;END\u0026gt; Output: \u0026#34;I am happy\u0026#34; Summary: Why This Architecture Works Feature Benefit No RNN Fully parallelizable - train on GPUs efficiently Self-Attention in Encoder Each word gets context from ALL other words Masked Attention in Decoder Can\u0026rsquo;t see future - trains with autoregressive generation Positional Encoding Preserves word order without RNN sequential processing Multi-Head Attention Learn multiple types of relationships simultaneously Residual Connections Gradient flow - enables training deep networks Layer Normalization Stability - faster convergence Key Innovations Parallelization: O(1) depth instead of O(n) for RNNs Long-range Dependencies: Attention can directly connect any two positions Scalability: Can increase model size with predictable improvements Transfer Learning: Pre-trained transformers (BERT, GPT) work across tasks "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.10-week10/1.10.2-day47-2025-11-11/","title":"Day 47 - Question Answering Modes","tags":[],"description":"","content":"Date: 2025-11-11 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nContext-Based vs. Closed-Book QA Two common QA setups share the same transformer backbone but differ in inputs and evaluation.\nContext-Based (Open Book) Input: question + supporting context paragraph(s). Output: span extraction or short generation grounded in context. Training: supervised spans (e.g., start/end indices) or seq2seq with context. Failure mode: wrong or missing span when context is noisy. Closed-Book Input: question only; the model must rely on internal knowledge. Output: generated answer without explicit context. Training: language modeling style on large corpora; often fine-tuned on QA pairs. Failure mode: hallucination; mitigated by stronger pre-training and knowledge distillation. Picking the Mode If you can supply documents at runtime -\u0026gt; prefer context-based (more controllable, cite-able). If latency/storage prevents retrieving context -\u0026gt; closed-book is lighter but riskier. Evaluation Notes Context-based: exact match / F1 over spans; check grounding to provided text. Closed-book: BLEU/ROUGE and human factuality checks; add retrieval if drift is high. Practice Targets for Today Draft examples for both modes using your domain data. Define metrics you will track (span EM/F1 vs. generative ROUGE/factuality). List retrieval options to upgrade closed-book to open-book if needed. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.11-week11/1.11.2-day52-2025-11-18/","title":"Day 52 - Capacity Provider Setup","tags":[],"description":"","content":"Date: 2025-11-18 (Tuesday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nBuilding a Capacity Provider Step 1 for LMI: define the instance layer (capacity provider) with VPC, roles, and instance choices.\nRequired Inputs Operator IAM role so Lambda can launch/manage instances VPC config (subnets + SG) where LMI instances live Instance Selection Supported: latest C/M/R families (x86/AMD/Graviton), large and above Override: allowed/excluded types; set architecture explicitly for ARM EBS encryption: default service key or your KMS Networking Notes Spread subnets across 3 AZs for resilience All egress/logs flow through instance ENI; no function-level VPC config Close inbound SG rules; ensure path to dependencies/CloudWatch endpoints Guardrails Max vCPU cap to bound spend Additional scaling knobs available per provider "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.12-week12/1.12.2-day57-2025-11-25/","title":"Day 57 - Bedrock Models &amp; AgentCore","tags":[],"description":"","content":"Date: 2025-11-25 (Tuesday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nBedrock Updates Adds managed open-weight models (Mistral Large 3, Ministral 3 3B/8B/14B, plus other providers) Reinforcement fine-tuning: feedback-driven tuning with big accuracy gains without large labeled sets AgentCore Enhancements Policy controls and quality monitoring for agents Improved memory and natural conversations for safer deployments Action Items Shortlist models for eval (latency/cost/quality vs. current choices) Plan RFT experiment for a target task; define feedback signals Map required policies/guardrails before enabling AgentCore in prod "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/4-eventparticipated/4.2-event2/","title":"Event 2 - AWS GenAI Builder Club: AI-Driven Development Life Cycle","tags":[],"description":"","content":"AWS GenAI Builder Club: AI-Driven Development Life Cycle - Reimagining Software Engineering Date \u0026amp; Time: Friday, October 3, 2025 | 14:00 (2:00 PM)\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nInstructors: Toan Huynh \u0026amp; My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nEvent Overview This AWS GenAI Builder Club gathering investigated the AI-Driven Development Lifecycle (AI-DLC), an innovative methodology for software engineering that incorporates AI as a core collaborator throughout the complete development journey. The session included interactive demonstrations of Amazon Q Developer and Kiro, presenting real-world AI applications in contemporary software development.\nAgenda Time Session Instructor 14:00 - 14:15 Welcoming - 14:15 - 15:30 AI-Driven Development Life Cycle Overview \u0026amp; Amazon Q Developer Demonstration Toan Huynh 15:30 - 15:45 Break - 15:45 - 16:30 Kiro Demonstration My Nguyen Key Concepts \u0026amp; Learnings 1. AI-Driven Development Lifecycle (AI-DLC) Overview Core Philosophy The AI-Driven Development Lifecycle signifies a fundamental transformation in software construction methods. Instead of treating AI as a secondary feature or basic code completion utility, AI-DLC positions AI as an intelligent collaborator throughout the complete development workflow.\nKey Principles:\nYou Maintain Control - AI serves as your assistant, not your supervisor. You need to retain decision-making power over project direction and implementation specifics.\nAI as Partner, Not Substitute - AI should pose important questions about your project needs, architecture, and objectives. The partnership should flow both ways, with you steering AI recommendations.\nDesign Before Building - Always develop a thorough plan before writing code. AI can assist in generating this plan, but you must examine, confirm, and improve it.\nThe Development Workflow Step 1: Develop a Project Plan\nEstablish clear project requirements and boundaries Request AI to produce a plan based on your specifications Critically assess the plan and ask for adjustments Confirm the plan is thorough and clear Step 2: Decompose into User Stories\nTransform the plan into user stories with explicit acceptance criteria Split large scope into smaller, handleable units Each unit becomes a mini-project that can be delegated to team members Calculate timelines for each unit (while being careful about over-estimation) Step 3: Establish Technology Stack\nExplicitly state the technologies, frameworks, and tools to be utilized Rather than instructing AI \u0026ldquo;don\u0026rsquo;t implement this,\u0026rdquo; say \u0026ldquo;implement this way\u0026rdquo; Affirmative guidance produces better success rates than negative limitations Step 4: Thorough Requirements \u0026amp; Design\nDocument requirements with accuracy and clarity Partner with AI to develop detailed specifications Specify data models, API contracts, and system architecture Produce design documents before implementation starts Step 5: Implementation \u0026amp; Confirmation\nBuild features following the plan Apply mob development methodology (team works collectively on code) Confirm all produced code as a team Perform code reviews and quality assessments Step 6: Testing \u0026amp; Deployment\nProgress through environments: Development (Dev) → Testing (QA) → User Acceptance Testing (UAT) → Production (Prod) Maintain quality gates at each phase Confirm functionality before production release Critical Success Factors Develop a Plan First - Don\u0026rsquo;t assume AI will manage everything. Always begin with a defined plan. Review Continuously - Consistently review AI recommendations and outputs. Significant error rates are possible. You Are the Director - Your worth lies in code validation and project supervision, not in authoring every line of code. Pose Clarifying Questions - Confirm AI grasps your project context by asking critical questions about requirements, architecture, and objectives. Apply Prompt Templates - Design structured prompts including user context, user stories, and particular requirements to obtain clearer AI responses. Save Plans to Files - Request AI to generate plans as files you can store, review, and adjust. This produces a living document for future consultation. Be Courteous to AI - Preserve respectful communication with AI tools. Positive rapport may benefit future interactions (and it\u0026rsquo;s simply good practice!). 2. Amazon Q Developer Demonstration What is Amazon Q Developer? Amazon Q Developer is an AI-enabled assistant that revolutionizes the software development lifecycle (SDLC) through autonomous capabilities across various platforms:\nAWS Console - Supports infrastructure and service configuration IDE (Integrated Development Environment) - Delivers code generation and optimization recommendations CLI (Command Line Interface) - Supports command generation and automation DevSecOps Platforms - Incorporates security practices into the development workflow Key Capabilities Code Generation \u0026amp; Quality\nSpeeds up code generation with AI-enabled suggestions Enhances code quality through smart recommendations Preserves smooth integration with current workflows Comprehends complex codebases and recommends optimizations Documentation \u0026amp; Testing\nAutomatically produces thorough documentation Generates unit tests with minimal manual effort Considerably improves code maintainability and reliability Minimizes boilerplate and repetitive coding tasks Intelligent Partnership\nFunctions as an intelligent partner utilizing large language models Merges deep AWS service knowledge with coding proficiency Assists developers in speeding up development cycles Improves code quality and reinforces security stance Automation Throughout Development Lifecycle\nAutomates standard tasks throughout the complete development lifecycle Decreases manual, repetitive work Enables developers to concentrate on higher-value, creative tasks Enhances overall productivity and efficiency Best Practices for Using Amazon Q Developer Supply Clear Context - Provide Q comprehensive information about your project, architecture, and requirements Apply Specific Prompts - Rather than vague requests, deliver specific, detailed prompts with examples Examine Suggestions - Always examine Q\u0026rsquo;s suggestions before applying them Iterate and Improve - If the initial suggestion isn\u0026rsquo;t ideal, refine your prompt and try again Utilize AWS Knowledge - Benefit from Q\u0026rsquo;s deep understanding of AWS services and best practices 3. Kiro Demonstration What is Kiro? Kiro is an autonomous IDE (Integrated Development Environment) created by Amazon Web Services that connects rapid AI-powered prototyping and production-ready software development. It\u0026rsquo;s presently in public preview.\nCore Philosophy Kiro represents the principle that AI should boost developer productivity while preserving professional standards, clear organization, thorough testing, documentation, and long-term maintainability.\nKey Features Specification-Driven Development\nWhen you provide a requirement (e.g., \u0026ldquo;add a product rating system\u0026rdquo;), Kiro transforms it into: User stories with explicit acceptance criteria Design documentation Task lists and implementation plans Organized specifications before code generation Agent Hooks \u0026amp; Automation\nAutomatically initiates tasks based on events: File saves initiate documentation updates Commits initiate test generation Particular actions initiate performance optimization Minimizes manual, repetitive work Steering \u0026amp; Project Context\nDevelop steering files (markdown) to outline: Project structure and arrangement Coding standards and conventions Preferred architecture patterns Team guidelines and best practices Assists Kiro in deeply understanding your project context Multi-File Analysis \u0026amp; Intent Understanding\nExamines multiple files concurrently Grasps functional goals across the codebase Implements changes aligned with overall project objectives Extends beyond basic code completion VS Code Integration\nConstructed on VS Code\u0026rsquo;s open-source base Import settings, themes, and extensions from VS Code Recognizable interface for existing VS Code users Effortless transition for developers Flexible AI Model Selection\nPresently uses Claude Sonnet 4 as default \u0026ldquo;Auto\u0026rdquo; mode merges multiple models based on context Equilibrium between quality and cost Adaptability to select different models for different tasks Advantages of Using Kiro Enhanced Transparency \u0026amp; Control\nBegin with specifications before code generation Examine and confirm specs before implementation Decrease hallucinated code or misaligned implementations Preserve clear traceability from requirements to code Decreased Boilerplate \u0026amp; Repetitive Tasks\nAgent hooks automate documentation generation Automatic unit test creation Automatic information updates Liberates developers for higher-value work Security \u0026amp; Privacy\nMost code operations occur locally Data only transmitted externally with explicit permission Preserves control over sensitive information Extensibility \u0026amp; Flexibility\nConnects external tools via MCP (Model Context Protocol) Supports multiple AI models Not restricted to a single AI environment Adaptable to different team workflows Limitations \u0026amp; Considerations Preview Status - Still in public preview; stability and features may change Complex Projects - May face challenges with deep contextual understanding in highly complex projects Oversight Required - Users still need to supervise and confirm AI decisions Future Pricing - Anticipated pricing tiers: Free: ~50 tasks/month Pro: ~1,000 tasks/month Pro+: ~3,000 tasks/month When to Use Kiro You desire an AI + programming workflow that preserves professionalism and clear organization Constructing rapid prototypes but concerned about production durability Investigating how AI can become a genuine programming colleague, not just a code suggestion tool You require specification-driven development with automated documentation and testing Common Pitfalls When Using AI in Development 1. Anticipating AI to Manage Everything Problem: Many developers anticipate AI to finish entire projects autonomously.\nSolution: Always develop a plan first and examine regularly. AI is a tool to boost productivity, not substitute developer judgment.\n2. Elevated Error Rates Problem: AI can commit mistakes, especially in complex scenarios.\nSolution: Implement regular examination cycles. Confirm all AI-generated code before deployment.\n3. Absence of Clear Requirements Problem: Vague or unclear requirements lead to vague AI outputs.\nSolution: Document requirements with precision. Partner with AI to develop detailed specifications before implementation.\n4. Negative Constraints Instead of Affirmative Guidance Problem: Instructing AI \u0026ldquo;don\u0026rsquo;t do this\u0026rdquo; is less productive than \u0026ldquo;do this.\u0026rdquo;\nSolution: Apply positive, specific instructions. Better success rates derive from clear affirmative guidance.\n5. Inadequate Project Context Problem: AI doesn\u0026rsquo;t comprehend your project\u0026rsquo;s unique requirements and constraints.\nSolution: Develop steering files, supply detailed context, and pose critical questions to AI about your project.\n6. Viewing AI as a Director Problem: Permitting AI to make all decisions about project direction and architecture.\nSolution: Remember: You are the director. Your worth lies in code confirmation and project oversight, not in authoring every line of code.\nKey Takeaways AI is Your Assistant - Preserve control over project decisions and implementation direction\nDesign First, Code Second - Always develop a thorough plan before implementation\nPartnership Over Automation - AI should pose questions and partner, not just execute commands\nClear Requirements Matter - Precision in requirements leads to superior AI outputs\nRegular Examination is Essential - Don\u0026rsquo;t anticipate AI to be perfect; examine and confirm continuously\nYou Are the Code Director - Your worth is in confirmation and oversight, not in authoring every line\nApply Structured Prompts - Templates with context, user stories, and requirements produce better results\nSave Plans to Files - Develop living documents you can consult and adjust\nAffirmative Guidance Works Better - Instruct AI what to do, not what to avoid\nExperience Matters - Apply these tools practically to comprehend their capabilities and limitations\nRecommended Tools \u0026amp; Resources Amazon Q Developer - AI-enabled development assistant integrated with AWS services Kiro IDE - Specification-driven development environment with AI partnership AWS CodeWhisperer - Code generation and optimization tool MCP (Model Context Protocol) - Framework for connecting external tools and services Conclusion The AI-Driven Development Lifecycle embodies a new paradigm in software engineering where AI and humans partner as equals. Success demands clear planning, regular examination, precise requirements, and preserving developer control over project direction. Tools such as Amazon Q Developer and Kiro are facilitating this new workflow, but they function best when developers comprehend their capabilities and limitations, and preserve their role as project directors and code validators.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.4-api-gateway-integration/5.4.2-integrate-lambda/","title":"Integrate API with Lambda","tags":[],"description":"","content":"Integrate API with Lambda In this step, you will configure the HTTP API so that incoming requests from the client are forwarded to your Lambda function.\nAPI Gateway will serve as the HTTP interface for your Bedrock-powered chatbot.\n🔹 Step 1 — Open the API you created Go to API Gateway Console and select the API:\nbedrock-chatbot-api\n🔹 Step 2 — Create a route Go to the Routes section Click Create Configure the route: Method: POST Resource path: /chat Click Create to add the route.\n🔹 Step 3 — Add Integration with Lambda Click the newly created /chat route In the Integration section, choose Attach integration Select Create and attach an integration, then configure: Integration type: Lambda function Region: The AWS region you are using Lambda function: lambda-bedrock-function (the Lambda you created earlier) Click Create.\n🎯 Expected Result You have now:\nCreated the /chat route Attached the route to your Lambda function Your API now has an endpoint ready for testing "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.2-prerequisite/","title":"Prerequisites","tags":[],"description":"","content":"Preparation Steps Before Starting Before building your AI API using Lambda + Bedrock, you need to configure several AWS resources and permissions.\n1. Choose an AWS Region that Supports Bedrock Amazon Bedrock is available in several AWS regions.\nFor this workshop, you may choose:\nus-east-1 (N. Virginia) – commonly used in AWS documentation ap-southeast-1 (Singapore) – also supports Bedrock and works normally Just make sure the model you want to use (Claude, Llama, Mistral…) is available in that region.\n2. Create an IAM Role for Lambda Lambda needs an IAM Role to call Bedrock models and write logs to CloudWatch.\nIn this section, you will first create a custom policy, then create a role and attach the policy.\n🔹 Step 1 — Create a New Policy Open IAM Console → Policies → Create policy In the “Specify permissions” page, select the JSON tab.\nDelete all existing content and replace it with the following: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34;, \u0026#34;bedrock:InvokeModelWithResponseStream\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next, give the policy a name (e.g., lambda-bedrock) and click Create policy. 🔹 Step 2 — Create the IAM Role and Attach the Policy Go to IAM Console → Roles → Create role Select: Trusted entity type: AWS service Use case: Lambda In “Add permissions”, search for the policy you created earlier (lambda-bedrock) and select it. Name your role, for example: lambda-bedrock-role Then click Create role.\nYour Lambda execution role is now ready with the minimum permissions required to call Amazon Bedrock and write CloudWatch logs.\n3. Verify the Model Using Bedrock Playground Before writing Lambda code to call the Converse API, test the model in the AWS Console.\n🔹 Step 1 — Open the Model Catalog Open Amazon Bedrock → Model catalog 🔹 Step 2 — Choose a Model and Open Playground Pick a model such as Claude 3.5 Sonnet, Llama 3.1, or Mistral 24.07 Click Open in playground 🔹 Step 3 — Send a Test Prompt Enter a simple question to confirm the model is responding correctly in your selected region.\n🔹 Step 4 — (Optional) Check if the Model Supports the Converse API If you want to confirm whether the selected model supports Converse API, follow these steps:\nReturn to the model page in the catalog Scroll down to Code examples AWS will open an example code page. If the model supports Converse API, the example will include:\nbedrock.converse(...) Like this:\n4. Recommended (Optional) Knowledge You may find it helpful to know the following:\nBasics of AWS Lambda How to create an HTTP API with API Gateway How to view logs in CloudWatch This workshop is beginner-friendly and does not require deep AWS experience.\nIn the next step, you will create a Lambda function and write your first code snippet to call the Converse API in Amazon Bedrock.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"FitAI Challenge An application that helps users lose weight through exercise challenges, integrated with AI for tracking and evaluation 1. Executive Summary FitAI Challenge is a website developed for Vietnamese users, aiming to promote fitness and exercise culture through sports challenges that incorporate gamification and artificial intelligence (AI). The website uses an AI Camera to recognize and count exercise movements such as push-ups, squats, planks, and jumping jacks, while also analyzing posture to provide accurate evaluations. Users can participate in individual challenges to earn FitPoints upon completing tasks, which can be redeemed for vouchers, gifts, or discounts from partner merchants. FitAI Challenge targets students, young adults, and working professionals — individuals who need motivation to maintain regular workout habits amid their busy lives.\n2. Problem Statement What’s the Problem? In Vietnam, most existing fitness applications primarily focus on basic guidance or step counting, and there is currently no platform that combines AI-based motion recognition, gamification, and an online fitness challenge community. Users often lack motivation to exercise consistently and do not have tools that can accurately evaluate their workout performance. In addition, gyms and sports brands also lack creative engagement channels to connect with young and active customer groups.\nThe Solution FitAI Challenge uses an AI Camera to recognize, count, and evaluate the accuracy of workout movements through Computer Vision. All user workout data is stored and processed via AWS Cloud using a serverless architecture: AWS Lambda: processes AI data and backend requests. AWS S3: stores videos, images, and temporary results. The website is developed using React Native with a friendly and intuitive interface. Users can: Participate in individual, group, or nationwide challenges. Earn FitPoints upon completing exercises. Redeem FitPoints for vouchers or gifts from partners (Shopee, Grab, CGV, etc.). Track leaderboards and share achievements on social media.\nBenefits and Return on Investment For users: Create daily workout motivation through challenge and reward mechanisms. Receive transparent performance evaluations supported by AI. Connect with the fitness community through leaderboards and sharing feeds. For partner businesses: A branding channel associated with a healthy lifestyle. Access to a young, dynamic, and health-conscious customer base. For the development team: Establish a unique “Fitness + Gamification + E-commerce” business model in Vietnam. Serverless cloud architecture helps reduce operating costs and allows easy scalability. The MVP can be developed within the first 3 months with low infrastructure costs (estimated at 0.80 USD/month on AWS).\n3. Solution Architecture FitAI Challenge is an intelligent sports training platform that applies an AWS Serverless architecture combined with an AI/ML pipeline. The system’s goal is to record workout data, analyze performance, and generate AI-powered feedback to provide personalized coaching for users. Data from the web application is sent to Amazon API Gateway, processed by AWS Lambda (Java), and stored in Amazon S3 along with the Docker Database.\nAWS Services Used Service Role Amazon Route 53 Manages domain names and routes traffic to CloudFront. AWS WAF Protects frontend and API layers from DDoS and OWASP attacks. Amazon CloudFront Delivers static content (web app built from Java web, HTML, CSS, JS). Amazon API Gateway Receives requests from the frontend and forwards them to Lambda functions. AWS Lambda (Java) Handles business logic (registration, login, data upload, scoring, AI pipeline). AWS Step Functions \u0026amp; SQS Coordinates workflows between Lambda and SageMaker/Bedrock. Amazon Cognito Authenticates users, manages login sessions, and controls access permissions. Amazon S3 Stores raw data, videos, images, and analysis results. Docker Runs the Java Spring Boot API backend and hosts the database (PostgreSQL or MongoDB). Amazon SageMaker Runs inference for computer vision/pose estimation models. Amazon Bedrock Generates natural language feedback, training suggestions, and summary reports. Amazon SES Sends authentication emails and user result notifications. Amazon CloudWatch Monitors logs, Lambda performance, costs, and system efficiency. IAM Manages access permissions and security across services. AWS CodePipeline / CodeBuild / CodeDeploy CI/CD pipeline for automating Java backend and Lambda deployment. Component Design Frontend Layer: The web app displays the user interface and connects to the API Gateway. The content is built and deployed on S3 + CloudFront. Users access the system through Route 53 → WAF → CloudFront → API Gateway. Application Layer: The API Gateway receives requests from the frontend. Lambda (Java) executes business functions: AuthLambda: handles user login and authentication. UploadLambda: receives workout data, images, or videos. AIPipelineLambda: triggers the AI workflow (SageMaker + Bedrock). SaveResultLambda: stores training results and AI feedback.\n4. Technical Implementation Implementation Phases\nPhase Description Achieved Outcome 1. AWS Infrastructure Setup Deploy VPC, Private Subnets, Route 53, WAF, S3, API Gateway, Lambda, Cognito, RDS MySQL, VPC Endpoints (S3, Bedrock). Secure, isolated, and ready-to-use base infrastructure. 2. CI/CD Pipeline Set up CodeCommit + CodeBuild + CodeDeploy + CodePipeline for Java backend and Lambda; build \u0026amp; deploy web to S3/CloudFront. Automated deployment for backend and frontend. 3. Build Lambda Functions (Java) Create Lambdas for Auth, Upload, AI Pipeline, Save Result; connect to RDS MySQL and S3. Completed serverless backend. 4. AI Pipeline Integrate SQS, Step Functions, and Bedrock; build workflow: receive job → analyze data → score → generate AI feedback. Smooth AI operation with automated user feedback. 5. Web App Deployment Build web app → Deploy to S3 + CloudFront → configure domain in Route 53. Stable online user interface. 6. Monitoring \u0026amp; Cost Optimization Use CloudWatch + Cost Explorer to track logs, performance, and costs; tune Lambda, RDS, and CloudFront configurations. Stable system with low cost and tight monitoring. 5. Timeline \u0026amp; Milestones Before internship (Month 0):\nDesign detailed architecture based on the new diagram.\nExperiment with a simple AI pipeline using Bedrock (text feedback).\nInternship (Months 1–3):\nMonth 1:\nSet up infrastructure: VPC, Subnets, RDS MySQL, Cognito, API Gateway, Lambda, S3, CloudFront.\nConfigure CI/CD (CodeCommit, CodeBuild, CodeDeploy, CodePipeline).\nMonth 2:\nDevelop Java backend and Lambda functions.\nBuild the AI pipeline with SQS, Step Functions, and Bedrock.\nMonth 3:\nIntegrate frontend with backend.\nRun performance tests, pilot with 10–20 users, prepare the final demo.\nPost-deployment:\nContinue optimizing the AI model and add deeper gamification over the next year.\n6. Budget Estimation You can view costs on the AWS Pricing Calculator, or download the attached budget estimate.\nInfrastructure Costs (MVP estimate)\nAWS Services: Amazon API Gateway: 0.38 USD / month (≈300 requests/month, 1 KB/request). Amazon Bedrock: 0.32 USD / month (1 req/min, 350 input tokens, 70 output tokens). Amazon CloudFront: 1.20 USD / month (5 GB transfer, 500,000 HTTPS requests). Amazon CloudWatch: 1.85 USD / month (5 metrics, 0.5 GB logs). Amazon Cognito: 0.00 USD / month (≤100 MAU). Amazon Route 53: 0.51 USD / month (1 hosted zone). Amazon S3: 0.04 USD / month (1 GB storage, 1,000 PUT/POST/LIST, 20,000 GET). Amazon SES: 0.30 USD / month (3,000 emails from EC2/Lambda). Amazon Simple Queue Service (SQS): ≈0.00 USD / month (0.005 million requests/month). AWS Lambda: 0.00 USD / month (≈300,000 requests/month, 512 MB ephemeral storage). AWS Step Functions: 0.00 USD / month (500 workflows, 5 state transitions/workflow). AWS Web Application Firewall (WAF): 6.12 USD / month (1 Web ACL, 1 rule). Amazon RDS MySQL (auto-stop mode): 0–3 USD / month (depending on runtime). Total: around 10.7 – 12 USD / month depending on RDS usage; equivalent to 128 – 144 USD / 12 months.\n7. Risk Assessment Risk Matrix\nTechnical: AI misidentifies movements or fails on image/video processing; misconfigured VPC/Endpoints cause service disruption. User: Users don’t maintain workout habits; low return rate. Market \u0026amp; Partners: Hard to expand reward partners and co-branding; partner policy changes. Cost: Unexpected cost increases when user volume spikes (CloudFront, Bedrock). Mitigation Strategies\nOptimize AI models with continuous training; monitor quality via logs; add basic validation before scoring. Deepen gamification (streak chains, friend groups, seasonal challenges, appealing rewards). Prepare clear value propositions; diversify partner types (sports, healthy food, entertainment). Set AWS Budgets + Alarms per service (CloudFront, Bedrock, Lambda, RDS). Contingency Plans\nIf AI fails → use fallback logic (simple time/rep-based scoring) and clearly notify users. If user volume drops → launch community challenges, pair with social media campaigns. If commercial partners withdraw → maintain internal FitPoints with small rewards (in-app badges/vouchers) while finding new partners. 8. Expected Outcomes Technical Improvements:\nComplete the AI motion recognition system with \u0026gt;90% accuracy. Stable app that supports up to 10,000 concurrent active users on serverless architecture. Optimize architecture to keep infra cost around 10–12 USD/month in the early stage. Long-term Value:\nBuild a sustainable Vietnamese fitness community connected via online challenges. Become the pioneering \u0026ldquo;AI + Fitness + Gamification\u0026rdquo; platform in Vietnam. Establish a workout data foundation to expand into health analytics, personalized training programs, and future AI projects. 9. Document Attachments Full Proposal Documents:\nFitAI Challenge Proposal (English Version) FitAI Challenge Proposal (Vietnamese Version) "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.2-week2/","title":"Week 2 - AWS Networking Services","tags":[],"description":"","content":"Week: 2025-09-15 to 2025-09-19\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 2 Overview This week focused on AWS networking services, spanning foundational VPC concepts through enterprise-grade connectivity patterns.\nKey Topics Amazon VPC and subnet design Security Groups and NACLs Internet Gateway and NAT Gateway VPC Peering and Transit Gateway Elastic Load Balancing (ALB, NLB, GWLB) Hands-on Labs Lab 03: Amazon VPC \u0026amp; Networking Basics Lab 10: Hybrid DNS (Route 53 Resolver) Lab 19: VPC Peering Lab 20: AWS Transit Gateway "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.3-create-lambda-and-bedrock-call/","title":"Create Lambda and Call Bedrock","tags":[],"description":"","content":"Create Lambda and Call Bedrock In this section, you will create a Lambda function and configure it to call an Amazon Bedrock model using the Converse API.\nThe overall architecture was introduced earlier, and this step focuses on implementing the components needed for Lambda to send a prompt to Bedrock and receive a response.\nIn the following steps, you will:\nCreate a new Lambda function Assign the Execution Role you prepared in the Prerequisites section Write the initial code to call a model using the Converse API After completing this section, your Lambda function will be able to interact directly with Bedrock and handle AI Question–Answer requests.\nContents Create Lambda Function Assign IAM Role to Lambda Add Code to Call Bedrock via Converse API "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.1-week1/1.1.3-day03-2025-09-10/","title":"Day 03 - AWS Management Tools","tags":[],"description":"","content":"Date: 2025-09-10 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Management Tools AWS Management Console Log in as Root User or IAM User (requires 12-digit Account ID). Search and access individual service dashboards. Support Center allows you to open support cases directly. AWS Command Line Interface (CLI) Open-source command-line tool for interacting with AWS services. Provides functionality equivalent to the Console. Key Features:\nCross-platform support (Windows, macOS, Linux) Scriptable and automatable Direct access to AWS service APIs Supports profiles for multiple accounts AWS SDK (Software Development Kit) Simplifies integration of AWS services within applications. Handles authentication, retries, and data serialization/deserialization automatically. Supported Languages:\nPython (Boto3) JavaScript/Node.js Java .NET Ruby, PHP, Go, and more "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.2-week2/1.2.3-day08-2025-09-17/","title":"Day 08 - VPC Security &amp; Flow Logs","tags":[],"description":"","content":"Date: 2025-09-17 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Security Security Group (SG) A stateful virtual firewall that controls inbound and outbound traffic to AWS resources. Rules are based on protocol, port, source, or another security group. Only allow rules are supported. Applied to Elastic Network Interfaces (ENIs). Security Group Characteristics:\nStateful: return traffic automatically allowed Supports allow rules only Evaluates all rules before deciding Applies to instance level (ENI) Network Access Control List (NACL) A stateless virtual firewall that operates at the subnet level. Rules control inbound and outbound traffic by protocol, port, and source. Default NACL allows all traffic. NACL Characteristics:\nStateless: must explicitly allow return traffic Supports both allow and deny rules Rules processed in number order Applies to subnet level VPC Flow Logs Capture metadata about IP traffic to and from network interfaces in your VPC. Logs can be delivered to Amazon CloudWatch Logs or S3. Flow Logs do not record packet payloads. Flow Log Use Cases:\nTroubleshoot connectivity issues Monitor traffic patterns Security analysis Compliance requirements Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (continued) Launch EC2 Instances in Subnets → 04-1 Test Connection Between Instances → 04-2 Create NAT Gateway (Private ↔ Internet) → 04-3 EC2 Instance Connect Endpoint (no bastion) → 04-5 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.3-week3/1.3.3-day13-2025-09-24/","title":"Day 13 - Instance Store &amp; User Data","tags":[],"description":"","content":"Date: 2025-09-24 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Advanced Features Instance Store Instance Store provides temporary block-level storage physically attached to the EC2 host. Characteristics\nVery high I/O and throughput Data lost when instance stops or terminates Cannot be detached or snapshotted Use Cases\nCaching or temporary data processing Applications with their own redundancy or replication Instance Store vs EBS:\nFeature Instance Store EBS Persistence Temporary Persistent Performance Very high High Snapshot No Yes Detachable No Yes Cost Included Additional User Data User Data scripts run automatically at instance launch (once per AMI provision). Linux – bash scripts Windows – PowerShell scripts User Data Examples:\n#!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Metadata EC2 Instance Metadata provides details about the running instance such as private/public IP, hostname, and security groups. Often used in user data scripts for dynamic configuration. Accessing Metadata:\n# Get instance ID curl http://169.254.169.254/latest/meta-data/instance-id # Get public IP curl http://169.254.169.254/latest/meta-data/public-ipv4 # Get IAM role credentials curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Create Budget by Template → 07-01 Create Cost Budget Tutorial → 07-02 Create Usage Budget → 07-03 Create Reserved Instance Budget → 07-04 Create Savings Plans Budget → 07-05 Clean Up Budgets → 07-06 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.4-week4/1.4.3-day18-2025-10-01/","title":"Day 18 - AWS Snow Family &amp; Hybrid Storage","tags":[],"description":"","content":"Date: 2025-10-01 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Snow Family Purpose-built devices and services to move large datasets into and out of AWS when networks are limited or data volumes are massive.\nAWS Snowcone: Small, rugged device (~8 TB). Suited for edge and remote sites. AWS Snowball: Snowball Edge Storage Optimized: Up to ~80 TB usable storage. Snowball Edge Compute Optimized: Adds powerful compute with ~42 TB storage. AWS Snowmobile: Exabyte-scale data transfer (up to 100 PB) in a secure containerized data center. Snow Family Comparison:\nDevice Storage Compute Use Case Snowcone 8 TB 2 vCPUs Edge, IoT Snowball Storage 80 TB 40 vCPUs Data migration Snowball Compute 42 TB 52 vCPUs Edge computing Snowmobile 100 PB N/A Datacenter migration When to Use Snow Family:\nLimited or expensive bandwidth Large data volumes (TB to PB) Remote or disconnected locations Edge computing requirements Regulatory data residency AWS Storage Gateway Hybrid cloud storage service that connects on-premises applications with cloud-backed storage.\nGateway Types File Gateway\nNFS/SMB file shares backed by S3 objects. Use cases: user shares, application backups, archives. Volume Gateway\niSCSI block storage backed by S3 with EBS snapshots. Modes: Cached volumes: Primary data in S3; local cache on-prem. Stored volumes: Primary data on-prem; async copy to S3. Use cases: on-prem block workloads with cloud backup/DR. Tape Gateway\nVirtual Tape Library (VTL) for existing backup apps (e.g., NetBackup, Veeam). Writes appear as tape but land in S3/Glacier. Use cases: tape replacement and archival modernization. Hands-On Labs Lab 24 – AWS Storage Gateway (On-Premises Integration) Create Storage Gateway → 24-2.1 Create File Shares → 24-2.2 Mount File Shares On-Prem → 24-2.3 Clean Up Resources → 24-3 Lab 14 – AWS VM Import/Export (Part 1) VMware Workstation → 14-01 Export Virtual Machine from On-Premises → 14-02.1 Upload Virtual Machine to AWS → 14-02.2 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.5-week5/1.5.3-day23-2025-10-08/","title":"Day 23 - Amazon Cognito &amp; Organizations","tags":[],"description":"","content":"Date: 2025-10-08 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Cognito Managed authentication/authorization and user management for web \u0026amp; mobile apps. Components: User Pools: Sign-up/sign-in user directories. Identity Pools: Federated identities for temporary AWS credentials to access services. Cognito User Pools Features:\nSign-up and sign-in Social identity providers (Google, Facebook, Amazon) SAML identity providers Multi-factor authentication (MFA) Email and phone verification Custom authentication flows Lambda triggers for customization Cognito Identity Pools Features:\nTemporary AWS credentials Authenticated and unauthenticated access Role-based access control Integration with User Pools Support for external identity providers AWS Organizations Centrally manage multiple AWS accounts in a single organization. Benefits\nCentralized account management Consolidated Billing Hierarchies with Organizational Units (OUs) Guardrails with Service Control Policies (SCPs) Organizational Units (OUs) Group accounts by department, project, or environment; nest OUs for hierarchical policies. Example OU Structure:\nRoot ├── Production OU │ ├── Web App Account │ └── Database Account ├── Development OU │ ├── Dev Account │ └── Test Account └── Security OU └── Audit Account Consolidated Billing One invoice for all accounts; volume pricing benefits; no extra cost. Benefits:\nVolume discounts across accounts Easier tracking and reporting Simplified payment method Reserved Instance sharing Hands-On Labs Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Part 2) Switch Roles → 28-5.1 Access EC2 Console – Tokyo → 28-5.2.1 Access EC2 Console – N. Virginia → 28-5.2.2 Create EC2 (No Qualified Tags) → 28-5.2.3 Edit EC2 Resource Tag → 28-5.2.4 Policy Check → 28-5.2.5 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Part 1) Create EC2 Instance with Tag → 27-2.1.1 Manage Tags in AWS Resources → 27-2.1.2 Filter Resources by Tag → 27-2.1.3 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.6-week6/1.6.3-day28-2025-10-15/","title":"Day 28 - Amazon Redshift","tags":[],"description":"","content":"Date: 2025-10-15 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Redshift Fully managed cloud data warehouse optimized for large-scale analytics (OLAP).\nColumnar storage, compression, MPP execution; scales from hundreds of GB to PB. Integrations: S3, Kinesis, DynamoDB, BI tools; strong security features. Concurrency Scaling adds capacity automatically during spikes. Architecture: cluster (leader node + compute nodes), each compute node has slices. Deployment options:\nRedshift Provisioned Redshift Serverless Redshift Spectrum (query S3 directly) Use cases: enterprise BI, data lake analytics, dashboards, trend analysis, forecasting.\nRedshift Features:\nColumnar Storage: Optimized for analytics queries Massively Parallel Processing (MPP): Distributes queries across nodes Result Caching: Speeds up repeated queries Automatic Compression: Reduces storage costs Workload Management (WLM): Query prioritization Concurrency Scaling: Handle burst workloads Redshift vs Traditional Data Warehouse:\nFeature Redshift Traditional DW Setup Minutes Weeks/Months Scaling Elastic Fixed capacity Cost Pay-as-you-go Large upfront Maintenance Managed Self-managed Redshift Spectrum:\nQuery data directly in S3 without loading Separate compute and storage Support for various file formats (Parquet, ORC, JSON) Cost-effective for infrequently accessed data Hands-On Labs Lab 43 – AWS Database Migration Service (DMS) (Part 2) MSSQL → Aurora MySQL Target Config → 43-07 MSSQL → Aurora MySQL Create Project → 43-08 MSSQL → Aurora MySQL Schema Conversion → 43-09 Oracle → MySQL Schema Conversion (1) → 43-10 Create Migration Task \u0026amp; Endpoints → 43-11 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.7-week7/1.7.3-day33-2025-10-22/","title":"Day 33 - Next.js App Router","tags":[],"description":"","content":"Date: 2025-10-22 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Next.js 16 App Router Leverage Server Components to fetch data directly on the server and avoid bloated bundles. The /app/books/[id]/page.tsx route handles data fetching and returns a pre-rendered UI. generateMetadata supplies SEO meta tags based on the book payload. // app/books/[id]/page.tsx import { getBook } from \u0026#34;@/lib/api\u0026#34;; export default async function BookDetail({ params }) { const book = await getBook(params.id); if (!book) return notFound(); return \u0026lt;BookPage book={book} /\u0026gt;; } Error \u0026amp; Not Found Handling Only not-found.tsx is needed for missing books—treat it as an expected branch. Skip error.tsx to avoid double handling; log unexpected issues on the backend. Keep UX consistent with a CTA back to the listing page plus a support hotline. Environment \u0026amp; Config Use explicit environment variables: NEXT_PUBLIC_API_URL for the frontend and API_URL for route handlers. Keep .env.example in sync whenever a new variable is introduced. Centralize URL construction in lib/api.ts to prevent duplication. Insight Server Components noticeably reduce latency when rendering detail pages. The App Router enforces a clear folder structure, making it easier to split new slices. When pointing to the mock API, fetch directly from Prism by swapping the base URL. Hands-On Labs Create a custom not-found.tsx with navigation CTAs. Implement a reusable getBook(id) helper for server components and tests. Run npm run lint \u0026amp;\u0026amp; npm run build to ensure the Next.js configuration stays clean. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.8-week8/1.8.3-day38-2025-10-29/","title":"Day 38 - Seq2seq Models &amp; LSTM Deep Dive","tags":[],"description":"","content":"Date: 2025-10-29 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nSeq2seq Model Sequence-to-Sequence (Seq2seq) models introduce an encoder-decoder architecture effective for tasks like machine translation and text summarization.\nKey Features: Maps variable-length sequences to fixed-length memory Input and outputs can have different lengths Uses LSTMs and GRUs to avoid vanishing and exploding gradients Encoder takes word tokens as input → hidden state vectors → decoder generates output sequence LSTM Architecture: Deep Dive What is LSTM? LSTM (Long Short-Term Memory) is like a mini version of the human brain when processing memory.\nLSTM Structure = 3 Gates + 1 Cell State 1. Forget Gate – Deciding What to Forget Decides what information to discard from the old state.\nFormula:\nf_t = σ(W_f · [h_{t-1}, x_t] + b_f) Brain analogy:\nUseless messages from someone who ghosted you → forget Formulas you use daily → keep 2. Input Gate – Deciding What to Remember Decides what new information to add to memory.\nFormulas:\ni_t = σ(W_i · [h_{t-1}, x_t] + b_i) Ĉ_t = tanh(W_C · [h_{t-1}, x_t] + b_C) Brain analogy:\nValuable information → store in long-term memory Irrelevant noise → discard immediately 3. Cell State Update – Long-term Memory Updates long-term memory by combining forget and input gates.\nFormula:\nC_t = f_t ⊙ C_{t-1} + i_t ⊙ Ĉ_t Where:\nf_t ⊙ C_{t-1} = what to keep from old memory i_t ⊙ Ĉ_t = what to add from new input 4. Output Gate – Deciding What to Output Decides which memory to use for current output.\nFormulas:\no_t = σ(W_o · [h_{t-1}, x_t] + b_o) h_t = o_t ⊙ tanh(C_t) Brain analogy:\nWhen taking an NLP exam → recall LSTM formulas When talking to someone → recall conversation context When doing DevOps → recall AWS specs LSTM vs Human Brain Human Brain LSTM Long-term memory Cell State Filter out unnecessary information Forget Gate Accept new valuable information Input Gate Retrieve appropriate memory to respond Output Gate Learn from sequential experiences RNN backbone Don\u0026rsquo;t forget quickly Long-term dependencies What is a Gate? Gate = cognitive filter\nEach gate = a mechanism that decides \u0026ldquo;keep or discard\u0026rdquo;\nExample: When You Study NLP Forget Gate: \u0026ldquo;Do I still need to remember this outdated method?\u0026rdquo; → Discard if no Input Gate: \u0026ldquo;Is this new concept valuable?\u0026rdquo; → Store if yes Output Gate: \u0026ldquo;What knowledge do I need right now?\u0026rdquo; → Retrieve relevant parts Hidden State Limitations Hidden state doesn\u0026rsquo;t have a token limit, but has a capacity limit for effective memory.\nMathematical Perspective: Hidden state = fixed-size vector (e.g., 128, 256, 512 dimensions) Can process 10 tokens or 10,000 tokens → won\u0026rsquo;t crash Problem: can\u0026rsquo;t remember everything Why? Even with cell state, gradients weaken over many time steps Long-term dependencies get lost Tokens far from the start have weak influence on final output Solution: This is why we need Attention mechanism!\nThrottling in NLP Two Meanings of Throttling: 1. System-Level Throttling (API) Limiting request rate or token processing to:\nProtect GPU resources Distribute resources fairly Avoid server overload Control costs Examples:\nOpenAI GPT: 10 requests/second, 90k tokens/min Anthropic Claude: 20 requests/second HuggingFace: timeout if generation takes too long 2. Model-Level Throttling (Architecture) LSTM, Transformer, and Attention all have mechanisms to limit information processing at any given time:\n(A) LSTM Throttling → Forget Gate When sequence is too long:\nForget gate automatically \u0026ldquo;throttles\u0026rdquo; old information Only allows part of meaning to pass through Like network throttling: \u0026ldquo;overload → reduce bandwidth → drop packets\u0026rdquo; (B) Transformer Throttling → Context Window Limit\nBERT: 512 tokens GPT-3: 2048-4096 tokens GPT-4: 128k-1M tokens Claude 3.5 Sonnet: 200k-1M tokens When input exceeds limit:\nModel cuts data Or refuses to process Or downgrades attention quality (C) Attention Throttling → Sparse Attention In long-context models (Longformer, BigBird, Mistral):\nCan\u0026rsquo;t compute full n² attention Only attend to important regions (local attention) Or global tokens Or sliding window (D) Token Generation Throttling Some decoders will:\nSlow down token generation Limit sampling Apply temperature control Cut beam search When input is noisy or uncertain, this acts like a brake: \u0026ldquo;Not sure → slow down generation → increase quality\u0026rdquo;\nSummary LSTM is not just a model — it\u0026rsquo;s a computational mimicry of how human memory works. Understanding gates helps you understand why certain information persists while other information fades away.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.9-week9/1.9.3-day43-2025-11-05/","title":"Day 43 - Scaled Dot-Product Attention Deep Dive","tags":[],"description":"","content":"Date: 2025-11-05 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nScaled Dot-Product Attention: The Core Mechanism This is the heart and soul of transformers. Understanding this deeply is critical.\nThe Attention Formula (Q × K^T) Attention(Q, K, V) = softmax(─────────) × V √(d_k) Where:\nQ = Query matrix (what are we looking for?) K = Key matrix (what can we attend to?) V = Value matrix (what information do we get?) d_k = dimension of keys (usually 64) Step-by-Step Computation Let\u0026rsquo;s compute attention for a simple example:\nInput Sentence: \u0026ldquo;I am happy\u0026rdquo;\nSetup Phase Step 1: Create Word Embeddings\nI: [0.1, 0.2, 0.3] am: [0.4, 0.5, 0.6] happy: [0.7, 0.8, 0.9] Step 2: Convert to Q, K, V In practice, we learn linear projections:\nQ = Embedding × W_q K = Embedding × W_k V = Embedding × W_v For simplicity, let\u0026rsquo;s say:\nQ = [0.1, 0.2, 0.3] K = [0.1, 0.2, 0.3] V = [0.1, 0.2, 0.3] [0.4, 0.5, 0.6] [0.4, 0.5, 0.6] [0.4, 0.5, 0.6] [0.7, 0.8, 0.9] [0.7, 0.8, 0.9] [0.7, 0.8, 0.9] (In reality, Q, K, V would be different projections, but this shows the concept)\nComputation Phase Step 3: Compute Q × K^T (dot products)\nQ × K^T = [0.1, 0.2, 0.3] [0.1, 0.4, 0.7] [0.4, 0.5, 0.6] × [0.2, 0.5, 0.8] [0.7, 0.8, 0.9] [0.3, 0.6, 0.9] Q₁·K₁ = 0.1×0.1 + 0.2×0.2 + 0.3×0.3 = 0.01 + 0.04 + 0.09 = 0.14 Q₁·K₂ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.04 + 0.10 + 0.18 = 0.32 Q₁·K₃ = 0.1×0.7 + 0.2×0.8 + 0.3×0.9 = 0.07 + 0.16 + 0.27 = 0.50 Result matrix: [0.14, 0.32, 0.50] [0.32, 0.77, 1.22] [0.50, 1.22, 1.94] Interpretation:\nQ₁ (query for \u0026ldquo;I\u0026rdquo;) has similarity scores: [0.14, 0.32, 0.50] 0.14 with \u0026ldquo;I\u0026rdquo; itself 0.32 with \u0026ldquo;am\u0026rdquo; 0.50 with \u0026ldquo;happy\u0026rdquo; Step 4: Scale by √d_k\nd_k = 3 (embedding dimension), so √d_k = √3 ≈ 1.73\nScaled matrix = Q×K^T / √3: [0.14/1.73, 0.32/1.73, 0.50/1.73] [0.08, 0.18, 0.29] [0.32/1.73, 0.77/1.73, 1.22/1.73] = [0.18, 0.44, 0.70] [0.50/1.73, 1.22/1.73, 1.94/1.73] [0.29, 0.70, 1.12] Why scale?\nWhen d_k is large (e.g., 64), dot products get very large Large numbers cause softmax to have very small gradients (saturation) Scaling by √d_k keeps numbers in reasonable range for training Step 5: Apply Softmax\nSoftmax converts scores to probabilities (sum to 1):\nsoftmax(x) = exp(x) / sum(exp(x)) For first row [0.08, 0.18, 0.29]: exp(0.08) ≈ 1.083 exp(0.18) ≈ 1.197 exp(0.29) ≈ 1.336 Sum ≈ 3.616 Probabilities: [1.083/3.616, 1.197/3.616, 1.336/3.616] ≈ [0.30, 0.33, 0.37] All three rows:\nSoftmax weights (attention matrix): [0.30, 0.33, 0.37] [0.26, 0.37, 0.37] [0.25, 0.36, 0.39] Interpretation:\nWord \u0026ldquo;I\u0026rdquo; pays 30% attention to itself, 33% to \u0026ldquo;am\u0026rdquo;, 37% to \u0026ldquo;happy\u0026rdquo; Word \u0026ldquo;am\u0026rdquo; pays 26% attention to \u0026ldquo;I\u0026rdquo;, 37% to itself, 37% to \u0026ldquo;happy\u0026rdquo; Word \u0026ldquo;happy\u0026rdquo; pays 25% to \u0026ldquo;I\u0026rdquo;, 36% to \u0026ldquo;am\u0026rdquo;, 39% to itself Step 6: Multiply by Value Matrix (V)\nContext = Softmax_weights × V Context(for \u0026#34;I\u0026#34;): 0.30×[0.1,0.2,0.3] + 0.33×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9] = [0.03,0.06,0.09] + [0.13,0.17,0.20] + [0.26,0.30,0.33] = [0.42, 0.53, 0.62] Context(for \u0026#34;am\u0026#34;): 0.26×[0.1,0.2,0.3] + 0.37×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9] = [0.026,0.052,0.078] + [0.148,0.185,0.222] + [0.259,0.296,0.333] = [0.433, 0.533, 0.633] Context(for \u0026#34;happy\u0026#34;): 0.25×[0.1,0.2,0.3] + 0.36×[0.4,0.5,0.6] + 0.39×[0.7,0.8,0.9] = [0.025,0.05,0.075] + [0.144,0.18,0.216] + [0.273,0.312,0.351] = [0.442, 0.542, 0.642] Output Context Matrix:\n[0.42, 0.53, 0.62] [0.433, 0.533, 0.633] [0.442, 0.542, 0.642] Each word now has a context vector that combines information from all words weighted by attention scores!\nWhy Scaled Dot-Product Attention? Aspect Reason Dot Product Efficient similarity measure (just matrix multiplication) Scaling Prevents softmax saturation (keeps gradients healthy) Softmax Converts similarities to normalized weights [0,1] Multiply by V Gets actual information (weighted combination) Multi-Head Attention: Multiple Perspectives Instead of one attention head, we use h = 8 (or more) attention heads:\nMultiHeadAttention(Q, K, V) = Concat(Head₁, ..., Head₈) × W_o Where: Head_i = Attention(Q × W_q^i, K × W_k^i, V × W_v^i) Different heads learn different relationships:\nHead 1: Subject-verb relationships Head 2: Adjective-noun relationships Head 3: Pronoun-antecedent relationships Head 4-8: Other semantic patterns Example:\nSentence: \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34; Head 1 (subject-verb): - \u0026#34;fox\u0026#34; → \u0026#34;jumps\u0026#34;: 0.9 - \u0026#34;dog\u0026#34; → has_property: 0.7 Head 2 (adjective-noun): - \u0026#34;quick\u0026#34; → \u0026#34;fox\u0026#34;: 0.85 - \u0026#34;brown\u0026#34; → \u0026#34;fox\u0026#34;: 0.8 - \u0026#34;lazy\u0026#34; → \u0026#34;dog\u0026#34;: 0.9 Head 3 (spatial): - \u0026#34;over\u0026#34; → connects \u0026#34;fox\u0026#34; and \u0026#34;dog\u0026#34;: 0.8 All these different perspectives combined give rich contextual understanding.\nComputational Efficiency Why is scaled dot-product attention so efficient?\nMatrix Operations: Just multiplication and softmax (GPU-optimized) Parallelizable: Can process entire sequences at once Memory Efficient: O(n²) memory for n-length sequence (acceptable) Fast Training: Modern GPUs can do billions of dot products/second Comparison:\nRNN: O(n) sequential steps → slow Attention: O(1) depth, O(n²) memory → fast! Key Insights ✅ Attention is learned: W_q, W_k, W_v are trainable parameters ✅ Position-free: No sequential dependencies - can attend across any distance ✅ Interpretable: Can visualize which words attend to which ✅ Efficient: Uses only matrix operations (GPU-friendly)\nNext Steps Now that we understand scaled dot-product attention, we\u0026rsquo;ll explore:\nSelf-attention (query=key=value) Masked attention (decoder only sees past) Encoder-decoder attention (cross-lingual connections) Multi-head attention in detail (learning multiple patterns) "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.10-week10/1.10.3-day48-2025-11-12/","title":"Day 48 - BERT Bidirectional Context","tags":[],"description":"","content":"Date: 2025-11-12 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nHow BERT Learns BERT pre-trains with bidirectional context so each token sees both left and right neighbors.\nMasked Language Modeling (MLM) Randomly mask ~15% tokens; predict the original token. Loss encourages contextual embeddings that consider surrounding words. Input: learning from deep learning is like watching the sunset with my best [MASK] Target: friend Next Sentence Prediction (NSP) Task: predict if sentence B follows sentence A. Helps sentence-level coherence for tasks like QA and classification. Downstream Use Start from pre-trained weights. Option A: freeze encoder, train a lightweight head (feature-based). Option B: fine-tune encoder + head with a small learning rate. Tips Keep max_seq_length aligned to task data; long docs may need chunking. Watch for catastrophic forgetting; gradual unfreezing can help. Small batch? Use gradient accumulation to stabilize updates. Practice Targets for Today Prepare a BERT QA fine-tune plan (dataset, max length, lr, epochs). Decide whether to freeze lower layers for your data size. Add evaluation checkpoints (dev set EM/F1) to catch overfitting early. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.11-week11/1.11.3-day53-2025-11-19/","title":"Day 53 - Functions on LMI","tags":[],"description":"","content":"Date: 2025-11-19 (Wednesday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nCreating and Publishing Functions Create functions as usual, but bind them to a capacity provider and publish a version to launch instances.\nSupported Features Packaging: ZIP or OCI Runtimes: Java, Python, Node, .NET Layers, extensions, function URLs, response streaming, durable functions 15-minute timeout (longer via durable functions) Resource Settings Memory/CPU shapes influence instance type selection Multi-concurrency per instance available; plan for throughput vs. cost Multiple functions can share one capacity provider (shared pool) Workflow Create capacity provider (VPC, role, instance rules) Create function with provider ARN Publish version to trigger instance provisioning and deploy execution environments "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.12-week12/1.12.3-day58-2025-11-26/","title":"Day 58 - Vectors &amp; Privacy Data","tags":[],"description":"","content":"Date: 2025-11-26 (Wednesday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nVector Storage \u0026amp; Synthetic Data Amazon S3 Vectors GA: up to 2B vectors/index, ~100ms latency, lower cost vs. specialized DBs Clean Rooms synthetic data: generate privacy-safe datasets for collaborative ML Migration Considerations Index sizing vs. current store; sharding and region placement Query latency targets and cost estimates vs. existing vector DB Access patterns and security for joint data collaborations Action Items Design a POC migrating one collection to S3 Vectors; measure perf/cost Identify datasets needing synthetic generation for shared use Define retention/encryption and access policies for vector data "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.3-create-lambda-and-bedrock-call/5.3.3-test-lambda/","title":"Test Lambda Function","tags":[],"description":"","content":"Test Lambda Function After creating the Lambda Function and adding the code to call Amazon Bedrock using the Converse API, the next step is to test the function to ensure everything works correctly.\nIn this section, you will:\nCreate a test event in the Lambda Console Send a sample question to the Bedrock model Review the response and check logs if any errors occur 🔹 Step 1 — Open Lambda and Create a Test Event Go to AWS Lambda Console Select the function: bedrock-chatbot-lambda Click the Test button Choose Create new event if this is your first test Enter an event name, for example: Event name: test-bedrock-converse Scroll down to the JSON editor and replace the entire content with:\n{ \u0026#34;question\u0026#34;: \u0026#34;What is Amazon Bedrock?\u0026#34; } Example illustration:\nClick Save to store the test event.\n🔹 Step 2 — Run the Test and Review the Output Select the event you just created Click Test If everything is working correctly, Lambda should return a response similar to:\n{ \u0026#34;answer\u0026#34;: \u0026#34;Amazon Bedrock is a fully managed service...\u0026#34; } The \u0026quot;answer\u0026quot; field contains the model’s generated response.\n🔹 Step 3 — Check CloudWatch Logs if Errors Occur If the Lambda function throws an error or behaves unexpectedly:\nOpen the Monitor tab Click View logs in CloudWatch Open the most recent log stream to inspect the execution details Common issues and fixes:\n❌ AccessDeniedException Cause: Lambda Execution Role lacks bedrock:InvokeModel Fix: Re-check the IAM Role created in the Prerequisites section ❌ Timeout Cause: Default Lambda timeout (3 seconds) is too short Fix: Go to Configuration → General and increase timeout to 10–20 seconds ❌ KeyError or missing question field Cause: Payload is missing the \u0026quot;question\u0026quot; key Fix: Ensure the test event JSON is formatted like: { \u0026#34;question\u0026#34;: \u0026#34;Your question here\u0026#34; } 🎯 Expected Outcome After completing this step, you will:\nConfirm that Lambda successfully invokes Bedrock Receive responses from the model via the Converse API Verify that your IAM Role and MODEL_ID are correctly configured Be ready to move on to building the API endpoint "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Migrating from AWS CodeDeploy to Amazon ECS for blue/green deployments This blog introduces how to migrate from AWS CodeDeploy to Amazon ECS blue/green deployments. It explains that ECS now supports native blue/green deployments, removing the need for CodeDeploy. Key advantages include support for ECS ServiceConnect, headless services, Amazon EBS volumes, and multiple target groups. It details API, CLI, and console differences, lifecycle hook mappings, and deployment workflow variations. Three migration options are outlined:\nIn-place update simplest, no downtime New service using existing load balancer safer, minimal disruption New service with new load balancer zero downtime, higher cost Blog 2 - Break down data silos and seamlessly query Iceberg tables in Amazon SageMaker from Snowflake This blog explains how to query Apache Iceberg tables in Amazon SageMaker lakehouse directly from Snowflake, enabling unified analytics without ETL or data duplication.\nUsing AWS Glue Data Catalog, Lake Formation, and the Glue Iceberg REST endpoint, Snowflake users can securely query data stored in Amazon S3.\nThe integration supports credential vending via Lake Formation and SigV4 authentication for secure access. It enables seamless cross-platform analytics, improving data accessibility, consistency, and cost efficiency.\nThe guide outlines prerequisites, IAM and Lake Formation setup, and Snowflake catalog integration steps, empowering organizations to break data silos and enhance decision-making across their data ecosystem.\nBlog 3 - Navigating Amazon GuardDuty protection plans and Extended Threat Detection This blog introduces Amazon GuardDuty, which now offers a suite of protection plans and Extended Threat Detection (ETD) to enhance AWS security monitoring.\nProtection plans extend coverage to specific services—S3, EKS, EC2, ECS, Lambda, RDS, and more—detecting threats such as malware, privilege escalation, and data exfiltration.\nETD leverages AI/ML to correlate events across services, identifying multi-stage attacks with high confidence and mapping them to MITRE ATTACK tactics.\nGuardDuty recommends tailored plan combinations by workload type (EC2, containerized, serverless, database, or regulated environments).\nTogether, these features deliver comprehensive, automated threat detection and actionable insights to secure AWS workloads with minimal operational overhead.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.3-week3/","title":"Week 3 - AWS Compute Services","tags":[],"description":"","content":"Week: 2025-09-22 to 2025-09-26\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 3 Overview This week highlighted AWS compute services, especially Amazon EC2 and supporting capabilities for scaling, storage, and pricing.\nKey Topics Amazon EC2 and instance families AMIs and backup strategies EBS volumes vs. Instance Store EC2 Auto Scaling patterns EC2 pricing models Amazon Lightsail, EFS, and FSx Hands-on Labs Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.4-api-gateway-integration/","title":"Create API Gateway","tags":[],"description":"","content":"Create API Gateway In this section, you will create an HTTP API on Amazon API Gateway so that clients can send questions to your Lambda function.\nAPI Gateway will act as the HTTP endpoint intermediary between the client and your AI Q\u0026amp;A service.\nContents 5.4.1 – Create HTTP API 5.4.2 – Integrate API with Lambda After completing this section, you will have a public (or private, depending on configuration) HTTP endpoint that allows clients such as Postman, cURL, or web frontends to send questions to Lambda and receive responses from Bedrock.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.1-week1/1.1.4-day04-2025-09-11/","title":"Day 04 - Cost Optimization on AWS","tags":[],"description":"","content":"Date: 2025-09-11 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Cost Optimization on AWS Cost Optimization Strategies Choose the right resource types and Regions. Use pricing models such as Reserved Instances, Savings Plans, and Spot Instances. Remove or schedule idle resources. Leverage serverless architectures. Continuously review and improve cost efficiency with AWS Budgets and Cost Explorer. Tag resources with Cost Allocation Tags for department-level tracking. AWS Pricing Calculator calculator.aws\nCreate and share cost estimates for common services. Pricing varies by Region. Key Features:\nEstimate costs before deployment Compare pricing across regions Export and share estimates Template-based estimation AWS Support Plans Four tiers: Basic, Developer, Business, and Enterprise. Plans can be upgraded temporarily during critical incidents. Support Plan Comparison Feature Basic Developer Business Enterprise Cost Free $29/month $100/month $15,000/month Response Time N/A 12-24 hours 1 hour (urgent) 15 min (critical) Technical Support Forums only Business hours 24/7 24/7 + TAM Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Create Budget by Template → 07-01 Create Cost Budget Tutorial → 07-02 Create Usage Budget → 07-03 Create Reserved Instance (RI) Budget → 07-04 Create Savings Plans Budget → 07-05 Clean Up Budgets → 07-06 Lab 09 – AWS Support Plans AWS Support Packages → 09-01 Types of Support Requests → 09-02 Change Support Package → 09-03 Manage Support Requests → 09-04 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.2-week2/1.2.4-day09-2025-09-18/","title":"Day 09 - VPC Connectivity &amp; Load Balancing","tags":[],"description":"","content":"Date: 2025-09-18 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Peering \u0026amp; Transit Gateway VPC Peering Enables direct, private connectivity between two VPCs without traversing the Internet. Does not support transitive routing or overlapping CIDRs. VPC Peering Limitations:\nNo transitive peering No overlapping CIDR blocks Limited to 125 peering connections per VPC Cross-region peering supported AWS Transit Gateway (TGW) Acts as a hub to connect multiple VPCs and on-prem networks, simplifying complex mesh topologies. TGW Attachments associate subnets in specific AZs with a TGW. All subnets within the same AZ can reach the TGW once attached. Transit Gateway Benefits:\nCentralized connectivity hub Simplified network architecture Scalable to thousands of VPCs Supports inter-region peering VPN \u0026amp; Direct Connect Site-to-Site VPN Establishes a secure IPSec connection between an on-premises data center and AWS VPC. Consists of: Virtual Private Gateway (VGW): AWS-managed, multi-AZ endpoints. Customer Gateway (CGW): Customer-managed device or software appliance. AWS Direct Connect Provides a dedicated private network connection between an on-prem data center and AWS. Typical latency: 20–30 ms. In Vietnam, available through Hosted Connections (via partners). Bandwidth is adjustable. Hands-On Labs Lab 10 – Hybrid DNS (Route 53 Resolver) Generate Key Pair → 10-02.1 Initialize CloudFormation Template → 10-02.2 Configure Security Group → 10-02.3 Set up DNS System → 10-05 Create Route 53 Outbound Endpoint → 10-05.1 Create Resolver Rules → 10-05.2 Create Inbound Endpoints → 10-05.3 Lab 19 – VPC Peering Initialize CloudFormation Templates → 19-02.1 Create Security Group → 19-02.2 Create EC2 Instance (Test Peering) → 19-02.3 Create Peering Connection → 19-04 Configure Route Tables (Cross-VPC) → 19-05 Enable Cross-Peer DNS → 19-06 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.3-week3/1.3.4-day14-2025-09-25/","title":"Day 14 - EC2 Auto Scaling","tags":[],"description":"","content":"Date: 2025-09-25 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon EC2 Auto Scaling EC2 Auto Scaling automatically adjusts the number of EC2 instances based on demand. Benefits\nElastic capacity adjustment Increased application availability Cost optimization Components\nAuto Scaling Group (ASG) – logical group of EC2 instances Launch Template / Configuration – defines instance parameters Scaling Policies – rules for adding/removing instances Scaling Policies Simple / Step Scaling – add/remove instances when thresholds are met Target Tracking – maintain a metric (e.g., CPU = 50%) Scheduled Scaling – scale on a predefined schedule Predictive Scaling – uses ML to forecast and scale proactively Scaling Policy Examples:\n{ \u0026#34;TargetTrackingScalingPolicyConfiguration\u0026#34;: { \u0026#34;PredefinedMetricSpecification\u0026#34;: { \u0026#34;PredefinedMetricType\u0026#34;: \u0026#34;ASGAverageCPUUtilization\u0026#34; }, \u0026#34;TargetValue\u0026#34;: 50.0 } } Integration with Load Balancer ASGs often pair with Elastic Load Balancers (ELB). New instances automatically register; terminated instances deregister automatically. Auto Scaling Best Practices:\nUse multiple AZs for high availability Set appropriate cooldown periods Monitor CloudWatch metrics Use lifecycle hooks for custom actions Test scaling policies before production EC2 Pricing Options On-Demand: Pay per hour/second. Most expensive but flexible. Reserved Instances: 1- or 3-year commitment for discount; tied to specific instance type/family. Savings Plans: 1- or 3-year commitment; flexible across instance families. Spot Instances: Use spare capacity at up to 90% discount; can be terminated with 2-minute notice. Combine multiple pricing models within an Auto Scaling Group for cost optimization.\nPricing Comparison:\nModel Discount Flexibility Commitment On-Demand 0% High None Reserved 40-60% Low 1-3 years Savings Plans 40-60% Medium 1-3 years Spot 50-90% Low None Hands-On Labs Lab 09 – AWS Support Plans AWS Support Packages → 09-01 Types of Support Requests → 09-02 Change Support Package → 09-03 Manage Support Requests → 09-04 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.4-week4/1.4.4-day19-2025-10-02/","title":"Day 19 - Disaster Recovery on AWS","tags":[],"description":"","content":"Date: 2025-10-02 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Disaster Recovery (DR) on AWS Disaster Recovery is about restoring IT services after major incidents (outages, disasters, hardware failures, cyberattacks).\nRTO (Recovery Time Objective): How quickly to restore service. RPO (Recovery Point Objective): How much data loss (time window) is acceptable. DR Strategies (ordered by complexity \u0026amp; cost) Backup \u0026amp; Restore\nMaintain backups only (EBS/RDS snapshots, S3/Glacier). Restore to new infrastructure during incidents. RTO: hours–days. RPO: depends on backup frequency. Cost: lowest. Pilot Light\nMinimal core services always running on AWS. Scale out to full production during DR. RTO: hours. RPO: minutes. Cost: moderate. Warm Standby\nFull system running at reduced scale on AWS. Scale up on failover. RTO: minutes–hours. RPO: seconds–minutes. Cost: higher. Multi-Site (Active/Active or Active/Passive)\nProduction running across on-prem and AWS, or multi-Region AWS. Traffic can be shifted instantly (Route 53, Global Accelerator). RTO/RPO: near zero. Cost: highest. DR Strategy Comparison:\nStrategy RTO RPO Cost Complexity Backup \u0026amp; Restore Hours-Days Hours $ Low Pilot Light Hours Minutes $$ Medium Warm Standby Minutes Seconds $$$ Medium-High Multi-Site Seconds Near-zero $$$$ High DR Best Practices Planning Define RTO and RPO requirements Document recovery procedures Identify critical systems and dependencies Establish communication plans Implementation Automate recovery processes Use multiple AZs and Regions Implement data replication Regular backup testing Testing Conduct DR drills regularly Test recovery procedures Measure actual RTO/RPO Update documentation Hands-On Labs Lab 14 – AWS VM Import/Export (Part 2) Import Virtual Machine to AWS → 14-02.3 Deploy Instance from AMI → 14-02.4 Set Up S3 Bucket ACL → 14-03.1 Export Virtual Machine from Instance → 14-03.2 Resource Cleanup on AWS → 14-05 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.5-week5/1.5.4-day24-2025-10-09/","title":"Day 24 - SCPs, Identity Center &amp; KMS","tags":[],"description":"","content":"Date: 2025-10-09 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Service Control Policies (SCPs) Define maximum permissions for accounts; they limit but do not grant permissions. Apply to accounts or OUs; affect all users/roles, including root; Deny overrides Allow. Example SCP (deny bucket deletion)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:DeleteBucket\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } SCP Use Cases:\nPrevent accounts from leaving the organization Restrict regions where resources can be created Enforce encryption requirements Prevent disabling security services Require specific tags on resources SCP Best Practices:\nStart with least privilege Test in non-production first Use explicit denies for critical controls Document SCP purposes Regular review and updates AWS Identity Center (formerly AWS SSO) Centralizes access to AWS accounts and external applications. Identity sources: built-in, AWS Managed Microsoft AD, on-prem AD (trust/AD Connector), or external IdPs. Permission Sets define what users/groups can do in target accounts (materialized as IAM roles). Multiple permission sets per user are supported. Identity Center Features:\nSingle sign-on to multiple AWS accounts Integration with Microsoft Active Directory SAML 2.0 support Multi-factor authentication Centralized permission management Audit logging with CloudTrail AWS Key Management Service (KMS) Managed keys for data protection with deep service integration and full auditability. Highlights\nCreate/manage keys without operating your own HSM infrastructure. Fine-grained access via IAM \u0026amp; key policies; usage logged in CloudTrail. Key categories\nCustomer-managed keys, AWS-managed keys, and AWS-owned keys. KMS Key Types:\nSymmetric: Single encryption key (AES-256) Asymmetric: Public/private key pair (RSA, ECC) KMS Features:\nAutomatic key rotation Key policies and grants Envelope encryption Integration with AWS services CloudTrail logging Multi-region keys Hands-On Labs Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Part 1) Create Policy and Role → 33-2.1 Create Group and User → 33-2.2 Create KMS Key → 33-3 Create S3 Bucket → 33-4.1 Upload Data to S3 → 33-4.2 Lab 30 – IAM Restriction Policy Create Restriction Policy → 30-3 Create IAM Limited User → 30-4 Test IAM User Limits → 30-5 Clean Up Resources → 30-6 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.6-week6/1.6.4-day29-2025-10-16/","title":"Day 29 - Amazon ElastiCache","tags":[],"description":"","content":"Date: 2025-10-16 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon ElastiCache Managed in-memory caching service for Redis and Memcached to reduce latency and offload databases.\nMicrosecond reads, Multi-AZ with failover, simple scaling, encryption/auth, automated ops. Redis: rich data structures, backups, replication, cluster mode. Memcached: simple, horizontally scalable cache with auto-discovery. Common uses: web/mobile acceleration, DB query caching, session stores, leaderboards, pub/sub, queues.\nElastiCache for Redis Features:\nData Structures: Strings, lists, sets, sorted sets, hashes, bitmaps, hyperloglogs Persistence: Snapshots and AOF (Append-Only File) Replication: Primary-replica with automatic failover Cluster Mode: Partition data across multiple shards Pub/Sub: Real-time messaging Lua Scripting: Server-side scripting Geospatial: Location-based queries ElastiCache for Memcached Features:\nMulti-threaded: Utilize multiple cores Auto Discovery: Automatic node discovery Horizontal Scaling: Add/remove nodes easily Simple: Easy to use, no persistence Redis vs Memcached:\nFeature Redis Memcached Data Structures Rich (lists, sets, etc.) Simple (key-value) Persistence Yes No Replication Yes No Multi-AZ Yes No Backup/Restore Yes No Pub/Sub Yes No Multi-threaded No Yes Caching Strategies Cache-Aside (Lazy Loading) Application checks cache first On miss, load from database and populate cache Pros: Only requested data is cached Cons: Cache miss penalty, stale data possible Write-Through Write to cache and database simultaneously Pros: Data always fresh, no cache misses on reads Cons: Write penalty, unused data may be cached Write-Behind (Write-Back) Write to cache immediately, async write to database Pros: Fast writes, reduced database load Cons: Risk of data loss, complexity Use Cases Session Store:\n# Store user session in Redis redis.setex(f\u0026#34;session:{user_id}\u0026#34;, 3600, session_data) # Retrieve session session = redis.get(f\u0026#34;session:{user_id}\u0026#34;) Leaderboard:\n# Add score to sorted set redis.zadd(\u0026#34;leaderboard\u0026#34;, {user_id: score}) # Get top 10 top_10 = redis.zrevrange(\u0026#34;leaderboard\u0026#34;, 0, 9, withscores=True) Rate Limiting:\n# Increment counter with expiry pipe = redis.pipeline() pipe.incr(f\u0026#34;rate:{user_id}\u0026#34;) pipe.expire(f\u0026#34;rate:{user_id}\u0026#34;, 60) count = pipe.execute()[0] if count \u0026gt; 100: raise RateLimitExceeded() Hands-On Labs Lab 43 – AWS Database Migration Service (DMS) (Part 3) Inspect S3 → 43-12 Create Serverless Migration → 43-13 Create Event Notification → 43-14 Logs → 43-15 Troubleshoot: Memory Pressure → 43-16 Troubleshoot: Table Error → 43-17 "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.7-week7/1.7.4-day34-2025-10-23/","title":"Day 34 - FastAPI Clean Architecture","tags":[],"description":"","content":"Date: 2025-10-23 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Clean Architecture Overview Separate config, models, routes, and core logic to simplify scaling. Keep main.py lightweight—only boot the app, load config, and mount routers. Use Pydantic models to standardize request/response shapes so the contract matches OpenAPI. backend/ ├── main.py ├── core/ │ └── config.py ├── models/ │ └── book.py ├── routes/ │ └── books.py └── services/ └── books.py Configuration \u0026amp; Dependencies core/config.py reads environment variables and centralizes CORS, API prefixes, and debug flags. Use FastAPI’s dependency injection to pass the service layer into routers. Enables swapping data sources (in-memory → PostgreSQL) without changing function contracts. CORS \u0026amp; API Stability Restrict CORS to required origins (http://localhost:3000 during development). Allow only GET for the first slice to shrink the attack surface. Keep /openapi.json accessible so contract-testing tools can pull the spec. Start Simple, Refactor Later Begin with an in-memory repository for fast demos, then add a real database. Document TODOs clearly to avoid losing them between sprints. Keep logging minimal and focus on critical faults (timeouts, data mismatches). Hands-On Labs Refactor main.py so it only handles app bootstrapping and router registration. Build the get_book_detail(id) service layer using spec-driven fake data. Configure CORSMiddleware to match the frontend mock and production URLs. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.8-week8/1.8.4-day39-2025-10-30/","title":"Day 39 - NMT &amp; Text Summarization","tags":[],"description":"","content":"Date: 2025-10-30 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nNeural Machine Translation (NMT) Architecture Overview The input sentence is converted to a numerical representation and encoded into a deep representation by a six-layer encoder, which is subsequently decoded by a six-layer decoder into the translation in the target language.\nEncoder and Decoder Layers Layers consist of:\nSelf-attention: Helps model focus on different parts of input Feed-forward layers: Process information Encoder-decoder attention layer (decoder only): Uses deep representation from last encoder layer Attention Mechanism Example Translation Task: \u0026ldquo;The woman took the empty magazine out of her gun\u0026rdquo;\nTarget Language: Czech\nVisualization of Self-Attention When translating \u0026ldquo;magazine\u0026rdquo;, the attention mechanism:\nCreates strong attention link between \u0026lsquo;magazine\u0026rsquo; and \u0026lsquo;gun\u0026rsquo; This helps correctly translate \u0026ldquo;magazine\u0026rdquo; as \u0026ldquo;zásobník\u0026rdquo; (gun magazine) Instead of \u0026ldquo;časopis\u0026rdquo; (news magazine) Why Attention Matters Attention = mechanism that helps model focus on the most important parts of input when generating output\nIn other words: Attention = selective information processing instead of consuming everything at once\nIn NLP, attention allows the model to decide which words most strongly influence understanding another word in the sentence.\nNMT Implementation Details Model Architecture Components: Inputs: Input tokens (source language) Target tokens (target language) Step 1: Make Copies Create two copies each of input and target tokens (needed in different places of model)\nStep 2: Encoder One copy of input tokens → encoder Transform into key and value vectors Go through embedding layer → LSTM Step 3: Pre-attention Decoder One copy of target tokens → pre-attention decoder Shift sequence right + add start-of-sentence token (teacher forcing) Go through embedding layer → LSTM Output becomes query vectors Note: Encoder and pre-attention decoder can run in parallel (no dependencies)\nStep 4: Prepare for Attention Get query, key, value vectors Create padding mask to identify padding tokens Use copy of input tokens for this step Step 5: Attention Layer Pass queries, keys, values, and mask to attention layer\nOutputs context vectors and mask Step 6: Post-attention Decoder Drop mask, pass context vectors through:\nLSTM Dense layer LogSoftmax Step 7: Output Model returns:\nLog probabilities Copy of target tokens (for loss computation) Text Summarization Summarization = condensing content while preserving main ideas\nTwo Types: 1. Extractive Summarization Concept: Select the most important sentences from original text\nCharacteristics:\nDoesn\u0026rsquo;t rewrite text Preserves original wording Like \u0026ldquo;highlighting key sentences\u0026rdquo; Process (Classical TextRank):\nSplit into sentences Convert sentences to embeddings Calculate similarity (cosine) Create graph (sentences as nodes) Rank using TextRank Select top-ranked sentences Result: Subset of original text\n2. Abstractive Summarization Concept: Rewrite main ideas in new sentences\nCharacteristics:\nCreates sentences that never appeared in original Understands content → paraphrases Requires strong models (seq2seq, Transformer) Example: Original article discusses prosecutor\u0026rsquo;s investigation process\u0026hellip;\nGenerated summary:\n\u0026ldquo;Prosecutor: So far no videos were used in the crash investigation.\u0026rdquo;\nThis sentence doesn\u0026rsquo;t exist in original but captures the main idea.\nExtractive vs Abstractive Summary Feature Extractive Abstractive Approach Select existing sentences Generate new sentences Creativity Low High Complexity Simpler More complex Accuracy More faithful to source May introduce errors Model TextRank, graph-based Seq2seq, Transformer TextRank Pipeline Step-by-step extractive summarization:\nCombine articles → full text Split sentences Convert sentences → vectors (embeddings) Create similarity matrix Build graph (sentences = nodes, edges = similarity) Rank nodes using TextRank algorithm Select top-ranked sentences → Summary This is the classical algorithm that dominated before deep learning!\nSyntax and Semantics Review Syntax – Sentence Structure Syntax examines how words combine to form grammatically correct sentences.\nIncludes: Word order: English uses S–V–O (Subject–Verb–Object) Phrase structure: NP (Noun Phrase), VP (Verb Phrase), PP (Prepositional Phrase) Dependency relations: How words relate to each other NLP Relevance: POS tagging Parsing Named Entity Recognition Machine translation Question answering Semantics – Meaning of Words and Sentences Semantics focuses on meaning independent of external context.\nIncludes: Lexical semantics: Word meaning Compositional semantics: Sentence meaning Synonymy / antonymy: Similar/opposite meanings Hypernymy / hyponymy: General/specific relationships NLP Relevance: Word embeddings Similarity measures Semantic search Text classification Pragmatics – Intended Meaning in Context Pragmatics studies meaning from context, speaker intention, and real-world knowledge.\nCovers: Implicature: Hidden meaning Deixis: Context-dependent references (this/that/here/you) Speech acts: Promises, requests, apologies Politeness, formality, sarcasm: Tone and intention NLP Relevance: Dialogue systems Chatbots Sentiment and irony detection Contextual language models (BERT, GPT) "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.9-week9/1.9.4-day44-2025-11-06/","title":"Day 44 - Attention Types: Self, Masked, and Encoder-Decoder","tags":[],"description":"","content":"Date: 2025-11-06 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nThree Types of Attention in Transformers The transformer uses attention in three different ways. Understanding each is crucial.\nType 1: Self-Attention (Encoder) Definition: Each position attends to all positions in the same sequence.\nUse Case: In the encoder, we want each word to understand its context by looking at all other words.\nExample:\nSentence: \u0026#34;The cat sat on the mat\u0026#34; For word \u0026#34;cat\u0026#34;: - Attend to \u0026#34;The\u0026#34;: 0.15 (article) - Attend to \u0026#34;cat\u0026#34;: 0.40 (itself) - Attend to \u0026#34;sat\u0026#34;: 0.20 (verb) - Attend to \u0026#34;on\u0026#34;: 0.10 - Attend to \u0026#34;the\u0026#34;: 0.08 - Attend to \u0026#34;mat\u0026#34;: 0.07 Result: \u0026#34;cat\u0026#34; context = weighted combination of all 6 words Why useful:\nCaptures full sentence context Can identify relationships (subject-verb, adjective-noun, etc.) Each word gets information from entire sentence Implementation:\nQ = K = V = Input (same source!) attention(Q, K, V) = softmax(Q×K^T / √d_k) × V Since Q, K, V come from the same place, it\u0026rsquo;s called \u0026ldquo;self-attention\u0026rdquo;.\nType 2: Masked Self-Attention (Decoder) Problem: During training, if the decoder can \u0026ldquo;see\u0026rdquo; future words, it cheats!\nExample - The Problem:\nTask: Translate \u0026#34;Je suis heureux\u0026#34; → \u0026#34;I am happy\u0026#34; Training: Step 1: Predict \u0026#34;am\u0026#34; using... \u0026#34;am\u0026#34; (it can see the answer!) Step 2: Predict \u0026#34;happy\u0026#34; using \u0026#34;I am happy\u0026#34; (knows the answer!) Step 3: Predict \u0026#34;happy\u0026#34; is done (cheating!) Result: Model trains perfectly but fails at test time! Solution: Mask (hide) future positions during self-attention.\nMasked Self-Attention:\nInstead of: We do: [0.30, 0.33, 0.37] [0.30, -∞, -∞] [0.26, 0.37, 0.37] → [0.26, 0.37, -∞] [0.25, 0.36, 0.39] [0.25, 0.36, 0.39] After softmax: [1.00, 0.00, 0.00] [0.30, 0.70, 0.00] [0.25, 0.36, 0.39] (normalized) Mask Matrix:\nMask = [1, 0, 0] [1, 1, 0] [1, 1, 1] Or: -∞ for masked positions Effect:\nPosition 0: Attends to position 0 only Position 1: Attends to positions 0, 1 only Position 2: Attends to positions 0, 1, 2 Decoder can only use past information! Why this works:\nDuring training, can use autoregressive generation During inference, generates word-by-word naturally Prevents the model from \u0026ldquo;seeing the answer\u0026rdquo; Type 3: Encoder-Decoder Attention Purpose: Decoder attends to encoder output.\nExample:\nEncoder processes: \u0026#34;Je suis heureux\u0026#34; (French) Produces: Context vectors C Decoder processes: \u0026#34;\u0026#34; (empty start) For generating first word: - Query: from decoder (what should I translate?) - Key, Value: from encoder (what French words should I look at?) Result: Decoder attends to French words to generate English Key Difference from Self-Attention:\nSelf-Attention: Encoder-Decoder: Q, K, V all from input Q from decoder Same sequence K, V from encoder Attends within self Attends to other sequence Use Cases:\nDecoder looking back at encoder output Allows translation: French → English Allows summarization: Document → Summary Generally useful for seq2seq tasks Comparison: All Three Types Type Q Source K, V Source Purpose Self-Attention Input Input Understand context within same sequence Masked Self-Attention Input Input (masked future) Autoregressive generation, prevent cheating Encoder-Decoder Decoder Encoder Cross-sequence understanding Masked Attention in Detail The Math Before masking:\nAttention = softmax(Q×K^T / √d_k) × V With masking:\nScores = Q×K^T / √d_k Mask matrix M: M[i,j] = 0 if j \u0026lt;= i (allowed) M[i,j] = -∞ if j \u0026gt; i (mask future) Masked_scores = Scores + M Attention = softmax(Masked_scores) × V Example with Real Numbers Original attention scores (3×3):\n[0.1, 0.2, 0.3] [0.4, 0.5, 0.6] [0.7, 0.8, 0.9] Mask matrix:\n[0, -∞, -∞] [0, 0, -∞] [0, 0, 0] After adding mask:\n[0.1, -∞, -∞] [0.4, 0.5, -∞] [0.7, 0.8, 0.9] After softmax (applying exp and normalize):\nexp(0.1) / exp(0.1) = 1.0, softmax([0.1]) = [1.0] So: Row 0: [1.0, 0, 0] exp(0.4) ≈ 1.49, exp(0.5) ≈ 1.65 Row 1: [1.49/(1.49+1.65), 1.65/(1.49+1.65), 0] ≈ [0.47, 0.53, 0] Row 2: softmax([0.7, 0.8, 0.9]) (all allowed) Final attention weights:\n[1.0, 0.0, 0.0] [0.47, 0.53, 0.0] [0.25, 0.33, 0.42] Key insight: Position 2 can only use information from positions 0, 1, 2 (not future)\nComplete Transformer Attention Flow INPUT: \u0026#34;Je suis heureux\u0026#34; ↓ ENCODER LAYERS (repeat 6 times): ├─ Self-Attention: Each French word attends to all French words ├─ Feed-Forward → Output: C (French context vectors) DECODER LAYERS (repeat 6 times): ├─ Masked Self-Attention: Each generated word attends to previous words ├─ Encoder-Decoder Attention: Generated word attends to French context ├─ Feed-Forward → Output: Logits for next word prediction OUTPUT: \u0026#34;I am happy\u0026#34; Key Insights ✅ Self-Attention: Bidirectional understanding (encoder) ✅ Masked Attention: Unidirectional generation (decoder) ✅ Encoder-Decoder: Cross-sequence transfer ✅ Masking prevents cheating: Model can\u0026rsquo;t use future information\nWhy Not Always Use All Three? BERT (Encoder-only): Uses only self-attention (bidirectional, good for classification) GPT (Decoder-only): Uses only masked self-attention (autoregressive, good for generation) T5 (Full): Uses all three (balanced, good for seq2seq tasks) Next: Implementation Now that we understand the three attention types, we\u0026rsquo;ll see how to implement them in code!\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.10-week10/1.10.4-day49-2025-11-13/","title":"Day 49 - T5 Text-to-Text Multitask","tags":[],"description":"","content":"Date: 2025-11-13 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nOne Model, Many Tasks T5 frames everything as text-to-text, so the same model handles QA, summarization, translation, and classification via prompts.\nPrompt-Based Framing Examples: question: When is Pi Day? context: ... -\u0026gt; March 14 summarize: \u0026lt;article\u0026gt; translate English to German: \u0026lt;sentence\u0026gt; Consistent format lets the model share representations across tasks. Data Scale Matters Pre-trained on the C4 corpus (~800 GB) vs. Wikipedia (~13 GB). Larger, cleaner corpora improve downstream generalization. Multitask Benefits Shared encoder-decoder improves transfer between tasks. Better low-resource performance thanks to cross-task signals. Practical Notes Control output length with decoder max_length and repetition penalty. For QA, ensure prompts clearly separate question and context. Mixed-task fine-tuning: balance batches to avoid task dominance. Practice Targets for Today Draft prompts for your QA and summarization tasks. Decide model size vs. GPU budget (T5-small/base/large). Plan a multitask mix (ratio per task) for fine-tuning. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.11-week11/1.11.4-day54-2025-11-20/","title":"Day 54 - Networking &amp; Observability","tags":[],"description":"","content":"Date: 2025-11-20 (Thursday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nTraffic Paths and Logging All traffic from LMI functions egresses through the instance ENI in the provider VPC; plan connectivity and monitoring accordingly.\nEgress \u0026amp; Destinations Dependency access must route from provider VPC (NAT/Transit/VPC peering/PrivateLink) CloudWatch logs also traverse the instance ENI; allow path to endpoint (public or PrivateLink) Security Groups No inbound needed; keep SG inbound closed Outbound rules must cover dependencies + CloudWatch Observability CloudWatch logging remains native; ensure endpoint reachability Monitor instance metrics (EC2 billing, vCPU usage) + Lambda metrics (invokes, errors) Gotchas Function-level VPC settings are ignored for LMI Instance-level limits (bandwidth/ENI) still apply; size instances accordingly "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.12-week12/1.12.4-day59-2025-11-27/","title":"Day 59 - SageMaker AI Platform","tags":[],"description":"","content":"Date: 2025-11-27 (Thursday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nServerless \u0026amp; Resilient Training SageMaker AI serverless MLflow for quick experimentation (zero infra, auto scale) HyperPod training adds checkpointless recovery and elastic scaling based on resource availability Benefits Faster experiment cycles without cluster setup Reduced failure recovery overhead; better utilization of heterogeneous capacity Action Items Set up a small MLflow serverless workspace for current experiments Test checkpointless/elastic training on a representative model; note cost/time deltas Update MLOps playbook to include new training modes and failure handling "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship period, I was fortunate to attend multiple meaningful events that significantly enhanced my professional development through insightful knowledge and unforgettable experiences.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 – 17:00 VNT, Thursday, September 18, 2025\nLocation: 36th Floor, 2 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Vietnam Cloud Day 2025 served as an extensive AWS gathering that brought together government representatives, AWS leaders, and prominent industry figures for keynote presentations. The conference offered two primary tracks: a live broadcast session featuring keynotes and panel conversations about the GenAI transformation and executive strategies, along with specialized breakout sessions addressing unified data infrastructure for AI/analytics, GenAI implementation strategies, AI-powered development workflows, GenAI application security, and productivity-enhancing AI agents. This event highlighted AWS\u0026rsquo;s newest offerings and forward-looking plans for AI advancement and cloud transformation.\nOutcomes: Acquired knowledge about enterprise-level AI deployment approaches, explored AWS solutions for data infrastructure and GenAI rollout, and grasped security best practices for AI applications while learning modernization techniques for outdated systems.\nEvent 2 Event Name: AWS GenAI Builder Club - AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 (2:00 PM), Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nDescription: This AWS GenAI Builder Club gathering centered on the AI-Driven Development Lifecycle (AI-DLC), examining how generative AI revolutionizes software creation from initial design through deployment and ongoing maintenance. The session showcased live demonstrations of Amazon Q Developer and Kiro, illustrating how AI can handle routine tasks and free developers to concentrate on more valuable, innovative work. The program covered AI-DLC fundamentals, Amazon Q Developer features, and interactive Kiro walkthroughs.\nOutcomes: Discovered real-world AI applications within the software development process, obtained practical exposure to Amazon Q Developer and Kiro tools, and learned effective methods for incorporating AI as a core partner in development workflows to boost efficiency and enhance code standards.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.4-week4/","title":"Week 4 - AWS Storage Services","tags":[],"description":"","content":"Week: 2025-09-29 to 2025-10-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 4 Overview This week deep-dived into AWS storage services, ranging from S3 object storage to hybrid storage integrations.\nKey Topics Amazon S3 and storage classes S3 static website hosting S3 Glacier for archival workloads AWS Snow Family AWS Storage Gateway Disaster Recovery strategies AWS Backup Hands-on Labs Lab 13: AWS Backup Lab 14: AWS VM Import/Export Lab 24: AWS Storage Gateway Lab 25: Amazon FSx Lab 57: Amazon S3 \u0026amp; CloudFront "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.1-week1/1.1.5-day05-2025-09-12/","title":"Day 05 - AWS Well-Architected Framework","tags":[],"description":"","content":"Date: 2025-09-12 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nExploration AWS Well-Architected Framework A set of design principles and best practices for building reliable, secure, efficient, and cost-effective cloud architectures. The Well-Architected Tool in the Console provides self-assessments and improvement guidance. Six Pillars of Well-Architected Framework 1. Operational Excellence Focus on running and monitoring systems Continuous improvement of processes Automation of changes Response to events 2. Security Protect information and systems Identity and access management Detective controls Infrastructure protection Data protection 3. Reliability Recover from failures automatically Scale horizontally for resilience Test recovery procedures Manage change through automation 4. Performance Efficiency Use computing resources efficiently Select the right resource types Monitor performance Make informed decisions 5. Cost Optimization Avoid unnecessary costs Understand spending patterns Select appropriate services Optimize over time 6. Sustainability Minimize environmental impact Understand your impact Maximize utilization Use managed services Best Practices Review Design Principles Stop guessing capacity needs: Use auto-scaling Test at production scale: Clone environments easily Automate architecture experimentation: Use IaC Allow for evolutionary architectures: Design for change Drive architectures using data: Monitor and measure Improve through game days: Practice failure scenarios Week 1 Summary Tuần này đã hoàn thành các kiến thức nền tảng về AWS:\n✅ Hiểu về Cloud Computing và lợi ích\n✅ Nắm được AWS Global Infrastructure\n✅ Biết cách sử dụng AWS Management Tools\n✅ Học về Cost Optimization strategies\n✅ Tìm hiểu AWS Well-Architected Framework\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.2-week2/1.2.5-day10-2025-09-19/","title":"Day 10 - Elastic Load Balancing","tags":[],"description":"","content":"Date: 2025-09-19 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Elastic Load Balancing (ELB) Overview A fully managed service distributing traffic across multiple targets (EC2, containers, etc.). Supports HTTP, HTTPS, TCP, TLS. Can be deployed in public or private subnets. Provides DNS names; only NLB supports static IPs. Includes health checks and access logs (to S3). Supports sticky sessions (session affinity). Types: Application, Network, Classic, and Gateway Load Balancer. Application Load Balancer (ALB) Operates at Layer 7 (HTTP/HTTPS). Supports path-based routing (e.g., /mobile vs /desktop). Targets: EC2, Lambda, IP addresses, containers (ECS/EKS). ALB Features:\nHost-based routing Path-based routing HTTP header-based routing Query string parameter-based routing WebSocket support HTTP/2 support Network Load Balancer (NLB) Operates at Layer 4 (TCP/TLS). Supports static IPs and handles millions of requests per second. Targets: EC2, IP addresses, containers (ECS/EKS). NLB Features:\nUltra-low latency Static IP addresses Preserve source IP Long-lived TCP connections TLS termination Gateway Load Balancer (GWLB) Operates at Layer 3 (IP packets). Uses the GENEVE protocol on port 6081. Routes traffic to virtual appliances such as firewalls or monitoring tools. Partner list: aws.amazon.com/elasticloadbalancing/partners Exploration AWS Advanced Networking – Specialty Study Guide Official study guide covering exam topics, AWS network design principles, and real-world architecture scenarios. Hands-On Labs Lab 20 – AWS Transit Gateway Preparation Steps → 20-02 Create Transit Gateway → 20-03 Create TGW Attachments → 20-04 Create TGW Route Tables → 20-05 Add TGW Routes to VPC Route Tables → 20-06 Week 2 Summary Tuần này đã hoàn thành kiến thức về AWS Networking:\n✅ Amazon VPC và Subnets\n✅ Security Groups và NACLs\n✅ VPC Peering và Transit Gateway\n✅ VPN và Direct Connect\n✅ Elastic Load Balancing (ALB, NLB, GWLB)\nLabs completed: 4 labs (VPC Basics, Hybrid DNS, VPC Peering, Transit Gateway)\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.3-week3/1.3.5-day15-2025-09-26/","title":"Day 15 - Lightsail, EFS &amp; FSx","tags":[],"description":"","content":"Date: 2025-09-26 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Lightsail Simplified compute service with predictable monthly pricing (starting ~$3.5/month). Includes bundled data transfer at lower rates than EC2. Ideal for small workloads, development, or testing environments. Supports snapshots for backups. Runs inside a managed VPC and can connect to standard VPCs via one-click peering. Lightsail Use Cases:\nSimple web applications WordPress sites Development/test environments Small business applications Learning and experimentation Lightsail vs EC2:\nFeature Lightsail EC2 Pricing Fixed monthly Pay-as-you-go Complexity Simple Advanced Scalability Limited Unlimited Target Small projects Enterprise Amazon EFS (Elastic File System) Fully managed NFSv4 file system mountable by multiple EC2 instances simultaneously. Scales automatically to petabytes. Pay only for the storage used (unlike EBS provisioned size). Can be mounted from on-prem via VPN or Direct Connect. EFS Features:\nConcurrent access from multiple instances Automatic scaling Regional service (multi-AZ) Lifecycle management Encryption at rest and in transit EFS Storage Classes:\nStandard: Frequently accessed files Infrequent Access (IA): Lower cost for rarely accessed files One Zone: Single AZ for cost savings Amazon FSx Managed, scalable file systems for Windows, Lustre, and NetApp ONTAP. AWS handles setup, scaling, and backups. Accessible from EC2, on-prem servers, or users via SMB or NFS protocols. FSx Variants:\nFSx for Windows File Server Native Windows file system SMB protocol support Active Directory integration DFS namespaces FSx for Lustre High-performance computing Machine learning workloads Sub-millisecond latencies S3 integration FSx for NetApp ONTAP Multi-protocol (NFS, SMB, iSCSI) Data deduplication and compression Snapshots and replication AWS Application Migration Service (MGN) Used for migrating or replicating physical/virtual servers to AWS for DR or modernization. Continuously replicates source machines to lightweight staging instances on EC2. During cut-over, MGN launches fully functional EC2 instances from the replicated data. Migration Phases:\nInstall agent on source servers Continuous replication to AWS Testing with non-disruptive test instances Cutover to production Exploration Microsoft Workloads on AWS A curated series covering deployment, optimization, and best practices for running Microsoft workloads on AWS. Week 3 Summary Tuần này đã hoàn thành kiến thức về AWS Compute:\n✅ Amazon EC2 và Instance Types\n✅ AMI, EBS, Instance Store\n✅ EC2 Auto Scaling\n✅ EC2 Pricing Options\n✅ Lightsail, EFS, FSx\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.4-week4/1.4.5-day20-2025-10-03/","title":"Day 20 - AWS Backup &amp; FSx","tags":[],"description":"","content":"Date: 2025-10-03 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Backup Centralized backup service for automating and governing data protection at scale.\nKey Capabilities Central management: Define and apply backup policies across services. Multi-service support: EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway, S3, and more. Scheduling \u0026amp; lifecycle: Automate backups and retention. Compliance: Support for governance and audit requirements. Benefits Operational simplicity: No custom scripts or disparate tools. Time savings: Automated, policy-driven protection. Reporting \u0026amp; audit: Visibility into backup status and compliance. Backup Vault Lock Immutability controls to prevent modifications or deletions of protected backups for strict compliance. AWS Backup Features:\nCross-region backup copy Cross-account backup Backup policies (plans) Lifecycle management Encryption at rest Tag-based backup policies Backup Plan Example:\n{ \u0026#34;BackupPlanName\u0026#34;: \u0026#34;DailyBackups\u0026#34;, \u0026#34;Rules\u0026#34;: [{ \u0026#34;RuleName\u0026#34;: \u0026#34;DailyRule\u0026#34;, \u0026#34;ScheduleExpression\u0026#34;: \u0026#34;cron(0 5 ? * * *)\u0026#34;, \u0026#34;StartWindowMinutes\u0026#34;: 60, \u0026#34;CompletionWindowMinutes\u0026#34;: 120, \u0026#34;Lifecycle\u0026#34;: { \u0026#34;DeleteAfterDays\u0026#34;: 30, \u0026#34;MoveToColdStorageAfterDays\u0026#34;: 7 } }] } Exploration AWS Skill Builder Curated learning plans and deep-dive content for storage specialists: Storage Learning Plan: Block Storage Storage Learning Plan: Object Storage Hands-On Labs Lab 13 – AWS Backup Create S3 Bucket → 13-02.1 Deploy Infrastructure → 13-02.2 Create Backup Plan → 13-03 Set Up Notifications → 13-04 Test Restore → 13-05 Clean Up Resources → 13-06 Lab 25 – Amazon FSx (File Systems) Create SSD Multi-AZ File System → 25-2.2 Create HDD Multi-AZ File System → 25-2.3 Create New File Shares → 25-3 Test Performance → 25-4 Monitor Performance → 25-5 Enable Data Deduplication → 25-6 Enable Shadow Copies → 25-7 Manage User Sessions and Open Files → 25-8 Enable User Storage Quotas → 25-9 Scale Throughput Capacity → 25-11 Scale Storage Capacity → 25-12 Delete Environment → 25-13 Week 4 Summary Tuần này đã hoàn thành kiến thức về AWS Storage:\n✅ Amazon S3 và Storage Classes\n✅ S3 Static Website và CORS\n✅ AWS Snow Family\n✅ AWS Storage Gateway\n✅ Disaster Recovery Strategies\n✅ AWS Backup\nLabs completed: 5 labs (Backup, VM Import/Export, Storage Gateway, FSx, S3 \u0026amp; CloudFront)\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.5-week5/1.5.5-day25-2025-10-10/","title":"Day 25 - AWS Security Hub &amp; Automation","tags":[],"description":"","content":"Date: 2025-10-10 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Security Hub Aggregates and prioritizes security findings and posture across accounts/services. Capabilities\nAutomated checks, normalized findings, prioritized remediation workflows. Compliance standards: CIS AWS Foundations, PCI DSS, AWS Foundational Security Best Practices. Integrations\nGuardDuty, Inspector, Macie, Firewall Manager, IAM Access Analyzer, plus partner tools. Outcomes\nLess time aggregating, more time fixing; unified visibility and improved security hygiene. Security Hub Features:\nContinuous security posture monitoring Automated compliance checks Centralized findings across accounts Integration with 50+ AWS and partner services Custom insights and dashboards Automated remediation with EventBridge Security Standards:\nAWS Foundational Security Best Practices: 50+ controls CIS AWS Foundations Benchmark: Industry best practices PCI DSS: Payment card industry standards NIST: National Institute of Standards framework Security Automation AWS Services for Automation:\nAWS Config: Track resource configuration changes Amazon EventBridge: Event-driven automation AWS Lambda: Serverless remediation functions AWS Systems Manager: Automated patching and compliance Common Automation Patterns:\nAuto-remediate non-compliant resources Automated incident response Security group rule validation Encryption enforcement Tag compliance Exploration AWS Certified Security – Specialty: All-in-One Exam Guide (SCS-C01) Comprehensive preparation material for the Security Specialty certification. Hands-On Labs Lab 18 – AWS Security Hub Enable Security Hub → 18-02 Score for Each Set of Criteria → 18-03 Clean Up Resources → 18-04 Lab 22 – AWS Lambda Automation with Slack Create VPC → 22-2.1 Create Security Group → 22-2.2 Create EC2 Instance → 22-2.3 Incoming Webhooks (Slack) → 22-2.4 Create Tag for Instance → 22-3 Create Role for Lambda → 22-4 Function: Stop Instance → 22-5.1 Function: Start Instance → 22-5.2 Check Result → 22-6 Clean Up Resources → 22-7 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Part 2) Use Tags with CLI → 27-2.2 Create a Resource Group → 27-3 Clean Up Resources → 27-4 Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Part 2) Create CloudTrail → 33-5.1 Log to CloudTrail → 33-5.2 Create Amazon Athena → 33-5.3 Query with Athena → 33-5.4 Test \u0026amp; Share Encrypted S3 Data → 33-6 Resource Cleanup → 33-7 Lab 44 – IAM Advanced Role Control Create IAM Group → 44-2 Create IAM Users → 44-3.1 Check Permissions → 44-3.2 Create Admin IAM Role → 44-4.1 Configure Switch Role → 44-4.2 Restrict Switch Role by IP → 44-4.3.1 Restrict Switch Role by Time → 44-4.3.2 Clean Up Resources → 44-5 Week 5 Summary Tuần này đã hoàn thành kiến thức về AWS Security:\n✅ Shared Responsibility Model\n✅ AWS IAM (Users, Groups, Roles, Policies)\n✅ Amazon Cognito\n✅ AWS Organizations \u0026amp; SCPs\n✅ AWS Identity Center\n✅ AWS KMS\n✅ AWS Security Hub\nLabs completed: 8 labs (Security Hub, Lambda Automation, Resource Groups, IAM Policies, KMS \u0026amp; CloudTrail, Advanced Role Control)\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.6-week6/1.6.5-day30-2025-10-17/","title":"Day 30 - Database Migration &amp; Best Practices","tags":[],"description":"","content":"Date: 2025-10-17 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Database Migration Service (DMS) AWS DMS helps migrate databases to AWS quickly and securely with minimal downtime.\nKey Features:\nHomogeneous Migrations: Same database engine (e.g., Oracle to Oracle) Heterogeneous Migrations: Different engines (e.g., Oracle to Aurora) Continuous Replication: Keep source and target in sync Schema Conversion: AWS Schema Conversion Tool (SCT) Migration Types:\nFull Load: One-time migration of existing data Full Load + CDC: Initial load plus ongoing changes CDC Only: Replicate only ongoing changes Supported Sources:\nOracle, SQL Server, MySQL, PostgreSQL, MongoDB, SAP ASE, IBM Db2 Amazon RDS, Amazon Aurora, Amazon S3 Supported Targets:\nAmazon RDS, Amazon Aurora, Amazon Redshift, Amazon DynamoDB Amazon S3, Amazon Elasticsearch, Amazon Kinesis Data Streams Database Best Practices Performance Optimization RDS/Aurora:\nUse appropriate instance types Enable Enhanced Monitoring Optimize queries and indexes Use Read Replicas for read-heavy workloads Enable Performance Insights Redshift:\nChoose appropriate distribution keys Use sort keys for frequently filtered columns Vacuum and analyze tables regularly Use columnar compression Implement workload management (WLM) ElastiCache:\nChoose appropriate node types Use cluster mode for Redis scalability Implement proper cache eviction policies Monitor cache hit rates Use connection pooling Security Best Practices Encryption at Rest: Enable for all databases Encryption in Transit: Use SSL/TLS connections Network Isolation: Deploy in private subnets IAM Authentication: Use for RDS/Aurora when possible Secrets Manager: Store database credentials securely Security Groups: Restrict access to minimum required Audit Logging: Enable CloudWatch Logs and CloudTrail High Availability \u0026amp; Disaster Recovery RDS/Aurora:\nEnable Multi-AZ for production workloads Configure automated backups Test restore procedures regularly Use Aurora Global Database for multi-region DR Implement read replicas in different regions Redshift:\nEnable automated snapshots Copy snapshots to other regions Use Redshift Spectrum for data lake integration Implement cross-region snapshot copy ElastiCache:\nEnable Multi-AZ with automatic failover (Redis) Configure backup and restore (Redis) Use cluster mode for Redis scalability Implement application-level retry logic Cost Optimization Right-sizing: Choose appropriate instance types Reserved Instances: Commit for 1-3 years for discounts Aurora Serverless: For variable workloads Redshift Serverless: For intermittent analytics Storage Optimization: Use appropriate storage types Lifecycle Policies: Archive old data to S3/Glacier Monitor Usage: Use Cost Explorer and Budgets Exploration The Data Warehouse Toolkit Canonical reference for dimensional modeling and DW design patterns. Week 6 Summary Tuần này đã hoàn thành kiến thức về AWS Database Services:\n✅ Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP)\n✅ Amazon RDS \u0026amp; Aurora\n✅ Amazon Redshift\n✅ Amazon ElastiCache\n✅ AWS Database Migration Service\nLabs completed: 2 labs (RDS \u0026amp; EC2 Integration, Database Migration Service)\nTổng kết 6 tuần đầu (8/9 - 17/10/2025) 30 ngày làm việc đã hoàn thành:\nWeek 1: Cloud Computing Fundamentals AWS basics, infrastructure, management tools, cost optimization Week 2: AWS Networking Services VPC, subnets, security groups, load balancing, hybrid connectivity Week 3: AWS Compute Services EC2, AMI, storage, auto scaling, pricing models Week 4: AWS Storage Services S3, Glacier, Snow Family, Storage Gateway, backup \u0026amp; DR Week 5: AWS Security \u0026amp; Identity IAM, Cognito, Organizations, KMS, Security Hub Week 6: AWS Database Services RDS, Aurora, Redshift, ElastiCache, DMS Tổng số labs hoàn thành: 25+ labs\nTiếp theo: Tuần 7-8 sẽ bắt đầu từ ngày 20/10/2025 (Monday)\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.7-week7/1.7.5-day35-2025-10-24/","title":"Day 35 - Contract Testing &amp; Retrospective","tags":[],"description":"","content":"Date: 2025-10-24 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Contract Testing with Schemathesis Run schemathesis run --checks all --workers 4 --url http://127.0.0.1:8000/openapi.yaml to auto-generate tests. Schemathesis explores random scenarios (happy paths, edge cases, missing fields). Ensures the backend doesn’t break the schema when the frontend switches to the real API. Benefits Eliminates the need to handcraft complex test cases. Reduces mismatch risk after refactors. Acts as a quality gate inside the CI pipeline. Mistakes \u0026amp; Fixes Mistake Root Cause Fix Added both error.tsx and not-found.tsx Redundant handling Keep only not-found.tsx Used --base-url in Schemathesis Wrong CLI option Use the correct --url flag Timeout on /openapi.json CORS or slow resp Point Schemathesis to the YAML file Over-engineered backend Too many files early Start simple and refactor later Validated Workflow 1. Define Contract (OpenAPI) 2. Mock API (Prism) 3. Build Frontend with mock data 4. Implement Backend according to the spec 5. Switch to the real API 6. Contract Testing (Schemathesis) Enables frontend and backend teams to work in parallel. Reduces conflicts, accelerates demos, and keeps quality stable. Key Insights Contract-first development keeps specs aligned and shrinks integration bugs. Vertical slices make it possible to release in increments and gather early feedback. Automation (Prism, Schemathesis) cuts manual testing effort. Start simple and refactor gradually as scope grows. Hands-On Labs Run Schemathesis with the latest spec and capture the results. Refresh the README workflow so the whole team can follow it. Prepare the backlog for the next vertical slice based on demo feedback. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.8-week8/1.8.5-day40-2025-10-31/","title":"Day 40 - MT Evaluation &amp; Decoding Strategies","tags":[],"description":"","content":"Date: 2025-10-31 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nBLEU Score – Precision-Based Evaluation BLEU (Bilingual Evaluation Understudy) is an algorithm designed to evaluate machine translation quality.\nHow BLEU Works Core Concept: Compare candidate translation to one or more reference translations (often human translations)\nScore Range: 0 to 1\nCloser to 1 = better translation Closer to 0 = worse translation Calculating BLEU Score Vanilla BLEU (Problematic) Example:\nCandidate: \u0026ldquo;I I am I\u0026rdquo; Reference 1: \u0026ldquo;Eunice said I\u0026rsquo;m hungry\u0026rdquo; Reference 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; Process:\nCount how many candidate words appear in any reference Divide by total candidate words Result: 4/4 = 1.0 (perfect score!)\nProblem: This translation is terrible but gets perfect score! A model that outputs common words will score well.\nModified BLEU (Better) Key Change: After matching a word, exhaust it from references\nSame Example:\n\u0026ldquo;I\u0026rdquo; (first) → matches → exhaust \u0026ldquo;I\u0026rdquo; from references → count = 1 \u0026ldquo;I\u0026rdquo; (second) → no match left → count = 1 \u0026ldquo;am\u0026rdquo; → matches → exhaust \u0026ldquo;am\u0026rdquo; → count = 2 \u0026ldquo;I\u0026rdquo; (third) → no match left → count = 2 Result: 2/4 = 0.5 (more realistic!)\nBLEU Limitations ❌ Doesn\u0026rsquo;t consider semantic meaning\nOnly checks word matches ❌ Doesn\u0026rsquo;t consider sentence structure\n\u0026ldquo;Ate I was hungry because\u0026rdquo; vs \u0026ldquo;I ate because I was hungry\u0026rdquo; Both get same score! ✅ Still most widely adopted metric despite limitations\nROUGE Score – Recall-Based Evaluation ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nBLEU vs ROUGE Metric Focus Calculation BLEU Precision How many candidate words in reference? ROUGE Recall How many reference words in candidate? Calculating ROUGE-N Score Example:\nCandidate: \u0026ldquo;I I am I\u0026rdquo; Reference 1: \u0026ldquo;Younes said I am hungry\u0026rdquo; (5 words) Reference 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; (5 words) Process for Reference 1:\n\u0026ldquo;Younes\u0026rdquo; → no match → count = 0 \u0026ldquo;said\u0026rdquo; → no match → count = 0 \u0026ldquo;I\u0026rdquo; → match → count = 1 \u0026ldquo;am\u0026rdquo; → match → count = 2 \u0026ldquo;hungry\u0026rdquo; → no match → count = 2 ROUGE score for Ref 1: 2/5 = 0.4\nIf multiple references: Calculate for each, take maximum\nF1 Score – Combining BLEU and ROUGE Since BLEU = precision and ROUGE = recall, we can calculate F1 score:\nFormula:\nF1 = 2 × (Precision × Recall) / (Precision + Recall) F1 = 2 × (BLEU × ROUGE) / (BLEU + ROUGE) Example:\nBLEU = 0.5 ROUGE = 0.4 F1 = 2 × (0.5 × 0.4) / (0.5 + 0.4) = 4/9 ≈ 0.44 Beam Search Decoding Problem: Taking the highest probability word at each step doesn\u0026rsquo;t guarantee best overall sequence\nSolution: Beam search finds most likely sequences over a fixed window\nHow Beam Search Works Beam Width (B): Number of sequences to keep at each step\nProcess: Step 1: Start with SOS token Get probabilities for first word:\nI: 0.5 am: 0.4 hungry: 0.1 Keep top B=2: \u0026ldquo;I\u0026rdquo; and \u0026ldquo;am\u0026rdquo;\nStep 2: Calculate Conditional Probabilities For \u0026ldquo;I\u0026rdquo;:\nI am: 0.5 × 0.5 = 0.25 I I: 0.5 × 0.1 = 0.05 For \u0026ldquo;am\u0026rdquo;:\nam I: 0.4 × 0.7 = 0.28 am hungry: 0.4 × 0.2 = 0.08 Keep top B=2: \u0026ldquo;am I\u0026rdquo; (0.28) and \u0026ldquo;I am\u0026rdquo; (0.25)\nStep 3: Repeat Continue until all B sequences reach EOS token\nStep 4: Select Best Choose sequence with highest overall probability\nBeam Search Characteristics Advantages:\nBetter than greedy decoding (B=1) Finds globally better sequences Widely used in production Disadvantages:\nMemory intensive (store B sequences) Computationally expensive (run model B times per step) Penalizes long sequences (product of many probabilities) Solution for Long Sequences: Normalize by length: divide probability by number of words\nMinimum Bayes Risk (MBR) Decoding Concept: Generate multiple samples and find consensus\nMBR Process: Step 1: Generate Multiple Samples Create ~30 random samples from the model\nStep 2: Compare All Pairs For each sample, compare against all others using similarity metric (e.g., ROUGE)\nStep 3: Calculate Average Similarity For each candidate, compute average similarity with all other candidates\nStep 4: Select Best Choose the sample with highest average similarity (lowest risk)\nMBR Formula E* = argmax_E [ average ROUGE(E, E\u0026#39;) for all E\u0026#39; ] Where:\nE = candidate translation E\u0026rsquo; = all other candidates Goal: Find E that maximizes average ROUGE with every E' MBR Example (4 Candidates) Step 1: Calculate pairwise ROUGE scores\nROUGE(C1, C2), ROUGE(C1, C3), ROUGE(C1, C4) Average = R1 Step 2: Repeat for C2, C3, C4\nGet R2, R3, R4 Step 3: Select highest\nChoose candidate with max(R1, R2, R3, R4) MBR Characteristics Advantages:\nMore contextually accurate than random sampling Finds consensus translation Can outperform beam search Disadvantages:\nRequires generating many samples (expensive) Requires O(n²) comparisons When to Use:\nWhen you need high-quality translation When computational cost is acceptable When beam search outputs are inconsistent Summary: Decoding Strategies Method Description Pros Cons Greedy Pick highest prob at each step Fast, simple Suboptimal sequences Beam Search Keep top-B sequences Better quality Memory + compute cost Random Sampling Sample from distribution Diverse outputs Inconsistent quality MBR Consensus from samples High quality Very expensive Evaluation Metrics Summary Metric Type Focus Best For BLEU Precision Candidate → Reference General MT ROUGE Recall Reference → Candidate Summarization F1 Harmonic Mean Both precision \u0026amp; recall Balanced view Critical Note: All these metrics:\n❌ Don\u0026rsquo;t consider semantics ❌ Don\u0026rsquo;t consider sentence structure ✅ Only count n-gram matches Modern Alternative: Use neural metrics or human evaluation for critical applications!\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.9-week9/1.9.5-day45-2025-11-07/","title":"Day 45 - Transformer Decoder &amp; GPT2 Implementation","tags":[],"description":"","content":"Date: 2025-11-07 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nBuilding a Transformer Decoder: GPT2 Architecture Now let\u0026rsquo;s see how all these pieces come together in actual code!\nTransformer Decoder Structure (GPT2-style) Input: Tokenized sentence [1, 2, 3, 4, 5] ↓ Embedding Layer: Convert tokens to vectors ↓ Add Positional Encoding: Add position info ↓ ┌──────────────────────────────────┐ │ Decoder Block (N times) │ │ ├─ Masked Self-Attention │ │ ├─ Residual + LayerNorm │ │ ├─ Feed-Forward │ │ └─ Residual + LayerNorm │ └──────────────────────────────────┘ ↓ Linear Layer: Project to vocab size ↓ Softmax: Convert to probabilities ↓ Output: Probabilities for next word PyTorch Implementation Step 1: Word Embedding + Positional Encoding import torch import torch.nn as nn class TransformerDecoder(nn.Module): def __init__(self, vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1): super().__init__() # 1. Embedding layer self.embedding = nn.Embedding(vocab_size, d_model) # 2. Positional encoding (learned) self.positional_encoding = nn.Embedding(max_seq_len, d_model) # 3. Decoder blocks (repeat N times) self.decoder_layers = nn.ModuleList([ DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers) ]) # 4. Output layer self.final_layer = nn.Linear(d_model, vocab_size) self.softmax = nn.Softmax(dim=-1) self.d_model = d_model def forward(self, input_ids, mask=None): # input_ids shape: [batch_size, seq_length] batch_size, seq_len = input_ids.shape # 1. Embed tokens x = self.embedding(input_ids) # [batch, seq_len, d_model] # 2. Add positional encoding positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) pos_encoding = self.positional_encoding(positions) x = x + pos_encoding # [batch, seq_len, d_model] # 3. Pass through decoder layers for decoder_layer in self.decoder_layers: x = decoder_layer(x, mask) # 4. Project to vocab logits = self.final_layer(x) # [batch, seq_len, vocab_size] return logits Step 2: Decoder Block class DecoderBlock(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super().__init__() # 1. Masked multi-head attention self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) self.norm1 = nn.LayerNorm(d_model) # 2. Feed-forward network self.feed_forward = FeedForward(d_model, d_ff, dropout) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, mask=None): # x shape: [batch, seq_len, d_model] # Masked Self-Attention + Residual + Norm attn_output = self.self_attention(x, x, x, mask) # Q=K=V x = x + self.dropout(attn_output) x = self.norm1(x) # Feed-Forward + Residual + Norm ff_output = self.feed_forward(x) x = x + self.dropout(ff_output) x = self.norm2(x) return x Step 3: Multi-Head Attention class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads, dropout): super().__init__() assert d_model % num_heads == 0 self.num_heads = num_heads self.d_k = d_model // num_heads # Linear projections for Q, K, V self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) # Output projection self.W_o = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def forward(self, Q, K, V, mask=None): batch_size = Q.shape[0] # 1. Linear projections and split into multiple heads Q = self.W_q(Q) # [batch, seq_len, d_model] K = self.W_k(K) V = self.W_v(V) # Reshape for multi-head: [batch, seq_len, num_heads, d_k] Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now: [batch, num_heads, seq_len, d_k] # 2. Scaled dot-product attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [batch, num_heads, seq_len, seq_len] # 3. Apply mask (for causal masking in decoder) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 4. Softmax attention_weights = torch.softmax(scores, dim=-1) attention_weights = self.dropout(attention_weights) # 5. Multiply by value context = torch.matmul(attention_weights, V) # [batch, num_heads, seq_len, d_k] # 6. Concatenate heads context = context.transpose(1, 2).contiguous() context = context.view(batch_size, -1, self.d_model) # 7. Final linear projection output = self.W_o(context) return output Step 4: Feed-Forward Network class FeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout): super().__init__() self.linear1 = nn.Linear(d_model, d_ff) # 512 → 2048 self.linear2 = nn.Linear(d_ff, d_model) # 2048 → 512 self.relu = nn.ReLU() self.dropout = nn.Dropout(dropout) def forward(self, x): x = self.linear1(x) # Expand x = self.relu(x) # Non-linearity x = self.dropout(x) # Regularization x = self.linear2(x) # Compress return x Step 5: Causal Mask (for preventing future attendance) def create_causal_mask(seq_len, device): \u0026#34;\u0026#34;\u0026#34; Creates a mask that prevents attention to future positions. Output: [1, 0, 0, 0] [1, 1, 0, 0] [1, 1, 1, 0] [1, 1, 1, 1] Position i can only attend to positions 0...i \u0026#34;\u0026#34;\u0026#34; mask = torch.tril(torch.ones(seq_len, seq_len, device=device)) return mask.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, seq_len] # Usage: mask = create_causal_mask(seq_len=10, device=\u0026#39;cuda\u0026#39;) Training Loop def train_transformer(model, train_loader, epochs=10, learning_rate=0.0001): optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) loss_fn = nn.CrossEntropyLoss() for epoch in range(epochs): total_loss = 0 for batch_idx, (input_ids, target_ids) in enumerate(train_loader): # Forward pass logits = model(input_ids) # logits: [batch, seq_len, vocab_size] # target_ids: [batch, seq_len] # Calculate loss loss = loss_fn( logits.view(-1, vocab_size), target_ids.view(-1) ) # Backward pass optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() print(f\u0026#34;Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\u0026#34;) Inference (Text Generation) def generate_text(model, start_token, max_length=50, device=\u0026#39;cuda\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Generate text autoregressively using the trained transformer. \u0026#34;\u0026#34;\u0026#34; model.eval() generated = [start_token] with torch.no_grad(): for _ in range(max_length): # Prepare input input_ids = torch.tensor(generated, device=device).unsqueeze(0) # Forward pass logits = model(input_ids) # Get logits for last position last_logits = logits[0, -1, :] # [vocab_size] # Sample or greedy next_token = torch.argmax(last_logits).item() # Greedy # Or: next_token = torch.multinomial(softmax(last_logits), 1).item() # Sample generated.append(next_token) if next_token == end_token: break return generated Complete Working Example # Initialize model model = TransformerDecoder( vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1 ).to(\u0026#39;cuda\u0026#39;) # Create dummy data batch_size, seq_len = 32, 128 input_ids = torch.randint(0, 10000, (batch_size, seq_len)).to(\u0026#39;cuda\u0026#39;) # Forward pass output = model(input_ids) print(f\u0026#34;Output shape: {output.shape}\u0026#34;) # [32, 128, 10000] # Create causal mask mask = create_causal_mask(seq_len, \u0026#39;cuda\u0026#39;) # Forward with mask output_masked = model(input_ids, mask) print(f\u0026#34;Masked output shape: {output_masked.shape}\u0026#34;) # Generate text generated = generate_text(model, start_token=101, max_length=20) print(f\u0026#34;Generated sequence: {generated}\u0026#34;) Key Components Summary Component Purpose Size Embedding Token → Vector vocab_size → d_model Positional Encoding Add position info d_model Multi-Head Attention Learn relationships d_model → d_model Feed-Forward Non-linear transform d_model → d_ff → d_model LayerNorm Stabilize training per-element Output Layer Project to vocab d_model → vocab_size Why This Architecture Works ✅ Parallel Processing: All positions processed together (fast!) ✅ Long-range Dependencies: Direct attention to any position (no vanishing gradients!) ✅ Interpretable: Can visualize attention patterns ✅ Scalable: Can grow to billions of parameters\nGPT2 Variants GPT-2 Small: 117M parameters GPT-2 Medium: 345M parameters GPT-2 Large: 762M parameters GPT-2 XL: 1.5B parameters All use the same decoder architecture, just scaled up!\nNext Steps Pre-training: Train on large text corpus (Wikipedia, Books, etc.) Fine-tuning: Adapt to specific tasks (translation, classification, etc.) Evaluation: Measure quality (perplexity, BLEU, human evaluation) Deployment: Use for real-world applications Week 9 Summary Lý do transformer thay thế RNN: xử lý song song, giải quyết bottleneck và gradient. Kiến trúc tổng thể encoder–decoder và các biến thể. Cơ chế scaled dot-product attention và các loại attention (self, masked, encoder–decoder). Triển khai chi tiết các khối decoder và tính toán xác suất đầu ra. Nền tảng cho các mô hình hiện đại như BERT, GPT, T5. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.10-week10/1.10.5-day50-2025-11-14/","title":"Day 50 - Fine-Tuning Practice","tags":[],"description":"","content":"Date: 2025-11-14 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nFine-Tuning Recipes Today focuses on practical knobs: which layers to freeze, how to schedule learning rates, and how to evaluate transfer setups.\nFreezing vs. Training Freeze lower layers when data is small or domain is close to pre-training. Unfreeze progressively (top -\u0026gt; bottom) if accuracy plateaus. Add a task-specific head (classification, span QA, seq2seq) and start there. Hyperparameter Basics Learning rate: 1e-5 to 3e-5 for full encoder; slightly higher if mostly frozen. Warmup steps: ~5% to 10% of total steps to stabilize early training. Max sequence length: match task; chunk long docs for QA. Evaluation Loop Track loss + task metrics (EM/F1 for QA, ROUGE for summarization, accuracy/F1 for classification). Early stop on dev set; keep best checkpoint, not just last. Compare feature-based baseline vs. full fine-tune for a small subset. Deployment Considerations Distill or quantize for latency if accuracy holds. Cache tokenizer and truncation rules to avoid drift between train and serve. Log prompts/inputs to debug closed-book vs. context-based behaviors. Practice Targets for Today Run (or plan) a small grid: learning rate, freezing strategy, max length. Evaluate on a held-out set and record EM/F1 or ROUGE. Decide post-training steps: distillation, quantization, or caching. "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.11-week11/1.11.5-day55-2025-11-21/","title":"Day 55 - Scaling &amp; Ops Playbook","tags":[],"description":"","content":"Date: 2025-11-21 (Friday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nScaling and Cost Controls Plan instance-level scaling and multi-concurrency to match steady traffic while capping spend.\nScaling Guardrails Max vCPU per capacity provider to bound total instances Choose instance families per workload (compute/memory/network) Steady traffic: prefer larger instances with multi-concurrency; spiky: reconsider default Lambda Cost \u0026amp; Pricing Apply EC2 Savings Plans/Reserved Instances to LMI capacity Monitor utilization vs. commitments; adjust instance mix as needed Rollout Checklist Capacity provider configured with correct role/VPC/instance allowlist Function versions published; warm paths verified (no cold starts) CloudWatch access validated; alerts on errors/latency/concurrency Run benchmark comparing LMI vs. default Lambda for your workload "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.12-week12/1.12.5-day60-2025-11-28/","title":"Day 60 - Compute Launches","tags":[],"description":"","content":"Date: 2025-11-28 (Friday)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nGraviton5 and Trainium3 Graviton5: next-gen CPU with improved price/perf for general workloads on EC2 Trainium3 UltraServers (3nm): higher throughput training/inference at lower cost Fit Analysis Workloads to move to Graviton5 (web/app, data processing) and expected savings Training/inference jobs that benefit from Trainium3 vs. current GPUs/CPUs Action Items Select 1–2 services for Graviton5 benchmarking; track perf/cost Plan a Trainium3 POC for a target model; compare throughput and budget Update capacity planning with new instance families and pricing "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.5-testing-and-logs/","title":"Testing and Logs","tags":[],"description":"","content":"Testing the API with Thunder Client (VS Code) After integrating API Gateway with Lambda, you now have an HTTP endpoint ready for testing.\nIn this step, you will use Thunder Client – a popular VS Code extension to send requests and view responses.\n🔹 Step 1 — Get the Invoke URL from API Gateway In AWS API Gateway:\nOpen the API Gateway service. Select the API you just created, for example: bedrock-chatbot-api. In the left menu, select Deploy → Stages. Click on the $default stage. In the Stage details section, you will see the Invoke URL. Copy the Invoke URL, for example:\nhttps://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com Next, add the path route you configured, for example: /chat\n👉 The complete endpoint will be:\nhttps://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com/chat 🔹 Step 2 — Install and open Thunder Client in VS Code Open VS Code. Select the Extensions tab. Search for Thunder Client and click Install. After installation, the Thunder Client icon will appear in the sidebar. Click on the icon and select New Request. Select the POST method. Paste the endpoint into the URL field: https://v8p3h9umxg.execute-api.ap-southeast-1.amazonaws.com/chat 🔹 Step 3 — Send JSON body and check the response Select the Body → JSON tab. Enter the content: { \u0026#34;question\u0026#34;: \u0026#34;What is Amazon Bedrock?\u0026#34; } Click Send to submit the request.\nIf the system is working correctly, you will receive a response similar to:\n{ \u0026#34;answer\u0026#34;: \u0026#34;Amazon Bedrock is a fully managed service...\u0026#34; } This confirms that:\nAPI Gateway received the request successfully Lambda executed correctly and called Bedrock The system returned the expected result 🔧 If you encounter errors? 403 / AccessDeniedException → Check Lambda\u0026rsquo;s IAM Role 500 Internal Error → Check CloudWatch Logs Missing \u0026lsquo;question\u0026rsquo; field → Check the JSON body Timeout → Increase Lambda timeout to 10–20 seconds ✔ Conclusion Bạn đã kiểm thử thành công toàn bộ pipeline:\nClient → API Gateway → Lambda → Bedrock → Trả kết quả AI\nBạn đã hoàn tất phần kiểm thử của workshop.\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.5-week5/","title":"Week 5 - AWS Security &amp; Identity","tags":[],"description":"","content":"Week: 2025-10-06 to 2025-10-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 5 Overview This week concentrated on AWS security and identity management foundations.\nKey Topics Shared Responsibility Model AWS IAM (Users, Groups, Roles, Policies) Amazon Cognito AWS Organizations \u0026amp; SCPs AWS Identity Center (SSO) AWS KMS AWS Security Hub Hands-on Labs Lab 18: AWS Security Hub Lab 22: AWS Lambda Automation with Slack Lab 27: AWS Resource Groups \u0026amp; Tagging Lab 28: IAM Cross-Region Role \u0026amp; Policy Lab 30: IAM Restriction Policy Lab 33: AWS KMS \u0026amp; CloudTrail Integration Lab 44: IAM Advanced Role Control Lab 48: IAM Access Keys \u0026amp; Roles "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Build a Simple AI API with AWS Lambda + Bedrock + API Gateway Overview Amazon Bedrock is a fully managed service that provides access to leading large language models (LLMs) such as Claude, Llama, Mistral, and Titan.\nYou can integrate AI features into your applications using simple API calls without managing infrastructure or hosting models yourself.\nIn this workshop, you will build a simple AI Q\u0026amp;A API using:\nAWS Lambda – handles incoming requests and calls Bedrock Amazon Bedrock Runtime – sends prompts and receives model responses Amazon API Gateway – exposes an HTTP endpoint for client requests A key aspect of this workshop is the use of the Converse API — a unified interface for Bedrock models that support the Converse capability (e.g., Claude 3, Claude 3.5, Llama 3.1, Mistral 24.07…).\nWith the Converse API:\nYou can switch models by simply changing the modelId in Lambda There is no need to rewrite conversation-handling logic You can easily test or compare multiple models with the same API flow Note: Only models that support Converse API can be used with the code in this workshop.\nWorkshop Content Introduction Prerequisites Lambda Calls Bedrock Create API Gateway Testing Cleanup "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/5-workshop/5.6-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources After completing the workshop, you should delete the AWS resources you no longer need to avoid unnecessary costs.\nBelow is a list of the services you created and how to remove them.\n🔹 1. Delete the API Gateway Open the API Gateway Console Select the API you created, e.g., bedrock-chatbot-api Choose Actions → Delete Confirm deletion This prevents any further requests from reaching Lambda and avoids API Gateway charges.\n🔹 2. Delete the Lambda Function Open the Lambda Console Select the function bedrock-chatbot-lambda Choose Actions → Delete function Confirm deletion 🔹 3. Delete the IAM Role and Policy Delete the Policy: Open IAM Console → Policies Search for lambda-bedrock Click Delete Delete the Role: Open IAM Console → Roles Search for lambda-bedrock-role Click Delete ⚠️ Note: You can only delete the role after deleting the Lambda function that uses it.\n🔹 4. Check CloudWatch Log Groups (Optional) Lambda logs remain in CloudWatch and may accumulate storage charges over time.\nOpen the CloudWatch Console Select Logs → Log groups Find your Lambda log group (e.g., /aws/lambda/bedrock-chatbot-lambda) Choose Actions → Delete log group 🔹 5. Review Other Resources (If Applicable) Depending on how you expanded the workshop, you may have created additional resources such as:\nS3 buckets Step Functions KMS keys VPC / Security Groups If they are no longer needed, delete them to prevent charges.\n🎉 All Done! You have now cleaned up all resources created during this workshop.\nYour AWS account will no longer incur costs from the lab environment.\nThank you for participating in the workshop!\n"},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey (FCJ) - AWS from September 8, 2025 to November 28, 2025 (12 weeks), I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in multiple AWS cloud computing learning tracks and hands-on projects, through which I improved my skills in cloud architecture design, AWS services implementation, technical documentation, blog translation, and workshop development.\nKey Accomplishments Throughout the internship, I completed the following major tasks:\nAWS Learning Journey (12 Weeks): Systematically studied core AWS services including:\nCloud Computing Fundamentals \u0026amp; AWS Well-Architected Framework Networking (VPC, Load Balancing, VPN, Direct Connect) Compute (EC2, Auto Scaling, Lambda, Containers) Storage (S3, Glacier, EFS, FSx, Snow Family) Database (RDS, Aurora, DynamoDB, Redshift) Security \u0026amp; Identity (IAM, Cognito, KMS, Security Hub) Monitoring (CloudWatch, X-Ray, CloudTrail) FitAI Challenge Proposal: Designed a comprehensive serverless architecture for an AI-powered fitness application using AWS services including Lambda, API Gateway, SageMaker, Bedrock, S3, Cognito, and more.\nBlog Translation: Translated 3+ technical AWS blog posts from English to Vietnamese, covering topics such as:\nAWS CodeDeploy to Amazon ECS blue/green deployments Data integration between Amazon SageMaker and Snowflake Amazon GuardDuty protection plans and Extended Threat Detection Workshop Development: Created a hands-on workshop on \u0026ldquo;Secure Hybrid Access to S3 using VPC Endpoints\u0026rdquo; covering Gateway and Interface VPC endpoints.\nEvent Participation: Attended 2 major AWS events:\nVietnam Cloud Day 2025 (September 18, 2025) AWS GenAI Builder Club - AI-Driven Development Life Cycle (October 3, 2025) In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Strengths Strong technical foundation: Successfully learned and applied a wide range of AWS services across compute, storage, networking, database, and security domains. Documentation skills: Produced high-quality technical documentation including proposals, translated blogs, and workshop materials. Teamwork \u0026amp; collaboration: Actively participated in team activities and AWS events, networking with FCJ members and industry professionals. Self-learning ability: Demonstrated capability to independently study complex cloud concepts and implement hands-on labs. Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking and approach challenges more systematically Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively Better time management to meet deadlines more consistently "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.6-week6/","title":"Week 6 - AWS Database Services","tags":[],"description":"","content":"Week: 2025-10-13 to 2025-10-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 6 Overview This week focused on the AWS database landscape, covering managed relational engines, purpose-built NoSQL stores, in-memory caching, and analytics data warehouses.\nKey Topics Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Hands-on Labs Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.7-week7/","title":"Week 7 - Vertical Slice Delivery","tags":[],"description":"","content":"Week: 2025-10-20 to 2025-10-24\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 7 Overview This week delivered vertical slice 0 for the Ebook Demo project, emphasizing contract-first development and automated testing to enable an end-to-end demo.\nKey Topics Vertical Slice Architecture and the scope of slice 0 Contract-first development with OpenAPI + Prism mocks Next.js 16 App Router \u0026amp; Server Components FastAPI clean architecture and CORS configuration Schemathesis contract testing and retrospective learnings Hands-on Labs Demo checklist for vertical slice 0 Mock API with Prism and connect it to Next.js Refactor FastAPI backend following clean architecture Run Schemathesis and update the validated workflow "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.8-week8/","title":"Week 8 - Natural Language Processing &amp; Deep Learning","tags":[],"description":"","content":"Week: 2025-10-27 to 2025-10-31\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 8 Overview This week provides a comprehensive deep-dive into Natural Language Processing (NLP), covering linguistic foundations, modern NLP applications, sequence-to-sequence architectures, and evaluation methodologies. From understanding phonetics to implementing machine translation systems, this week bridges theory and practice in NLP.\nKey Topics Linguistic Foundations Core components: Phonetics, Phonology, Morphology, Syntax, Semantics, Pragmatics Understanding how language structure informs NLP design NLP Applications Search engines and intent recognition Online advertising with NER and relationship extraction Voice assistants and speech recognition Chatbots with NLU/NLG pipelines Machine translation systems Text summarization (extractive \u0026amp; abstractive) Deep Learning Architectures Seq2seq models with encoder-decoder architecture LSTM deep dive: forget gate, input gate, cell state, output gate Attention mechanism and self-attention Neural Machine Translation (NMT) implementation Evaluation \u0026amp; Decoding BLEU score (precision-based) ROUGE score (recall-based) F1 score for MT evaluation Beam search decoding Minimum Bayes Risk (MBR) sampling Hands-on Labs Building voicebot and chatbot workflows Implementing LSTM for sequence modeling Creating encoder-decoder with attention Neural machine translation end-to-end Evaluating translation quality with BLEU/ROUGE Implementing beam search and MBR "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.9-week9/","title":"Week 9 - Transformer Architecture &amp; Implementation","tags":[],"description":"","content":"Week: 2025-11-03 to 2025-11-07\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 9 Overview This week explores the Transformer architecture, a revolutionary model that replaced RNNs in NLP. We\u0026rsquo;ll understand why transformers are needed, how they work internally, and implement them from scratch. From attention mechanisms to the full encoder-decoder design, this week bridges theory and practical implementation.\nKey Topics RNN Limitations \u0026amp; Transformer Introduction Sequential processing bottlenecks in RNNs Vanishing gradient problems Information bottleneck with long sequences Why attention is all you need Transformer Architecture Encoder-decoder structure Multi-head attention layers Positional encoding Residual connections \u0026amp; layer normalization Feed-forward networks Attention Mechanisms Scale dot-product attention (core mechanism) Self-attention (same sentence) Masked attention (decoder) Encoder-decoder attention Multi-head attention for parallel computation Transformer Decoder \u0026amp; GPT2 Positional embeddings Decoder block implementation Feed-forward layer design Output probability calculation Applications \u0026amp; Models GPT-2 (Generative Pre-trained Transformer) BERT (Bidirectional Encoder Representations) T5 (Text-to-Text Transfer Transformer) Applications: Translation, Classification, QA, Summarization, Sentiment Analysis Learning Objectives ✅ Understand RNN limitations and why transformers solve them ✅ Grasp the complete transformer architecture ✅ Implement attention mechanisms from scratch ✅ Build a transformer decoder (GPT2-style) ✅ Recognize transformer applications and state-of-the-art models Daily Breakdown Day Focus Topics 41 RNN Problems Sequential processing, Vanishing gradients, Information bottleneck 42 Architecture Overview Encoder-decoder, Multi-head attention, Positional encoding 43 Attention Core Scale dot-product attention formula, Matrix operations, GPU efficiency 44 Attention Types Self-attention, Masked attention, Encoder-decoder attention 45 Decoder Implementation GPT2 architecture, Building blocks, Code walkthrough Prerequisites Deep understanding of RNNs, LSTMs, and attention from Week 8 Comfortable with matrix operations and linear algebra PyTorch or TensorFlow knowledge helpful Next Steps Study the paper \u0026ldquo;Attention is All You Need\u0026rdquo; (Vaswani et al., 2017) Implement transformer components incrementally Experiment with pre-trained models (BERT, GPT-2, T5) "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.10-week10/","title":"Week 10 - Transfer Learning, BERT &amp; T5","tags":[],"description":"","content":"Week: 2025-11-10 to 2025-11-14\nStatus: \u0026ldquo;In progress\u0026rdquo;\nWeek 10 Overview This week dives into transfer learning for NLP and how modern QA systems benefit from pre-trained transformers. We will contrast classical training with feature-based reuse and fine-tuning, then study two flagship models: BERT for bidirectional context and T5 for text-to-text multitask learning, along with practical QA setups (context-based vs. closed-book).\nKey Topics Transfer Learning Fundamentals Classical vs. transfer learning pipelines Reusing pre-trained weights to speed convergence Feature-based representations vs. fine-tuning Benefits: faster training, better predictions, less labeled data Question Answering Modes Context-based QA (span extraction with provided context) Closed-book QA (generate answers without context) How pre-training quality shapes QA performance BERT Bidirectional Context Masked language modeling for contextual embeddings Next sentence prediction for sentence-level coherence Using both left and right context to predict tokens Typical downstream uses: QA, sentiment, classification T5 Text-to-Text Multitask Unified text-to-text framing for multiple tasks Prompting the same model for rating, QA, summarization, translation Scaling with large corpora (e.g., C4 vs. Wikipedia) Multitask transfer to improve generalization Training \u0026amp; Data Strategy Labeled vs. unlabeled data mix; self-supervised masking Freezing backbone vs. adding task heads Fine-tuning recipes for downstream tasks (QA, summarization, translation) Learning Objectives ✅ Explain transfer learning and when to prefer it over training from scratch ✅ Distinguish feature-based reuse from full fine-tuning ✅ Compare context-based QA and closed-book QA setups ✅ Summarize how BERT and T5 pre-train and transfer across tasks ✅ Identify why transfer learning reduces data needs and training time Daily Breakdown Day Focus Topics 46 Transfer Learning Intro Classical vs. transfer pipeline, reuse weights, feature-based vs. fine-tuning, benefits 47 Question Answering Context-based span QA vs. closed-book QA, data needs, evaluation cues 48 BERT Bidirectionality Masked LM, next sentence prediction, leveraging both contexts for token prediction 49 T5 Multitask Model Text-to-text prompts, multitask sharing, scaling data (C4 vs. Wikipedia) 50 Fine-tuning Practice Freezing layers vs. adding heads, downstream tasks: QA, summarization, translation Prerequisites Solid grasp of transformer architecture from Week 9 Comfortable with attention mechanisms and encoder-decoder flow Basic familiarity with PyTorch or TensorFlow for fine-tuning Next Steps Read the BERT and T5 papers to internalize pre-training objectives Fine-tune a pre-trained BERT QA model (e.g., SQuAD-style span extraction) Experiment with T5 prompts for QA, summarization, and sentiment tasks Compare feature-based vs. fine-tuned performance on your own dataset "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.11-week11/","title":"Week 11 - Lambda Managed Instances &amp; AWS re:Invent Learnings","tags":[],"description":"","content":"Week: 2025-11-17 to 2025-11-21\nStatus: \u0026ldquo;Planned\u0026rdquo;\nWeek 11 Overview Content from AWS re:Invent 2025 (session CNS382) on Lambda Managed Instances (LMI): running Lambda functions on EC2 capacity with full serverless programming model, better control over compute, and reuse of EC2 pricing models. Focus on when to choose LMI vs. default Lambda, how to configure capacity providers, and how testing/ops practices change for predictable high-traffic workloads.\nKey Topics Why Lambda Managed Instances Keep Lambda dev experience while choosing EC2 instance families and pricing constructs Eliminate cold starts; support multi-concurrency per instance Apply EC2 Savings Plans/Reserved Instances; predictability for steady traffic Architecture \u0026amp; Setup Capacity Provider: VPC config, instance types (C/M/R families, x86/Graviton), scaling guardrails Steps: create capacity provider -\u0026gt; create function bound to provider -\u0026gt; publish version to launch instances LMI-managed lifecycle: AWS patches OS/runtime, handles routing/auto scaling; instances visible but immutable Networking \u0026amp; Security All egress via instance ENI in provider VPC; function-level VPC config disabled Close inbound SG rules; ensure paths to dependencies/CloudWatch (Internet or PrivateLink) EBS encryption (service key or custom KMS) Function Features \u0026amp; Scaling Supports ZIP/OCI packaging; Java/Python/Node/.NET runtimes; layers, extensions, function URLs, response streaming, durable functions Memory/CPU settings influence instance choice; optional overrides for allowed/excluded instance types Instance-level scaling guardrails (e.g., max vCPU) to control cost Workload Fit \u0026amp; Trade-offs Use LMI for high-traffic steady workloads, specialized compute/memory/network needs Keep default Lambda for spiky/short, unpredictable invocations Multi-concurrency and EC2 billing change cost/perf envelope Learning Objectives Explain when to prefer LMI vs. default Lambda for serverless apps Configure capacity providers with VPC, role, and instance constraints Describe LMI networking model and logging path Map Lambda features (packaging, runtimes, URLs, streaming, durable) to LMI Set scaling/cost guardrails and choose instance families for workload shapes Daily Breakdown Day Focus Topics 51 LMI Overview \u0026amp; Use Cases Why LMI, benefits, EC2 pricing reuse, when not to use it 52 Capacity Provider Setup VPC config, IAM operator role, instance families, KMS, guardrails 53 Functions on LMI Packaging/runtime support, memory/CPU mapping, multi-concurrency, publishing versions 54 Networking \u0026amp; Observability Egress paths, CloudWatch access, security groups, logging, monitoring notes 55 Scaling \u0026amp; Ops Playbook Max vCPU, steady traffic planning, cost controls, rollout checklist Prerequisites Familiarity with Lambda programming model and transformer material from prior weeks Basic EC2/VPC, IAM roles/policies, and CloudWatch logging Understanding of Savings Plans/Reserved Instances for EC2 Next Steps Draft a capacity provider for your target workload (VPC, instance shortlist, guardrails) Plan benchmarks comparing LMI vs. default Lambda on representative traffic Map monitoring and logging paths (CloudWatch endpoints, PrivateLink if needed) Decide pricing instruments (SP/RI) and multi-concurrency targets per function "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/1-worklog/1.12-week12/","title":"Week 12 - AWS re:Invent 2025 Announcements","tags":[],"description":"","content":"Week: 2025-11-24 to 2025-11-28\nStatus: \u0026ldquo;Planned\u0026rdquo;\nWeek 12 Overview Highlights from AWS re:Invent 2025: new Nova foundation models (speech-to-speech, multimodal, cost-effective reasoning), Bedrock expansions (open-weight models, reinforcement fine-tuning), vector and AI infra updates (S3 Vectors GA), and compute launches like Graviton5 CPUs and Trainium3 UltraServers. Focus on mapping announcements to practical adoption plans.\nKey Topics GenAI \u0026amp; Models Nova 2 family: Sonic (speech-to-speech), Lite (fast/cost-efficient), Omni (multimodal), Forge program for custom frontier models Nova Act for reliable UI agents; Bedrock AgentCore adds policy controls and quality evals Bedrock adds open-weight models (Mistral Large 3, Ministral 3) and reinforcement fine-tuning Vector \u0026amp; Data Amazon S3 Vectors GA: up to 2B vectors/index, ~100ms queries, lower cost vs. specialty DBs Clean Rooms synthetic data for privacy-preserving ML collaboration AI Dev Platform SageMaker AI serverless MLflow and new training features (checkpointless, elastic scaling) Compute \u0026amp; Hardware Graviton5 CPUs for better price/perf on EC2 Trainium3 UltraServers (3nm) for faster, cheaper training/inference Learning Objectives Identify which re:Invent AI/compute launches impact current workloads Plan pilot use cases for Nova models and Bedrock new capabilities Outline migration path to S3 Vectors for vector search storage Evaluate Graviton5/Trainium3 fit for cost/performance gains Daily Breakdown Day Focus Topics 56 Model Announcements Nova 2 variants (Sonic, Lite, Omni), Forge program, Nova Act agents 57 Bedrock \u0026amp; Agents Open-weight additions, reinforcement fine-tuning, AgentCore policy/quality 58 Vector \u0026amp; Data S3 Vectors GA, Clean Rooms synthetic data, vector scale/cost planning 59 SageMaker Platform Serverless MLflow, checkpointless \u0026amp; elastic training on HyperPod 60 Compute Launches Graviton5 CPUs, Trainium3 UltraServers, workload fit \u0026amp; migration checklist Prerequisites Familiarity with Bedrock model catalog and agent capabilities Basics of vector search architectures Understanding of EC2 instance families and accelerator choices Next Steps Select one pilot use case for Nova (speech, multimodal, or reasoning) and outline eval plan Draft migration/POC plan for S3 Vectors vs. current vector store Benchmark targets for Graviton5/Trainium3 against existing instances Define governance/policy needs before adopting AgentCore and Nova Act agents "},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://phuvo05.github.io/My-FCJ-Workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]